
<!DOCTYPE html>

<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"> <!--<![endif]-->
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Whole Slide Image Classification Using PyTorch and TIAToolbox — PyTorch Tutorials 2.2.0+cu121 documentation</title>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<link href="../_static/katex-math.css" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery.css" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-binder.css" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-dataframe.css" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-rendered-html.css" rel="stylesheet" type="text/css"/>
<link href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/custom.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/custom2.css" rel="stylesheet" type="text/css"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="../beginner/audio_io_tutorial.html" rel="next" title="Audio I/O"/>
<link href="../beginner/vt_tutorial.html" rel="prev" title="Optimizing Vision Transformer Model for Deployment"/>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
<!-- End Google Tag Manager -->
<script src="../_static/js/modernizr.min.js"></script>
<!-- Preload the theme fonts -->
<link as="font" crossorigin="anonymous" href="../_static/fonts/FreightSans/freight-sans-book.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" rel="preload" type="font/woff2"/>
<!-- Preload the katex fonts -->
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" rel="preload" type="font/woff2"/>
<link crossorigin="anonymous" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" rel="stylesheet"/>
</head>
<div class="container-fluid header-holder tutorials-header" id="header-holder">
<div class="container">
<div class="header-container">
<a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/"></a>
<div class="main-menu">
<ul>
<li>
<a href="https://pytorch.org/get-started">Get Started</a>
</li>
<li>
<a href="https://pytorch.org/ecosystem">Ecosystem</a>
</li>
<li>
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="resource-option with-down-arrow">
                Edge
              </a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/edge">
<span class="dropdown-title">About PyTorch Edge</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/executorch">
<span class="dropdown-title">ExecuTorch</span>
</a>
</div>
</div>
</li>
<li>
<a href="https://pytorch.org/blog/">Blog</a>
</li>
<li class="active">
<a href="https://pytorch.org/tutorials">Tutorials</a>
</li>
<li>
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="resource-option with-down-orange-arrow">
                Docs
              </a>
<div class="resources-dropdown-menu">
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
<span class="dropdown-title">PyTorch</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
<span class="dropdown-title">torchaudio</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
<span class="dropdown-title">torchtext</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
<span class="dropdown-title">torchvision</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
<span class="dropdown-title">torcharrow</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
<span class="dropdown-title">TorchData</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
<span class="dropdown-title">TorchRec</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
<span class="dropdown-title">TorchServe</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
<span class="dropdown-title">TorchX</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
<span class="dropdown-title">PyTorch on XLA Devices</span>
<p></p>
</a>
</div>
</div></li>
<li>
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="resource-option with-down-arrow">
                Resources
              </a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/features">
<span class="dropdown-title">About</span>
<p>Learn about PyTorch’s features and capabilities</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/foundation">
<span class="dropdown-title">PyTorch Foundation</span>
<p>Learn about the PyTorch foundation</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
<span class="dropdown-title">Community</span>
<p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
<span class="dropdown-title">Community Stories</span>
<p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/resources">
<span class="dropdown-title">Developer Resources</span>
<p>Find resources and get questions answered</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/events">
<span class="dropdown-title">Events</span>
<p>Find events, webinars, and podcasts</p>
</a>
<a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
<span class="dropdown-title">Forums</span>
<p>A place to discuss PyTorch code, issues, install, research</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/hub">
<span class="dropdown-title">Models (Beta)</span>
<p>Discover, publish, and reuse pre-trained models</p>
</a>
</div>
</div>
</li>
<li>
<a href="https://github.com/pytorch/pytorch">GitHub</a>
</li>
</ul>
</div>
<a class="main-menu-open-button" data-behavior="open-mobile-menu" href="#"></a>
</div>
</div>
</div>
<body class="pytorch-body">
<div class="table-of-contents-link-wrapper">
<span>Table of Contents</span>
<a class="toggle-table-of-contents" data-behavior="toggle-table-of-contents" href="#"></a>
</div>
<nav class="pytorch-left-menu" data-toggle="wy-nav-shift" id="pytorch-left-menu">
<div class="pytorch-side-scroll">
<div aria-label="main navigation" class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation">
<div class="pytorch-left-menu-search">
<div class="version">
                  2.2.0+cu121
                </div>
<div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search Tutorials" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<p class="caption" role="heading"><span class="caption-text">PyTorch Recipes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../recipes/recipes_index.html">See All Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prototype/prototype_index.html">See All Prototype Recipes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Introduction to PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/intro.html">Learn the Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/quickstart_tutorial.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/tensorqs_tutorial.html">Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/data_tutorial.html">Datasets &amp; DataLoaders</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/transforms_tutorial.html">Transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/buildmodel_tutorial.html">Build the Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/autogradqs_tutorial.html">Automatic Differentiation with <code class="docutils literal notranslate"><span class="pre">torch.autograd</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/optimization_tutorial.html">Optimizing Model Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/saveloadrun_tutorial.html">Save and Load the Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Introduction to PyTorch on YouTube</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/introyt.html">Introduction to PyTorch - YouTube Series</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/introyt/introyt1_tutorial.html">Introduction to PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/introyt/tensors_deeper_tutorial.html">Introduction to PyTorch Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/introyt/autogradyt_tutorial.html">The Fundamentals of Autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/introyt/modelsyt_tutorial.html">Building Models with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/introyt/tensorboardyt_tutorial.html">PyTorch TensorBoard Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/introyt/trainingyt.html">Training with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/introyt/captumyt.html">Model Understanding with Captum</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Learning PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/deep_learning_60min_blitz.html">Deep Learning with PyTorch: A 60 Minute Blitz</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/pytorch_with_examples.html">Learning PyTorch with Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/nn_tutorial.html">What is <cite>torch.nn</cite> <em>really</em>?</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard_tutorial.html">Visualizing Models, Data, and Training with TensorBoard</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Image and Video</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torchvision_tutorial.html">TorchVision Object Detection Finetuning Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html">Transfer Learning for Computer Vision Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/fgsm_tutorial.html">Adversarial Example Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/dcgan_faces_tutorial.html">DCGAN Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="spatial_transformer_tutorial.html">Spatial Transformer Networks Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/vt_tutorial.html">Optimizing Vision Transformer Model for Deployment</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Whole Slide Image Classification Using PyTorch and TIAToolbox</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Audio</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_io_tutorial.html">Audio I/O</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_resampling_tutorial.html">Audio Resampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_data_augmentation_tutorial.html">Audio Data Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_feature_extractions_tutorial.html">Audio Feature Extractions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_feature_augmentation_tutorial.html">Audio Feature Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_datasets_tutorial.html">Audio Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="speech_recognition_pipeline_tutorial.html">Speech Recognition with Wav2Vec2</a></li>
<li class="toctree-l1"><a class="reference internal" href="text_to_speech_with_torchaudio.html">Text-to-speech with Tacotron2</a></li>
<li class="toctree-l1"><a class="reference internal" href="forced_alignment_with_torchaudio_tutorial.html">Forced Alignment with Wav2Vec2</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Text</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/transformer_tutorial.html">Language Modeling with <code class="docutils literal notranslate"><span class="pre">nn.Transformer</span></code> and torchtext</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/bettertransformer_tutorial.html">Fast Transformer Inference with Better Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="char_rnn_classification_tutorial.html">NLP From Scratch: Classifying Names with a Character-Level RNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="char_rnn_generation_tutorial.html">NLP From Scratch: Generating Names with a Character-Level RNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="seq2seq_translation_tutorial.html">NLP From Scratch: Translation with a Sequence to Sequence Network and Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/text_sentiment_ngrams_tutorial.html">Text classification with the torchtext library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/translation_transformer.html">Language Translation with <code class="docutils literal notranslate"><span class="pre">nn.Transformer</span></code> and torchtext</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/torchtext_custom_dataset_tutorial.html">Preprocess custom text dataset using Torchtext</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Backends</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/onnx/intro_onnx.html">Introduction to ONNX</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reinforcement Learning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="reinforcement_q_learning.html">Reinforcement Learning (DQN) Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="reinforcement_ppo.html">Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="mario_rl_tutorial.html">Train a Mario-playing RL Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/pendulum.html">Pendulum: Writing your environment and transforms with TorchRL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deploying PyTorch Models in Production</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/onnx/intro_onnx.html">Introduction to ONNX</a></li>
<li class="toctree-l1"><a class="reference internal" href="flask_rest_api_tutorial.html">Deploying PyTorch in Python via a REST API with Flask</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/Intro_to_TorchScript_tutorial.html">Introduction to TorchScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_export.html">Loading a TorchScript Model in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/super_resolution_with_onnxruntime.html">(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="realtime_rpi.html">Real Time Inference on Raspberry Pi 4 (30 fps!)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Profiling PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/profiler.html">Profiling your PyTorch Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/hta_intro_tutorial.html">Introduction to Holistic Trace Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/hta_trace_diff_tutorial.html">Trace Diff using Holistic Trace Analysis</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Code Transforms with FX</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="fx_conv_bn_fuser.html">(beta) Building a Convolution/Batch Norm fuser in FX</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx_profiling_tutorial.html">(beta) Building a Simple CPU Performance Profiler with FX</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Frontend APIs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="memory_format_tutorial.html">(beta) Channels Last Memory Format in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="forward_ad_usage.html">Forward-mode Automatic Differentiation (Beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="jacobians_hessians.html">Jacobians, Hessians, hvp, vhp, and more: composing function transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="ensembling.html">Model ensembling</a></li>
<li class="toctree-l1"><a class="reference internal" href="per_sample_grads.html">Per-sample-gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_frontend.html">Using the PyTorch C++ Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/torch-script-parallelism.html">Dynamic Parallelism in TorchScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_autograd.html">Autograd in C++ Frontend</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Extending PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="custom_function_double_backward_tutorial.html">Double Backward with Custom Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_function_conv_bn_tutorial.html">Fusing Convolution and Batch Norm using Custom Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_extension.html">Custom C++ and CUDA Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/torch_script_custom_ops.html">Extending TorchScript with Custom C++ Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/torch_script_custom_classes.html">Extending TorchScript with Custom C++ Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/dispatcher.html">Registering a Dispatched Operator in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/extend_dispatcher.html">Extending dispatcher for a new backend in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/privateuseone.html">Facilitating New Backend Integration by PrivateUse1</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/profiler.html">Profiling your PyTorch Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard_profiler_tutorial.html">PyTorch Profiler With TensorBoard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/hyperparameter_tuning_tutorial.html">Hyperparameter tuning with Ray Tune</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/vt_tutorial.html">Optimizing Vision Transformer Model for Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="parametrizations.html">Parametrizations Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="pruning_tutorial.html">Pruning Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/dynamic_quantization_tutorial.html">(beta) Dynamic Quantization on an LSTM Word Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="dynamic_quantization_bert_tutorial.html">(beta) Dynamic Quantization on BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantized_transfer_learning_tutorial.html">(beta) Quantized Transfer Learning for Computer Vision Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/static_quantization_tutorial.html">(beta) Static Quantization with Eager Mode in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchserve_with_ipex.html">Grokking PyTorch Intel CPU performance from first principles</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchserve_with_ipex_2.html">Grokking PyTorch Intel CPU performance from first principles (Part 2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="nvfuser_intro_tutorial.html">Getting Started - Accelerate Your Scripts with nvFuser</a></li>
<li class="toctree-l1"><a class="reference internal" href="ax_multiobjective_nas_tutorial.html">Multi-Objective NAS with Ax</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_compile_tutorial.html">Introduction to <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="inductor_debug_cpu.html">Inductor CPU backend debugging and profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="scaled_dot_product_attention_tutorial.html">(Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="scaled_dot_product_attention_tutorial.html#using-sdpa-with-torch-compile">Using SDPA with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="scaled_dot_product_attention_tutorial.html#conclusion">Conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/knowledge_distillation_tutorial.html">Knowledge Distillation Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Parallel and Distributed Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../distributed/home.html">Distributed and Parallel Training Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/dist_overview.html">PyTorch Distributed Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/ddp_series_intro.html">Distributed Data Parallel in PyTorch - Video Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_parallel_tutorial.html">Single-Machine Model Parallel Best Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_tutorial.html">Getting Started with Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="dist_tuto.html">Writing Distributed Applications with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="FSDP_tutorial.html">Getting Started with Fully Sharded Data Parallel(FSDP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="FSDP_adavnced_tutorial.html">Advanced Model Training with Fully Sharded Data Parallel (FSDP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="process_group_cpp_extension_tutorial.html">Customize Process Group Backends Using Cpp Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_tutorial.html">Getting Started with Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_param_server_tutorial.html">Implementing a Parameter Server Using Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="dist_pipeline_parallel_tutorial.html">Distributed Pipeline Parallelism Using RPC</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_async_execution.html">Implementing Batch RPC Processing Using Asynchronous Executions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/rpc_ddp_tutorial.html">Combining Distributed DataParallel with Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline_tutorial.html">Training Transformer models using Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/ddp_pipeline.html">Training Transformer models using Distributed Data Parallel and Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/generic_join.html">Distributed Training with Uneven Inputs Using the Join Context Manager</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Mobile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/deeplabv3_on_ios.html">Image Segmentation DeepLabV3 on iOS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/deeplabv3_on_android.html">Image Segmentation DeepLabV3 on Android</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Recommendation Systems</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torchrec_tutorial.html">Introduction to TorchRec</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/sharding.html">Exploring TorchRec sharding</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multimodality</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/flava_finetuning_tutorial.html">TorchMultimodal Tutorial: Finetuning FLAVA</a></li>
</ul>
</div>
</div>
</nav>
<div class="pytorch-container">
<div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
<div class="pytorch-breadcrumbs-wrapper">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="pytorch-breadcrumbs">
<li>
<a href="../index.html">
          
            Tutorials
          
        </a> &gt;
      </li>
<li>Whole Slide Image Classification Using PyTorch and TIAToolbox</li>
<li class="pytorch-breadcrumbs-aside">
<a href="../_sources/intermediate/tiatoolbox_tutorial.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"/></a>
</li>
</ul>
</div>
</div>
<div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
</div>
<section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
<div class="pytorch-content-left">
<div class="pytorch-call-to-action-links">
<div id="tutorial-type">intermediate/tiatoolbox_tutorial</div>
<div id="google-colab-link">
<img class="call-to-action-img" src="../_static/images/pytorch-colab.svg">
<div class="call-to-action-desktop-view">Run in Google Colab</div>
<div class="call-to-action-mobile-view">Colab</div>
</img></div>
<div id="download-notebook-link">
<img class="call-to-action-notebook-img" src="../_static/images/pytorch-download.svg"/>
<div class="call-to-action-desktop-view">Download Notebook</div>
<div class="call-to-action-mobile-view">Notebook</div>
</div>
<div id="github-view-link">
<img class="call-to-action-img" src="../_static/images/pytorch-github.svg"/>
<div class="call-to-action-desktop-view">View on GitHub</div>
<div class="call-to-action-mobile-view">GitHub</div>
</div>
</div>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" style="display:none;visibility:hidden" width="0"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<div class="rst-content">
<div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
<div class="section" id="whole-slide-image-classification-using-pytorch-and-tiatoolbox">
<h1>Whole Slide Image Classification Using PyTorch and TIAToolbox<a class="headerlink" href="#whole-slide-image-classification-using-pytorch-and-tiatoolbox" title="Permalink to this heading">¶</a></h1>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>To get the most of this tutorial, we suggest using this
<a class="reference external" href="https://colab.research.google.com/github/pytorch/tutorials/blob/main/_static/tiatoolbox_tutorial.ipynb">Colab Version</a>. This will allow you to experiment with the information presented below.</p>
</div>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">¶</a></h2>
<p>In this tutorial, we will show how to classify Whole Slide Images (WSIs)
using PyTorch deep learning models with help from TIAToolbox. A WSI
is an image of a sample of human tissue taken through a surgery or biopsy and
scanned using specialized scanners. They are used by pathologists and
computational pathology researchers to <a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7522141/">study diseases such as cancer at the microscopic
level</a> in
order to understand for example tumor growth and help improve treatment
for patients.</p>
<p>What makes WSIs challenging to process is their enormous size. For
example, a typical slide image has in the order of <a class="reference external" href="https://doi.org/10.1117%2F12.912388">100,000x100,000
pixels</a> where each pixel can
correspond to about 0.25x0.25 microns on the slide. This introduces
challenges in loading and processing such images, not to mention
hundreds or even thousands of WSIs in a single study (larger studies
produce better results)!</p>
<p>Conventional image processing pipelines are not suitable for WSI
processing so we need better tools. This is where
<a class="reference external" href="https://github.com/TissueImageAnalytics/tiatoolbox">TIAToolbox</a> can
help as it brings a set of useful tools to import and process tissue
slides in a fast and computationally efficient manner. Typically, WSIs
are saved in a pyramid structure with multiple copies of the same image
at various magnification levels optimized for visualization. The level 0
(or the bottom level) of the pyramid contains the image at the highest
magnification or zoom level, whereas the higher levels in the pyramid
have a lower resolution copy of the base image. The pyramid structure is
sketched below.</p>
<p><img alt="WSI pyramid stack" src="../_images/read_bounds_tissue.webp"/> <em>WSI pyramid stack
(</em><a class="reference external" href="https://tia-toolbox.readthedocs.io/en/latest/_autosummary/tiatoolbox.wsicore.wsireader.WSIReader.html#">source</a><em>)</em></p>
<p>TIAToolbox allows us to automate common downstream analysis tasks such
as <a class="reference external" href="https://doi.org/10.1016/j.media.2022.102685">tissue
classification</a>. In this
tutorial we show how you can: 1. Load WSI images using
TIAToolbox; and 2. Use different PyTorch models to classify slides at
the patch-level. In this tutorial, we will provide an example of using
TorchVision <code class="docutils literal notranslate"><span class="pre">ResNet18</span></code> model and custom
<cite>HistoEncoder</cite> &lt;<a class="reference external" href="https://github.com/jopo666/HistoEncoder">https://github.com/jopo666/HistoEncoder</a>&gt;`__ model.</p>
<p>Let’s get started!</p>
</div>
<div class="section" id="setting-up-the-environment">
<h2>Setting up the environment<a class="headerlink" href="#setting-up-the-environment" title="Permalink to this heading">¶</a></h2>
<p>To run the examples provided in this tutorial, the following packages
are required as prerequisites.</p>
<ol class="arabic simple">
<li><p>OpenJpeg</p></li>
<li><p>OpenSlide</p></li>
<li><p>Pixman</p></li>
<li><p>TIAToolbox</p></li>
<li><p>HistoEncoder (for a custom model example)</p></li>
</ol>
<p>Please run the following command in your terminal to install these
packages:</p>
<p><cite>apt-get -y -qq install libopenjp2-7-dev libopenjp2-tools openslide-tools libpixman-1-dev</cite>
<cite>pip install -q ‘tiatoolbox&lt;1.5’ histoencoder &amp;&amp; echo “Installation is done.”</cite></p>
<p>Alternatively, you can run <code class="docutils literal notranslate"><span class="pre">brew</span> <span class="pre">install</span> <span class="pre">openjpeg</span> <span class="pre">openslide</span></code> to
install the prerequisite packages on MacOS instead of <code class="docutils literal notranslate"><span class="pre">apt-get</span></code>.
Further information on installation can be <a class="reference external" href="https://tia-toolbox.readthedocs.io/en/latest/installation.html">found
here</a>.</p>
<div class="section" id="importing-related-libraries">
<h3>Importing related libraries<a class="headerlink" href="#importing-related-libraries" title="Permalink to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">"""Import modules required to run the Jupyter notebook."""</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="c1"># Configure logging</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="k">if</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span><span class="o">.</span><span class="n">hasHandlers</span><span class="p">():</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span><span class="o">.</span><span class="n">handlers</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">"ignore"</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">".*The 'nopython' keyword.*"</span><span class="p">)</span>

<span class="c1"># Downloading data and files</span>
<span class="kn">import</span> <span class="nn">shutil</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">zipfile</span> <span class="kn">import</span> <span class="n">ZipFile</span>

<span class="c1"># Data processing and visualization</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="kn">import</span> <span class="nn">PIL</span>
<span class="kn">import</span> <span class="nn">contextlib</span>
<span class="kn">import</span> <span class="nn">io</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">confusion_matrix</span>

<span class="c1"># TIAToolbox for WSI loading and processing</span>
<span class="kn">from</span> <span class="nn">tiatoolbox</span> <span class="kn">import</span> <span class="n">logger</span>
<span class="kn">from</span> <span class="nn">tiatoolbox.models.architecture</span> <span class="kn">import</span> <span class="n">vanilla</span>
<span class="kn">from</span> <span class="nn">tiatoolbox.models.engine.patch_predictor</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">IOPatchPredictorConfig</span><span class="p">,</span>
    <span class="n">PatchPredictor</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">tiatoolbox.utils.misc</span> <span class="kn">import</span> <span class="n">download_data</span><span class="p">,</span> <span class="n">grab_files_from_dir</span>
<span class="kn">from</span> <span class="nn">tiatoolbox.utils.visualization</span> <span class="kn">import</span> <span class="n">overlay_prediction_mask</span>
<span class="kn">from</span> <span class="nn">tiatoolbox.wsicore.wsireader</span> <span class="kn">import</span> <span class="n">WSIReader</span>

<span class="c1"># Torch-related</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>

<span class="c1"># Configure plotting</span>
<span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">"figure.dpi"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">160</span>  <span class="c1"># for high resolution figure in notebook</span>
<span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">"figure.facecolor"</span><span class="p">]</span> <span class="o">=</span> <span class="s2">"white"</span>  <span class="c1"># To make sure text is visible in dark mode</span>

<span class="c1"># If you are not using GPU, change ON_GPU to False</span>
<span class="n">ON_GPU</span> <span class="o">=</span> <span class="kc">True</span>

<span class="c1"># Function to suppress console output for overly verbose code blocks</span>
<span class="k">def</span> <span class="nf">suppress_console_output</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">redirect_stderr</span><span class="p">(</span><span class="n">io</span><span class="o">.</span><span class="n">StringIO</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="section" id="clean-up-before-a-run">
<h3>Clean-up before a run<a class="headerlink" href="#clean-up-before-a-run" title="Permalink to this heading">¶</a></h3>
<p>To ensure proper clean-up (for example in abnormal termination), all
files downloaded or created in this run are saved in a single directory
<code class="docutils literal notranslate"><span class="pre">global_save_dir</span></code>, which we set equal to “./tmp/”. To simplify
maintenance, the name of the directory occurs only at this one place, so
that it can easily be changed, if desired.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">"ignore"</span><span class="p">)</span>
<span class="n">global_save_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"./tmp/"</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">rmdir</span><span class="p">(</span><span class="n">dir_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">Path</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Helper function to delete directory."""</span>
    <span class="k">if</span> <span class="n">Path</span><span class="p">(</span><span class="n">dir_path</span><span class="p">)</span><span class="o">.</span><span class="n">is_dir</span><span class="p">():</span>
        <span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">dir_path</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"Removing directory </span><span class="si">%s</span><span class="s2">"</span><span class="p">,</span> <span class="n">dir_path</span><span class="p">)</span>


<span class="n">rmdir</span><span class="p">(</span><span class="n">global_save_dir</span><span class="p">)</span>  <span class="c1"># remove  directory if it exists from previous runs</span>
<span class="n">global_save_dir</span><span class="o">.</span><span class="n">mkdir</span><span class="p">()</span>
<span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"Creating new directory </span><span class="si">%s</span><span class="s2">"</span><span class="p">,</span> <span class="n">global_save_dir</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="downloading-the-data">
<h3>Downloading the data<a class="headerlink" href="#downloading-the-data" title="Permalink to this heading">¶</a></h3>
<p>For our sample data, we will use one whole-slide image, and patches from
the validation subset of <a class="reference external" href="https://zenodo.org/record/1214456#.YJ-tn3mSkuU">Kather
100k</a> dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">wsi_path</span> <span class="o">=</span> <span class="n">global_save_dir</span> <span class="o">/</span> <span class="s2">"sample_wsi.svs"</span>
<span class="n">patches_path</span> <span class="o">=</span> <span class="n">global_save_dir</span> <span class="o">/</span> <span class="s2">"kather100k-validation-sample.zip"</span>
<span class="n">weights_path</span> <span class="o">=</span> <span class="n">global_save_dir</span> <span class="o">/</span> <span class="s2">"resnet18-kather100k.pth"</span>

<span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"Download has started. Please wait..."</span><span class="p">)</span>

<span class="c1"># Downloading and unzip a sample whole-slide image</span>
<span class="n">download_data</span><span class="p">(</span>
    <span class="s2">"https://tiatoolbox.dcs.warwick.ac.uk/sample_wsis/TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F.svs"</span><span class="p">,</span>
    <span class="n">wsi_path</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Download and unzip a sample of the validation set used to train the Kather 100K dataset</span>
<span class="n">download_data</span><span class="p">(</span>
    <span class="s2">"https://tiatoolbox.dcs.warwick.ac.uk/datasets/kather100k-validation-sample.zip"</span><span class="p">,</span>
    <span class="n">patches_path</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">with</span> <span class="n">ZipFile</span><span class="p">(</span><span class="n">patches_path</span><span class="p">,</span> <span class="s2">"r"</span><span class="p">)</span> <span class="k">as</span> <span class="n">zipfile</span><span class="p">:</span>
    <span class="n">zipfile</span><span class="o">.</span><span class="n">extractall</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">global_save_dir</span><span class="p">)</span>

<span class="c1"># Download pretrained model weights for WSI classification using ResNet18 architecture</span>
<span class="n">download_data</span><span class="p">(</span>
    <span class="s2">"https://tiatoolbox.dcs.warwick.ac.uk/models/pc/resnet18-kather100k.pth"</span><span class="p">,</span>
    <span class="n">weights_path</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"Download is complete."</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="reading-the-data">
<h2>Reading the data<a class="headerlink" href="#reading-the-data" title="Permalink to this heading">¶</a></h2>
<p>We create a list of patches and a list of corresponding labels. For
example, the first label in <code class="docutils literal notranslate"><span class="pre">label_list</span></code> will indicate the class of
the first image patch in <code class="docutils literal notranslate"><span class="pre">patch_list</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Read the patch data and create a list of patches and a list of corresponding labels</span>
<span class="n">dataset_path</span> <span class="o">=</span> <span class="n">global_save_dir</span> <span class="o">/</span> <span class="s2">"kather100k-validation-sample"</span>

<span class="c1"># Set the path to the dataset</span>
<span class="n">image_ext</span> <span class="o">=</span> <span class="s2">".tif"</span>  <span class="c1"># file extension of each image</span>

<span class="c1"># Obtain the mapping between the label ID and the class name</span>
<span class="n">label_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"BACK"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="c1"># Background (empty glass region)</span>
    <span class="s2">"NORM"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="c1"># Normal colon mucosa</span>
    <span class="s2">"DEB"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>  <span class="c1"># Debris</span>
    <span class="s2">"TUM"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>  <span class="c1"># Colorectal adenocarcinoma epithelium</span>
    <span class="s2">"ADI"</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>  <span class="c1"># Adipose</span>
    <span class="s2">"MUC"</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>  <span class="c1"># Mucus</span>
    <span class="s2">"MUS"</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>  <span class="c1"># Smooth muscle</span>
    <span class="s2">"STR"</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span>  <span class="c1"># Cancer-associated stroma</span>
    <span class="s2">"LYM"</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>  <span class="c1"># Lymphocytes</span>
<span class="p">}</span>

<span class="n">class_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">label_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="n">class_labels</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">label_dict</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>

<span class="c1"># Generate a list of patches and generate the label from the filename</span>
<span class="n">patch_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">label_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">class_name</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">label_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">dataset_class_path</span> <span class="o">=</span> <span class="n">dataset_path</span> <span class="o">/</span> <span class="n">class_name</span>
    <span class="n">patch_list_single_class</span> <span class="o">=</span> <span class="n">grab_files_from_dir</span><span class="p">(</span>
        <span class="n">dataset_class_path</span><span class="p">,</span>
        <span class="n">file_types</span><span class="o">=</span><span class="s2">"*"</span> <span class="o">+</span> <span class="n">image_ext</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">patch_list</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">patch_list_single_class</span><span class="p">)</span>
    <span class="n">label_list</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">label</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">patch_list_single_class</span><span class="p">))</span>

<span class="c1"># Show some dataset statistics</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">class_names</span><span class="p">,</span> <span class="p">[</span><span class="n">label_list</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">label</span><span class="p">)</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">class_labels</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">"Patch types"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">"Number of patches"</span><span class="p">)</span>

<span class="c1"># Count the number of examples per class</span>
<span class="k">for</span> <span class="n">class_name</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">label_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
        <span class="s2">"Class ID: </span><span class="si">%d</span><span class="s2"> -- Class Name: </span><span class="si">%s</span><span class="s2"> -- Number of images: </span><span class="si">%d</span><span class="s2">"</span><span class="p">,</span>
        <span class="n">label</span><span class="p">,</span>
        <span class="n">class_name</span><span class="p">,</span>
        <span class="n">label_list</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">label</span><span class="p">),</span>
    <span class="p">)</span>

<span class="c1"># Overall dataset statistics</span>
<span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"Total number of patches: </span><span class="si">%d</span><span class="s2">"</span><span class="p">,</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">patch_list</span><span class="p">)))</span>
</pre></div>
</div>
<img alt="tiatoolbox tutorial" class="sphx-glr-single-img" src="../_images/tiatoolbox_tutorial_001.png" srcset="../_images/tiatoolbox_tutorial_001.png"/><div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>|2023-11-14|13:15:59.299| [INFO] Class ID: 0 -- Class Name: BACK -- Number of images: 211
|2023-11-14|13:15:59.299| [INFO] Class ID: 1 -- Class Name: NORM -- Number of images: 176
|2023-11-14|13:15:59.299| [INFO] Class ID: 2 -- Class Name: DEB -- Number of images: 230
|2023-11-14|13:15:59.299| [INFO] Class ID: 3 -- Class Name: TUM -- Number of images: 286
|2023-11-14|13:15:59.299| [INFO] Class ID: 4 -- Class Name: ADI -- Number of images: 208
|2023-11-14|13:15:59.299| [INFO] Class ID: 5 -- Class Name: MUC -- Number of images: 178
|2023-11-14|13:15:59.299| [INFO] Class ID: 6 -- Class Name: MUS -- Number of images: 270
|2023-11-14|13:15:59.299| [INFO] Class ID: 7 -- Class Name: STR -- Number of images: 209
|2023-11-14|13:15:59.299| [INFO] Class ID: 8 -- Class Name: LYM -- Number of images: 232
|2023-11-14|13:15:59.299| [INFO] Total number of patches: 2000
</pre></div>
</div>
<p>As you can see for this patch dataset, we have 9 classes/labels with IDs
0-8 and associated class names. describing the dominant tissue type in
the patch:</p>
<ul class="simple">
<li><p>BACK ⟶ Background (empty glass region)</p></li>
<li><p>LYM ⟶ Lymphocytes</p></li>
<li><p>NORM ⟶ Normal colon mucosa</p></li>
<li><p>DEB ⟶ Debris</p></li>
<li><p>MUS ⟶ Smooth muscle</p></li>
<li><p>STR ⟶ Cancer-associated stroma</p></li>
<li><p>ADI ⟶ Adipose</p></li>
<li><p>MUC ⟶ Mucus</p></li>
<li><p>TUM ⟶ Colorectal adenocarcinoma epithelium</p></li>
</ul>
</div>
<div class="section" id="classify-image-patches">
<h2>Classify image patches<a class="headerlink" href="#classify-image-patches" title="Permalink to this heading">¶</a></h2>
<p>We demonstrate how to obtain a prediction for each patch within a
digital slide first with the <code class="docutils literal notranslate"><span class="pre">patch</span></code> mode and then with a large slide
using <code class="docutils literal notranslate"><span class="pre">wsi</span></code> mode.</p>
<div class="section" id="define-patchpredictor-model">
<h3>Define <code class="docutils literal notranslate"><span class="pre">PatchPredictor</span></code> model<a class="headerlink" href="#define-patchpredictor-model" title="Permalink to this heading">¶</a></h3>
<p>The PatchPredictor class runs a CNN-based classifier written in PyTorch.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model</span></code> can be any trained PyTorch model with the constraint that
it should follow the
<code class="docutils literal notranslate"><span class="pre">tiatoolbox.models.abc.ModelABC</span></code> <cite>(docs)</cite> &lt;<a class="reference external" href="https://tia-toolbox.readthedocs.io/en/latest/_autosummary/tiatoolbox.models.models_abc.ModelABC.html">https://tia-toolbox.readthedocs.io/en/latest/_autosummary/tiatoolbox.models.models_abc.ModelABC.html</a>&gt;`__
class structure. For more information on this matter, please refer to
<a class="reference external" href="https://github.com/TissueImageAnalytics/tiatoolbox/blob/develop/examples/07-advanced-modeling.ipynb">our example notebook on advanced model
techniques</a>.
In order to load a custom model, you need to write a small
preprocessing function, as in <code class="docutils literal notranslate"><span class="pre">preproc_func(img)</span></code>, which makes sure
the input tensors are in the right format for the loaded network.</p></li>
<li><p>Alternatively, you can pass <code class="docutils literal notranslate"><span class="pre">pretrained_model</span></code> as a string
argument. This specifies the CNN model that performs the prediction,
and it must be one of the models listed
<a class="reference external" href="https://tia-toolbox.readthedocs.io/en/latest/usage.html?highlight=pretrained%20models#tiatoolbox.models.architecture.get_pretrained_model">here</a>.
The command will look like this:
<code class="docutils literal notranslate"><span class="pre">predictor</span> <span class="pre">=</span> <span class="pre">PatchPredictor(pretrained_model='resnet18-kather100k',</span> <span class="pre">pretrained_weights=weights_path,</span> <span class="pre">batch_size=32)</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pretrained_weights</span></code>: When using a <code class="docutils literal notranslate"><span class="pre">pretrained_model</span></code>, the
corresponding pretrained weights will also be downloaded by default.
You can override the default with your own set of weights via the
<code class="docutils literal notranslate"><span class="pre">pretrained_weight</span></code> argument.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">batch_size</span></code>: Number of images fed into the model each time. Higher
values for this parameter require a larger (GPU) memory capacity.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Importing a pretrained PyTorch model from TIAToolbox</span>
<span class="n">predictor</span> <span class="o">=</span> <span class="n">PatchPredictor</span><span class="p">(</span><span class="n">pretrained_model</span><span class="o">=</span><span class="s1">'resnet18-kather100k'</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>

<span class="c1"># Users can load any PyTorch model architecture instead using the following script</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">vanilla</span><span class="o">.</span><span class="n">CNNModel</span><span class="p">(</span><span class="n">backbone</span><span class="o">=</span><span class="s2">"resnet18"</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span> <span class="c1"># Importing model from torchvision.models.resnet18</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">weights_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s2">"cpu"</span><span class="p">),</span> <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">preproc_func</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">PIL</span><span class="o">.</span><span class="n">Image</span><span class="o">.</span><span class="n">fromarray</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()(</span><span class="n">img</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">img</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">preproc_func</span> <span class="o">=</span> <span class="n">preproc_func</span>
<span class="n">predictor</span> <span class="o">=</span> <span class="n">PatchPredictor</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="predict-patch-labels">
<h3>Predict patch labels<a class="headerlink" href="#predict-patch-labels" title="Permalink to this heading">¶</a></h3>
<p>We create a predictor object and then call the <code class="docutils literal notranslate"><span class="pre">predict</span></code> method using
the <code class="docutils literal notranslate"><span class="pre">patch</span></code> mode. We then compute the classification accuracy and
confusion matrix.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">suppress_console_output</span><span class="p">():</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">imgs</span><span class="o">=</span><span class="n">patch_list</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"patch"</span><span class="p">,</span> <span class="n">on_gpu</span><span class="o">=</span><span class="n">ON_GPU</span><span class="p">)</span>

<span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">label_list</span><span class="p">,</span> <span class="n">output</span><span class="p">[</span><span class="s2">"predictions"</span><span class="p">])</span>
<span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">"Classification accuracy: </span><span class="si">%f</span><span class="s2">"</span><span class="p">,</span> <span class="n">acc</span><span class="p">)</span>

<span class="c1"># Creating and visualizing the confusion matrix for patch classification results</span>
<span class="n">conf</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">label_list</span><span class="p">,</span> <span class="n">output</span><span class="p">[</span><span class="s2">"predictions"</span><span class="p">],</span> <span class="n">normalize</span><span class="o">=</span><span class="s2">"true"</span><span class="p">)</span>
<span class="n">df_cm</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">class_names</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">class_names</span><span class="p">)</span>
<span class="n">df_cm</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>|2023-11-14|13:16:03.215| [INFO] Classification accuracy: 0.993000
</pre></div>
</div>
<div class="output_subarea output_html rendered_html output_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>BACK</th>
<th>NORM</th>
<th>DEB</th>
<th>TUM</th>
<th>ADI</th>
<th>MUC</th>
<th>MUS</th>
<th>STR</th>
<th>LYM</th>
</tr>
</thead>
<tbody>
<tr>
<th>BACK</th>
<td>1.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.00000</td>
</tr>
<tr>
<th>NORM</th>
<td>0.000000</td>
<td>0.988636</td>
<td>0.000000</td>
<td>0.011364</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.00000</td>
</tr>
<tr>
<th>DEB</th>
<td>0.000000</td>
<td>0.000000</td>
<td>0.991304</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.008696</td>
<td>0.00000</td>
</tr>
<tr>
<th>TUM</th>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.996503</td>
<td>0.000000</td>
<td>0.003497</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.00000</td>
</tr>
<tr>
<th>ADI</th>
<td>0.004808</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.990385</td>
<td>0.000000</td>
<td>0.004808</td>
<td>0.000000</td>
<td>0.00000</td>
</tr>
<tr>
<th>MUC</th>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.988764</td>
<td>0.000000</td>
<td>0.011236</td>
<td>0.00000</td>
</tr>
<tr>
<th>MUS</th>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.996296</td>
<td>0.003704</td>
<td>0.00000</td>
</tr>
<tr>
<th>STR</th>
<td>0.000000</td>
<td>0.000000</td>
<td>0.004785</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.004785</td>
<td>0.004785</td>
<td>0.985646</td>
<td>0.00000</td>
</tr>
<tr>
<th>LYM</th>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.004310</td>
<td>0.99569</td>
</tr>
</tbody>
</table>
</div>
</div>
<br/>
<br/></div>
<div class="section" id="predict-patch-labels-for-a-whole-slide">
<h3>Predict patch labels for a whole slide<a class="headerlink" href="#predict-patch-labels-for-a-whole-slide" title="Permalink to this heading">¶</a></h3>
<p>We now introduce <code class="docutils literal notranslate"><span class="pre">IOPatchPredictorConfig</span></code>, a class that specifies the
configuration of image reading and prediction writing for the model
prediction engine. This is required to inform the classifier which level
of the WSI pyramid the classifier should read, process data and generate
output.</p>
<p>Parameters of <code class="docutils literal notranslate"><span class="pre">IOPatchPredictorConfig</span></code> are defined as:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">input_resolutions</span></code>: A list, in the form of a dictionary,
specifying the resolution of each input. List elements must be in the
same order as in the target <code class="docutils literal notranslate"><span class="pre">model.forward()</span></code>. If your model
accepts only one input, you just need to put one dictionary
specifying <code class="docutils literal notranslate"><span class="pre">'units'</span></code> and <code class="docutils literal notranslate"><span class="pre">'resolution'</span></code>. Note that TIAToolbox
supports a model with more than one input. For more information on
units and resolution, please see <a class="reference external" href="https://tia-toolbox.readthedocs.io/en/latest/_autosummary/tiatoolbox.wsicore.wsireader.WSIReader.html#tiatoolbox.wsicore.wsireader.WSIReader.read_rect">TIAToolbox
documentation</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">patch_input_shape</span></code>: Shape of the largest input in (height, width)
format.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">stride_shape</span></code>: The size of a stride (steps) between two
consecutive patches, used in the patch extraction process. If the
user sets <code class="docutils literal notranslate"><span class="pre">stride_shape</span></code> equal to <code class="docutils literal notranslate"><span class="pre">patch_input_shape</span></code>, patches
will be extracted and processed without any overlap.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">wsi_ioconfig</span> <span class="o">=</span> <span class="n">IOPatchPredictorConfig</span><span class="p">(</span>
    <span class="n">input_resolutions</span><span class="o">=</span><span class="p">[{</span><span class="s2">"units"</span><span class="p">:</span> <span class="s2">"mpp"</span><span class="p">,</span> <span class="s2">"resolution"</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">}],</span>
    <span class="n">patch_input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">],</span>
    <span class="n">stride_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">predict</span></code> method applies the CNN on the input patches and get the
results. Here are the arguments and their descriptions:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mode</span></code>: Type of input to be processed. Choose from <code class="docutils literal notranslate"><span class="pre">patch</span></code>,
<code class="docutils literal notranslate"><span class="pre">tile</span></code> or <code class="docutils literal notranslate"><span class="pre">wsi</span></code> according to your application.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">imgs</span></code>: List of inputs, which should be a list of paths to the
input tiles or WSIs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">return_probabilities</span></code>: Set to <strong>True</strong> to get per class
probabilities alongside predicted labels of input patches. If you
wish to merge the predictions to generate prediction maps for
<code class="docutils literal notranslate"><span class="pre">tile</span></code> or <code class="docutils literal notranslate"><span class="pre">wsi</span></code> modes, you can set <code class="docutils literal notranslate"><span class="pre">return_probabilities=True</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ioconfig</span></code>: set the IO configuration information using the
<code class="docutils literal notranslate"><span class="pre">IOPatchPredictorConfig</span></code> class.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">resolution</span></code> and <code class="docutils literal notranslate"><span class="pre">unit</span></code> (not shown below): These arguments
specify the level or micron-per-pixel resolution of the WSI levels
from which we plan to extract patches and can be used instead of
<code class="docutils literal notranslate"><span class="pre">ioconfig</span></code>. Here we specify the WSI level as <code class="docutils literal notranslate"><span class="pre">'baseline'</span></code>,
which is equivalent to level 0. In general, this is the level of
greatest resolution. In this particular case, the image has only one
level. More information can be found in the
<a class="reference external" href="https://tia-toolbox.readthedocs.io/en/latest/usage.html?highlight=WSIReader.read_rect#tiatoolbox.wsicore.wsireader.WSIReader.read_rect">documentation</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">masks</span></code>: A list of paths corresponding to the masks of WSIs in the
<code class="docutils literal notranslate"><span class="pre">imgs</span></code> list. These masks specify the regions in the original WSIs
from which we want to extract patches. If the mask of a particular
WSI is specified as <code class="docutils literal notranslate"><span class="pre">None</span></code>, then the labels for all patches of that
WSI (even background regions) would be predicted. This could cause
unnecessary computation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">merge_predictions</span></code>: You can set this parameter to <code class="docutils literal notranslate"><span class="pre">True</span></code> if it’s
required to generate a 2D map of patch classification results.
However, for large WSIs this will require large available memory. An
alternative (default) solution is to set <code class="docutils literal notranslate"><span class="pre">merge_predictions=False</span></code>,
and then generate the 2D prediction maps using the
<code class="docutils literal notranslate"><span class="pre">merge_predictions</span></code> function as you will see later on.</p></li>
</ul>
<p>Since we are using a large WSI the patch extraction and prediction
processes may take some time (make sure to set the <code class="docutils literal notranslate"><span class="pre">ON_GPU=True</span></code> if
you have access to Cuda enabled GPU and PyTorch+Cuda).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">suppress_console_output</span><span class="p">():</span>
    <span class="n">wsi_output</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span>
        <span class="n">imgs</span><span class="o">=</span><span class="p">[</span><span class="n">wsi_path</span><span class="p">],</span>
        <span class="n">masks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s2">"wsi"</span><span class="p">,</span>
        <span class="n">merge_predictions</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">ioconfig</span><span class="o">=</span><span class="n">wsi_ioconfig</span><span class="p">,</span>
        <span class="n">return_probabilities</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">save_dir</span><span class="o">=</span><span class="n">global_save_dir</span> <span class="o">/</span> <span class="s2">"wsi_predictions"</span><span class="p">,</span>
        <span class="n">on_gpu</span><span class="o">=</span><span class="n">ON_GPU</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>We see how the prediction model works on our whole-slide images by
visualizing the <code class="docutils literal notranslate"><span class="pre">wsi_output</span></code>. We first need to merge patch prediction
outputs and then visualize them as an overlay on the original image. As
before, the <code class="docutils literal notranslate"><span class="pre">merge_predictions</span></code> method is used to merge the patch
predictions. Here we set the parameters
<code class="docutils literal notranslate"><span class="pre">resolution=1.25,</span> <span class="pre">units='power'</span></code> to generate the prediction map at
1.25x magnification. If you would like to have higher/lower resolution
(bigger/smaller) prediction maps, you need to change these parameters
accordingly. When the predictions are merged, use the
<code class="docutils literal notranslate"><span class="pre">overlay_patch_prediction</span></code> function to overlay the prediction map on
the WSI thumbnail, which should be extracted at the resolution used for
prediction merging.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">overview_resolution</span> <span class="o">=</span> <span class="p">(</span>
    <span class="mi">4</span>  <span class="c1"># the resolution in which we desire to merge and visualize the patch predictions</span>
<span class="p">)</span>
<span class="c1"># the unit of the `resolution` parameter. Can be "power", "level", "mpp", or "baseline"</span>
<span class="n">overview_unit</span> <span class="o">=</span> <span class="s2">"mpp"</span>
<span class="n">wsi</span> <span class="o">=</span> <span class="n">WSIReader</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">wsi_path</span><span class="p">)</span>
<span class="n">wsi_overview</span> <span class="o">=</span> <span class="n">wsi</span><span class="o">.</span><span class="n">slide_thumbnail</span><span class="p">(</span><span class="n">resolution</span><span class="o">=</span><span class="n">overview_resolution</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="n">overview_unit</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(),</span> <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">wsi_overview</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">"off"</span><span class="p">)</span>
</pre></div>
</div>
<img alt="tiatoolbox tutorial" class="sphx-glr-single-img" src="../_images/tiatoolbox_tutorial_002.png" srcset="../_images/tiatoolbox_tutorial_002.png"/><p>Overlaying the prediction map on this image as below gives:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualization of whole-slide image patch-level prediction</span>
<span class="c1"># first set up a label to color mapping</span>
<span class="n">label_color_dict</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">label_color_dict</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"empty"</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="n">colors</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s2">"Set1"</span><span class="p">)</span><span class="o">.</span><span class="n">colors</span>
<span class="k">for</span> <span class="n">class_name</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">label_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">label_color_dict</span><span class="p">[</span><span class="n">label</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">class_name</span><span class="p">,</span> <span class="mi">255</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">colors</span><span class="p">[</span><span class="n">label</span><span class="p">]))</span>

<span class="n">pred_map</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">merge_predictions</span><span class="p">(</span>
    <span class="n">wsi_path</span><span class="p">,</span>
    <span class="n">wsi_output</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">resolution</span><span class="o">=</span><span class="n">overview_resolution</span><span class="p">,</span>
    <span class="n">units</span><span class="o">=</span><span class="n">overview_unit</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">overlay</span> <span class="o">=</span> <span class="n">overlay_prediction_mask</span><span class="p">(</span>
    <span class="n">wsi_overview</span><span class="p">,</span>
    <span class="n">pred_map</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">label_info</span><span class="o">=</span><span class="n">label_color_dict</span><span class="p">,</span>
    <span class="n">return_ax</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="tiatoolbox tutorial" class="sphx-glr-single-img" src="../_images/tiatoolbox_tutorial_003.png" srcset="../_images/tiatoolbox_tutorial_003.png"/></div>
</div>
<div class="section" id="feature-extraction-with-a-pathology-specific-model">
<h2>Feature extraction with a pathology-specific model<a class="headerlink" href="#feature-extraction-with-a-pathology-specific-model" title="Permalink to this heading">¶</a></h2>
<p>In this section, we will show how to extract features from a pretrained
PyTorch model that exists outside TIAToolbox, using the WSI inference
engines provided by TIAToolbox. To illustrate this we will use
HistoEncoder, a computational-pathology specific model that has been
trained in a self-supervised fashion to extract features from histology
images. The model has been made available here:</p>
<p>‘HistoEncoder: Foundation models for digital pathology’
(<a class="reference external" href="https://github.com/jopo666/HistoEncoder">https://github.com/jopo666/HistoEncoder</a>) by Pohjonen, Joona and team at
the University of Helsinki.</p>
<p>We will plot a umap reduction into 3D (RGB) of the feature map to
visualize how the features capture the differences between some of the
above mentioned tissue types.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import some extra modules</span>
<span class="kn">import</span> <span class="nn">histoencoder.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="kn">from</span> <span class="nn">tiatoolbox.models.engine.semantic_segmentor</span> <span class="kn">import</span> <span class="n">DeepFeatureExtractor</span><span class="p">,</span> <span class="n">IOSegmentorConfig</span>
<span class="kn">from</span> <span class="nn">tiatoolbox.models.models_abc</span> <span class="kn">import</span> <span class="n">ModelABC</span>
<span class="kn">import</span> <span class="nn">umap</span>
</pre></div>
</div>
<p>TIAToolbox defines a ModelABC which is a class inheriting PyTorch
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html">nn.Module</a>
and specifies how a model should look in order to be used in the
TIAToolbox inference engines. The histoencoder model doesn’t follow this
structure, so we need to wrap it in a class whose output and methods are
those that the TIAToolbox engine expects.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">HistoEncWrapper</span><span class="p">(</span><span class="n">ModelABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Wrapper for HistoEnc model that conforms to tiatoolbox ModelABC interface."""</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">HistoEncWrapper</span><span class="p">,</span> <span class="n">encoder</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feat_extract</span> <span class="o">=</span> <span class="n">encoder</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">HistoEncWrapper</span><span class="p">,</span> <span class="n">imgs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">"""Pass input data through the model.</span>

<span class="sd">        Args:</span>
<span class="sd">            imgs (torch.Tensor):</span>
<span class="sd">                Model input.</span>

<span class="sd">        """</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">extract_features</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feat_extract</span><span class="p">,</span> <span class="n">imgs</span><span class="p">,</span> <span class="n">num_blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">avg_pool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">infer_batch</span><span class="p">(</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">batch_data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">on_gpu</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""Run inference on an input batch.</span>

<span class="sd">        Contains logic for forward operation as well as i/o aggregation.</span>

<span class="sd">        Args:</span>
<span class="sd">            model (nn.Module):</span>
<span class="sd">                PyTorch defined model.</span>
<span class="sd">            batch_data (torch.Tensor):</span>
<span class="sd">                A batch of data generated by</span>
<span class="sd">                `torch.utils.data.DataLoader`.</span>
<span class="sd">            on_gpu (bool):</span>
<span class="sd">                Whether to run inference on a GPU.</span>

<span class="sd">        """</span>
        <span class="n">img_patches_device</span> <span class="o">=</span> <span class="n">batch_data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">'cuda'</span><span class="p">)</span> <span class="k">if</span> <span class="n">on_gpu</span> <span class="k">else</span> <span class="n">batch_data</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="c1"># Do not compute the gradient (not training)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">img_patches_device</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">output</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]</span>
</pre></div>
</div>
<p>Now that we have our wrapper, we will create our feature extraction
model and instantiate a
<a class="reference external" href="https://tia-toolbox.readthedocs.io/en/v1.4.1/_autosummary/tiatoolbox.models.engine.semantic_segmentor.DeepFeatureExtractor.html">DeepFeatureExtractor</a>
to allow us to use this model over a WSI. We will use the same WSI as
above, but this time we will extract features from the patches of the
WSI using the HistoEncoder model, rather than predicting some label for
each patch.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># create the model</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">create_encoder</span><span class="p">(</span><span class="s2">"prostate_medium"</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HistoEncWrapper</span><span class="p">(</span><span class="n">encoder</span><span class="p">)</span>

<span class="c1"># set the pre-processing function</span>
<span class="n">norm</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="p">[</span><span class="mf">0.662</span><span class="p">,</span> <span class="mf">0.446</span><span class="p">,</span> <span class="mf">0.605</span><span class="p">],</span><span class="n">std</span><span class="o">=</span><span class="p">[</span><span class="mf">0.169</span><span class="p">,</span> <span class="mf">0.190</span><span class="p">,</span> <span class="mf">0.155</span><span class="p">])</span>
<span class="n">trans</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
    <span class="n">norm</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">model</span><span class="o">.</span><span class="n">preproc_func</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">(</span><span class="n">trans</span><span class="p">)</span>

<span class="n">wsi_ioconfig</span> <span class="o">=</span> <span class="n">IOSegmentorConfig</span><span class="p">(</span>
    <span class="n">input_resolutions</span><span class="o">=</span><span class="p">[{</span><span class="s2">"units"</span><span class="p">:</span> <span class="s2">"mpp"</span><span class="p">,</span> <span class="s2">"resolution"</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">}],</span>
    <span class="n">patch_input_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">],</span>
    <span class="n">output_resolutions</span><span class="o">=</span><span class="p">[{</span><span class="s2">"units"</span><span class="p">:</span> <span class="s2">"mpp"</span><span class="p">,</span> <span class="s2">"resolution"</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">}],</span>
    <span class="n">patch_output_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">],</span>
    <span class="n">stride_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
<p>When we create the <code class="docutils literal notranslate"><span class="pre">DeepFeatureExtractor</span></code>, we will pass the
<code class="docutils literal notranslate"><span class="pre">auto_generate_mask=True</span></code> argument. This will automatically create a
mask of the tissue region using otsu thresholding, so that the extractor
processes only those patches containing tissue.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># create the feature extractor and run it on the WSI</span>
<span class="n">extractor</span> <span class="o">=</span> <span class="n">DeepFeatureExtractor</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">auto_generate_mask</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">num_loader_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">num_postproc_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="k">with</span> <span class="n">suppress_console_output</span><span class="p">():</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">extractor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">imgs</span><span class="o">=</span><span class="p">[</span><span class="n">wsi_path</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s2">"wsi"</span><span class="p">,</span> <span class="n">ioconfig</span><span class="o">=</span><span class="n">wsi_ioconfig</span><span class="p">,</span> <span class="n">save_dir</span><span class="o">=</span><span class="n">global_save_dir</span> <span class="o">/</span> <span class="s2">"wsi_features"</span><span class="p">,)</span>
</pre></div>
</div>
<p>These features could be used to train a downstream model, but here in
order to get some intuition for what the features represent, we will use
a UMAP reduction to visualize the features in RGB space. The points
labeled in a similar color should have similar features, so we can check
if the features naturally separate out into the different tissue regions
when we overlay the UMAP reduction on the WSI thumbnail. We will plot it
along with the patch-level prediction map from above to see how the
features compare to the patch-level predictions in the following cells.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># First we define a function to calculate the umap reduction</span>
<span class="k">def</span> <span class="nf">umap_reducer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">nns</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""UMAP reduction of the input data."""</span>
    <span class="n">reducer</span> <span class="o">=</span> <span class="n">umap</span><span class="o">.</span><span class="n">UMAP</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">nns</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="n">dims</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s2">"manhattan"</span><span class="p">,</span> <span class="n">spread</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">reduced</span> <span class="o">=</span> <span class="n">reducer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">reduced</span> <span class="o">-=</span> <span class="n">reduced</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">reduced</span> <span class="o">/=</span> <span class="n">reduced</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reduced</span>

<span class="c1"># load the features output by our feature extractor</span>
<span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">global_save_dir</span> <span class="o">/</span> <span class="s2">"wsi_features"</span> <span class="o">/</span> <span class="s2">"0.position.npy"</span><span class="p">)</span>
<span class="n">feats</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">global_save_dir</span> <span class="o">/</span> <span class="s2">"wsi_features"</span> <span class="o">/</span> <span class="s2">"0.features.0.npy"</span><span class="p">)</span>
<span class="n">pos</span> <span class="o">=</span> <span class="n">pos</span> <span class="o">/</span> <span class="mi">8</span> <span class="c1"># as we extracted at 0.5mpp, and we are overlaying on a thumbnail at 4mpp</span>

<span class="c1"># reduce the features into 3 dimensional (rgb) space</span>
<span class="n">reduced</span> <span class="o">=</span> <span class="n">umap_reducer</span><span class="p">(</span><span class="n">feats</span><span class="p">)</span>

<span class="c1"># plot the prediction map the classifier again</span>
<span class="n">overlay</span> <span class="o">=</span> <span class="n">overlay_prediction_mask</span><span class="p">(</span>
    <span class="n">wsi_overview</span><span class="p">,</span>
    <span class="n">pred_map</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">label_info</span><span class="o">=</span><span class="n">label_color_dict</span><span class="p">,</span>
    <span class="n">return_ax</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># plot the feature map reduction</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">wsi_overview</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">pos</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">pos</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">reduced</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">"off"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">"UMAP reduction of HistoEnc features"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<ul class="sphx-glr-horizontal">
<li><img alt="tiatoolbox tutorial" class="sphx-glr-multi-img" src="../_images/tiatoolbox_tutorial_004.png" srcset="../_images/tiatoolbox_tutorial_004.png"/></li>
<li><img alt="UMAP reduction of HistoEnc features" class="sphx-glr-multi-img" src="../_images/tiatoolbox_tutorial_005.png" srcset="../_images/tiatoolbox_tutorial_005.png"/></li>
</ul>
<p>We see that the prediction map from our patch-level predictor, and the
feature map from our self-supervised feature encoder, capture similar
information about the tissue types in the WSI. This is a good sanity
check that our models are working as expected. It also shows that the
features extracted by the HistoEncoder model are capturing the
differences between the tissue types, and so that they are encoding
histologically relevant information.</p>
</div>
<div class="section" id="where-to-go-from-here">
<h2>Where to Go From Here<a class="headerlink" href="#where-to-go-from-here" title="Permalink to this heading">¶</a></h2>
<p>In this notebook, we show how we can use the <code class="docutils literal notranslate"><span class="pre">PatchPredictor</span></code> and
<code class="docutils literal notranslate"><span class="pre">DeepFeatureExtractor</span></code> classes and their <code class="docutils literal notranslate"><span class="pre">predict</span></code> method to predict
the label, or extract features, for patches of big tiles and WSIs. We
introduce <code class="docutils literal notranslate"><span class="pre">merge_predictions</span></code> and <code class="docutils literal notranslate"><span class="pre">overlay_prediction_mask</span></code> helper
functions that merge the patch prediction outputs and visualize the
resulting prediction map as an overlay on the input image/WSI.</p>
<p>All the processes take place within TIAToolbox and we can easily put the
pieces together, following our example code. Please make sure to set
inputs and options correctly. We encourage you to further investigate
the effect on the prediction output of changing <code class="docutils literal notranslate"><span class="pre">predict</span></code> function
parameters. We have demonstrated how to use your own pretrained model or
one provided by the research community for a specific task in the
TIAToolbox framework to do inference on large WSIs even if the model
structure is not defined in the TIAToolbox model class.</p>
<p>You can learn more through the following resources:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://tia-toolbox.readthedocs.io/en/latest/_notebooks/jnb/07-advanced-modeling.html">Advanced model handling with PyTorch and
TIAToolbox</a></p></li>
<li><p><a class="reference external" href="https://tia-toolbox.readthedocs.io/en/latest/_notebooks/jnb/full-pipelines/slide-graph.html">Creating slide graphs for WSI with a custom PyTorch graph neural
network</a></p></li>
</ul>
</div>
</div>
</article>
</div>
<footer>
<div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
<a accesskey="n" class="btn btn-neutral float-right" href="../beginner/audio_io_tutorial.html" rel="next" title="Audio I/O">Next <img class="next-page" src="../_static/images/chevron-right-orange.svg"/></a>
<a accesskey="p" class="btn btn-neutral" href="../beginner/vt_tutorial.html" rel="prev" title="Optimizing Vision Transformer Model for Deployment"><img class="previous-page" src="../_static/images/chevron-right-orange.svg"/> Previous</a>
</div>
<hr class="rating-hr hr-top"/>
<div class="rating-container">
<div class="rating-prompt">Rate this Tutorial</div>
<div class="stars-outer">
<i class="far fa-star" data-behavior="tutorial-rating" data-count="1" title="1 Star"></i>
<i class="far fa-star" data-behavior="tutorial-rating" data-count="2" title="2 Stars"></i>
<i class="far fa-star" data-behavior="tutorial-rating" data-count="3" title="3 Stars"></i>
<i class="far fa-star" data-behavior="tutorial-rating" data-count="4" title="4 Stars"></i>
<i class="far fa-star" data-behavior="tutorial-rating" data-count="5" title="5 Stars"></i>
</div>
</div>
<hr class="rating-hr hr-bottom">
<div role="contentinfo">
<p>
        © Copyright 2024, PyTorch.

    </p>
</div>
<div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
</hr></footer>
</div>
<script>
if((window.location.href.indexOf("/prototype/")!= -1) && (window.location.href.indexOf("/prototype/prototype_index")< 1))
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-flask" aria-hidden="true">&nbsp</i> This tutorial describes a prototype feature. Prototype features are typically not available as part of binary distributions like PyPI or Conda, except sometimes behind run-time flags, and are at an early stage for feedback and testing.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  } 
</script>
</div>
<div class="pytorch-content-right" id="pytorch-content-right">
<div class="pytorch-right-menu" id="pytorch-right-menu">
<div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
<ul>
<li><a class="reference internal" href="#">Whole Slide Image Classification Using PyTorch and TIAToolbox</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#setting-up-the-environment">Setting up the environment</a><ul>
<li><a class="reference internal" href="#importing-related-libraries">Importing related libraries</a></li>
<li><a class="reference internal" href="#clean-up-before-a-run">Clean-up before a run</a></li>
<li><a class="reference internal" href="#downloading-the-data">Downloading the data</a></li>
</ul>
</li>
<li><a class="reference internal" href="#reading-the-data">Reading the data</a></li>
<li><a class="reference internal" href="#classify-image-patches">Classify image patches</a><ul>
<li><a class="reference internal" href="#define-patchpredictor-model">Define <code class="docutils literal notranslate"><span class="pre">PatchPredictor</span></code> model</a></li>
<li><a class="reference internal" href="#predict-patch-labels">Predict patch labels</a></li>
<li><a class="reference internal" href="#predict-patch-labels-for-a-whole-slide">Predict patch labels for a whole slide</a></li>
</ul>
</li>
<li><a class="reference internal" href="#feature-extraction-with-a-pathology-specific-model">Feature extraction with a pathology-specific model</a></li>
<li><a class="reference internal" href="#where-to-go-from-here">Where to Go From Here</a></li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</section>
</div>
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript"></script>
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/katex.min.js"></script>
<script src="../_static/auto-render.min.js"></script>
<script src="../_static/katex_autorenderer.js"></script>
<script src="../_static/design-tabs.js"></script>
<script src="../_static/js/vendor/popper.min.js" type="text/javascript"></script>
<script src="../_static/js/vendor/bootstrap.min.js" type="text/javascript"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<script>

// Helper function to make it easier to call dataLayer.push() 
function gtag(){window.dataLayer.push(arguments);}

//add microsoft link

if(window.location.href.indexOf("/beginner/basics/")!= -1)
{
  var url="https://docs.microsoft.com/learn/paths/pytorch-fundamentals/?wt.mc_id=aiml-7486-cxa";
  switch(window.location.pathname.split("/").pop().replace('.html',''))
  {
    case"quickstart_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/9-quickstart?WT.mc_id=aiml-7486-cxa";
      break;
    case"tensorqs_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/2-tensors?WT.mc_id=aiml-7486-cxa";
      break;
    case"data_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/3-data?WT.mc_id=aiml-7486-cxa";
      break;
    case"transforms_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/4-transforms?WT.mc_id=aiml-7486-cxa";
      break;
    case"buildmodel_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/5-model?WT.mc_id=aiml-7486-cxa";
      break;
    case"autogradqs_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/6-autograd?WT.mc_id=aiml-7486-cxa";
      break;
    case"optimization_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/7-optimization?WT.mc_id=aiml-7486-cxa";
      break;
    case"saveloadrun_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/8-inference?WT.mc_id=aiml-7486-cxa";
    }
    
    $(".pytorch-call-to-action-links").children().first().before("<a href="+url+' data-behavior="call-to-action-event" data-response="Run in Microsoft Learn" target="_blank"><div id="microsoft-learn-link" style="padding-bottom: 0.625rem;border-bottom: 1px solid #f3f4f7;padding-right: 2.5rem;display: -webkit-box;  display: -ms-flexbox; display: flex; -webkit-box-align: center;-ms-flex-align: center;align-items: center;"><img class="call-to-action-img" src="../../_static/images/microsoft-logo.svg"/><div class="call-to-action-desktop-view">Run in Microsoft Learn</div><div class="call-to-action-mobile-view">Learn</div></div></a>')
  }

  !function(f,b,e,v,n,t,s)
  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
  n.callMethod.apply(n,arguments):n.queue.push(arguments)};
  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
  n.queue=[];t=b.createElement(e);t.async=!0;
  t.src=v;s=b.getElementsByTagName(e)[0];
  s.parentNode.insertBefore(t,s)}(window,document,'script',
  'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');

  $("[data-behavior='call-to-action-event']").on('click', function(){
    fbq('trackCustom', "Download", {
      tutorialTitle: $('h1:first').text(),
      downloadLink: this.href,
      tutorialLink: window.location.href,
      downloadTitle: $(this).attr("data-response")
    });
    gtag('event', 'click', {
      'event_category': $(this).attr("data-response"),
      'event_label': $("h1").first().text(),
      'tutorial_link': window.location.href
    });
   });

   $("[data-behavior='tutorial-rating']").on('click', function(){
    fbq('trackCustom', "Tutorial Rating", {
      tutorialLink: window.location.href,
      tutorialTitle: $('h1:first').text(),
      rating: $(this).attr("data-count")
    });

    gtag('event', 'click', {
      'event_category': 'Tutorial Rating',
      'event_label': $("h1").first().text(),
      'value': $(this).attr("data-count")
    });
   });

   if (location.pathname == "/") {
     $(".rating-container").hide();
     $(".hr-bottom").hide();
   }


</script>
<noscript>
<img height="1" src="https://www.facebook.com/tr?id=243028289693773&amp;ev=PageView
  &amp;noscript=1" width="1">
</img></noscript>
<script type="text/javascript">
  var collapsedSections = ['PyTorch Recipes', 'Learning PyTorch', 'Image and Video', 'Audio', 'Text', 'Backends', 'Reinforcement Learning', 'Deploying PyTorch Models in Production', 'Code Transforms with FX', 'Frontend APIs', 'Extending PyTorch', 'Model Optimization', 'Parallel and Distributed Training', 'Mobile'];
</script>
<img alt="" height="1" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0" style="border-style:none;" width="1">
<!-- Begin Footer -->
<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
<div class="container">
<div class="row">
<div class="col-md-4 text-center">
<h2>Docs</h2>
<p>Access comprehensive developer documentation for PyTorch</p>
<a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
</div>
<div class="col-md-4 text-center">
<h2>Tutorials</h2>
<p>Get in-depth tutorials for beginners and advanced developers</p>
<a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
</div>
<div class="col-md-4 text-center">
<h2>Resources</h2>
<p>Find development resources and get your questions answered</p>
<a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
</div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="container footer-container">
<div class="footer-logo-wrapper">
<a class="footer-logo" href="https://pytorch.org/"></a>
</div>
<div class="footer-links-wrapper">
<div class="footer-links-col">
<ul>
<li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
<li><a href="https://pytorch.org/get-started">Get Started</a></li>
<li><a href="https://pytorch.org/features">Features</a></li>
<li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
<li><a href="https://pytorch.org/blog/">Blog</a></li>
<li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
</ul>
</div>
<div class="footer-links-col">
<ul>
<li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
<li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
<li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
<li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
<li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
<li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
</ul>
</div>
<div class="footer-links-col">
<ul>
<li class="list-title">Stay up to date</li>
<li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
<li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
<li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
<li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
</ul>
</div>
<div class="footer-links-col">
<ul>
<li class="list-title">PyTorch Podcasts</li>
<li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
<li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
<li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
<li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
</ul>
</div>
</div>
<div class="privacy-policy">
<ul>
<li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
<li class="privacy-policy-links">|</li>
<li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
</ul>
</div>
<div class="copyright">
<p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
</div>
</div>
</footer>
<div class="cookie-banner-wrapper">
<div class="container">
<p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
<img class="close-button" src="../_static/images/pytorch-x.svg"/>
</div>
</div>
<!-- End Footer -->
<!-- Begin Mobile Menu -->
<div class="mobile-main-menu">
<div class="container-fluid">
<div class="container">
<div class="mobile-main-menu-header-container">
<a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/"></a>
<a class="main-menu-close-button" data-behavior="close-mobile-menu" href="#"></a>
</div>
</div>
</div>
<div class="mobile-main-menu-links-container">
<div class="main-menu">
<ul>
<li>
<a href="https://pytorch.org/get-started">Get Started</a>
</li>
<li>
<a href="https://pytorch.org/ecosystem">Ecosystem</a>
</li>
<li>
<a href="">Mobile</a>
</li>
<li>
<a href="https://pytorch.org/blog/">Blog</a>
</li>
<li class="active">
<a href="https://pytorch.org/tutorials">Tutorials</a>
</li>
<li class="resources-mobile-menu-title">
            Docs
          </li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
</li>
<li>
<a href="https://pytorch.org/text/stable/index.html">torchtext</a>
</li>
<li>
<a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
</li>
<li>
<a href="https://pytorch.org/torcharrow">torcharrow</a>
</li>
<li>
<a href="https://pytorch.org/data">TorchData</a>
</li>
<li>
<a href="https://pytorch.org/torchrec">TorchRec</a>
</li>
<li>
<a href="https://pytorch.org/serve/">TorchServe</a>
</li>
<li>
<a href="https://pytorch.org/torchx/">TorchX</a>
</li>
<li>
<a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
            Resources
          </li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/features">About</a>
</li>
<li>
<a href="https://pytorch.org/foundation">PyTorch Foundation</a>
</li>
<li>
<a href="https://pytorch.org/#community-module">Community</a>
</li>
<li>
<a href="https://pytorch.org/community-stories">Community Stories</a>
</li>
<li>
<a href="https://pytorch.org/resources">Developer Resources</a>
</li>
<li>
<a href="https://pytorch.org/events">Events</a>
</li>
<li>
<a href="https://discuss.pytorch.org/">Forums</a>
</li>
<li>
<a href="https://pytorch.org/hub">Models (Beta)</a>
</li>
</ul>
<li>
<a href="https://github.com/pytorch/pytorch">Github</a>
</li>
</ul>
</div>
</div>
</div>
<!-- End Mobile Menu -->
<script src="../_static/js/vendor/anchor.min.js" type="text/javascript"></script>
<script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</img></body>
</html>