
<!DOCTYPE html>

<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"> <!--<![endif]-->
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Grokking PyTorch Intel CPU performance from first principles (Part 2) — PyTorch Tutorials 2.2.0+cu121 documentation</title>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<link href="../_static/katex-math.css" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery.css" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-binder.css" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-dataframe.css" rel="stylesheet" type="text/css"/>
<link href="../_static/sg_gallery-rendered-html.css" rel="stylesheet" type="text/css"/>
<link href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/custom.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/custom2.css" rel="stylesheet" type="text/css"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="nvfuser_intro_tutorial.html" rel="next" title="Getting Started - Accelerate Your Scripts with nvFuser"/>
<link href="torchserve_with_ipex.html" rel="prev" title="Grokking PyTorch Intel CPU performance from first principles"/>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
<!-- End Google Tag Manager -->
<script src="../_static/js/modernizr.min.js"></script>
<!-- Preload the theme fonts -->
<link as="font" crossorigin="anonymous" href="../_static/fonts/FreightSans/freight-sans-book.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" rel="preload" type="font/woff2"/>
<!-- Preload the katex fonts -->
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" rel="preload" type="font/woff2"/>
<link crossorigin="anonymous" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" rel="stylesheet"/>
</head>
<div class="container-fluid header-holder tutorials-header" id="header-holder">
<div class="container">
<div class="header-container">
<a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/"></a>
<div class="main-menu">
<ul>
<li>
<a href="https://pytorch.org/get-started">Get Started</a>
</li>
<li>
<a href="https://pytorch.org/ecosystem">Ecosystem</a>
</li>
<li>
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="resource-option with-down-arrow">
                Edge
              </a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/edge">
<span class="dropdown-title">About PyTorch Edge</span>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/executorch">
<span class="dropdown-title">ExecuTorch</span>
</a>
</div>
</div>
</li>
<li>
<a href="https://pytorch.org/blog/">Blog</a>
</li>
<li class="active">
<a href="https://pytorch.org/tutorials">Tutorials</a>
</li>
<li>
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="resource-option with-down-orange-arrow">
                Docs
              </a>
<div class="resources-dropdown-menu">
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
<span class="dropdown-title">PyTorch</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
<span class="dropdown-title">torchaudio</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
<span class="dropdown-title">torchtext</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
<span class="dropdown-title">torchvision</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
<span class="dropdown-title">torcharrow</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
<span class="dropdown-title">TorchData</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
<span class="dropdown-title">TorchRec</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
<span class="dropdown-title">TorchServe</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
<span class="dropdown-title">TorchX</span>
<p></p>
</a>
<a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
<span class="dropdown-title">PyTorch on XLA Devices</span>
<p></p>
</a>
</div>
</div></li>
<li>
<div class="resources-dropdown" data-toggle="resources-dropdown" id="resourcesDropdownButton">
<a class="resource-option with-down-arrow">
                Resources
              </a>
<div class="resources-dropdown-menu">
<a class="nav-dropdown-item" href="https://pytorch.org/features">
<span class="dropdown-title">About</span>
<p>Learn about PyTorch’s features and capabilities</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/foundation">
<span class="dropdown-title">PyTorch Foundation</span>
<p>Learn about the PyTorch foundation</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
<span class="dropdown-title">Community</span>
<p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
<span class="dropdown-title">Community Stories</span>
<p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/resources">
<span class="dropdown-title">Developer Resources</span>
<p>Find resources and get questions answered</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/events">
<span class="dropdown-title">Events</span>
<p>Find events, webinars, and podcasts</p>
</a>
<a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
<span class="dropdown-title">Forums</span>
<p>A place to discuss PyTorch code, issues, install, research</p>
</a>
<a class="nav-dropdown-item" href="https://pytorch.org/hub">
<span class="dropdown-title">Models (Beta)</span>
<p>Discover, publish, and reuse pre-trained models</p>
</a>
</div>
</div>
</li>
<li>
<a href="https://github.com/pytorch/pytorch">GitHub</a>
</li>
</ul>
</div>
<a class="main-menu-open-button" data-behavior="open-mobile-menu" href="#"></a>
</div>
</div>
</div>
<body class="pytorch-body">
<div class="table-of-contents-link-wrapper">
<span>Table of Contents</span>
<a class="toggle-table-of-contents" data-behavior="toggle-table-of-contents" href="#"></a>
</div>
<nav class="pytorch-left-menu" data-toggle="wy-nav-shift" id="pytorch-left-menu">
<div class="pytorch-side-scroll">
<div aria-label="main navigation" class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation">
<div class="pytorch-left-menu-search">
<div class="version">
                  2.2.0+cu121
                </div>
<div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search Tutorials" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<p class="caption" role="heading"><span class="caption-text">PyTorch Recipes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../recipes/recipes_index.html">See All Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prototype/prototype_index.html">See All Prototype Recipes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Introduction to PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/intro.html">Learn the Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/quickstart_tutorial.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/tensorqs_tutorial.html">Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/data_tutorial.html">Datasets &amp; DataLoaders</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/transforms_tutorial.html">Transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/buildmodel_tutorial.html">Build the Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/autogradqs_tutorial.html">Automatic Differentiation with <code class="docutils literal notranslate"><span class="pre">torch.autograd</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/optimization_tutorial.html">Optimizing Model Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/basics/saveloadrun_tutorial.html">Save and Load the Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Introduction to PyTorch on YouTube</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/introyt.html">Introduction to PyTorch - YouTube Series</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/introyt/introyt1_tutorial.html">Introduction to PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/introyt/tensors_deeper_tutorial.html">Introduction to PyTorch Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/introyt/autogradyt_tutorial.html">The Fundamentals of Autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/introyt/modelsyt_tutorial.html">Building Models with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/introyt/tensorboardyt_tutorial.html">PyTorch TensorBoard Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/introyt/trainingyt.html">Training with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/introyt/captumyt.html">Model Understanding with Captum</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Learning PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/deep_learning_60min_blitz.html">Deep Learning with PyTorch: A 60 Minute Blitz</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/pytorch_with_examples.html">Learning PyTorch with Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/nn_tutorial.html">What is <cite>torch.nn</cite> <em>really</em>?</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard_tutorial.html">Visualizing Models, Data, and Training with TensorBoard</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Image and Video</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torchvision_tutorial.html">TorchVision Object Detection Finetuning Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/transfer_learning_tutorial.html">Transfer Learning for Computer Vision Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/fgsm_tutorial.html">Adversarial Example Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/dcgan_faces_tutorial.html">DCGAN Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="spatial_transformer_tutorial.html">Spatial Transformer Networks Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/vt_tutorial.html">Optimizing Vision Transformer Model for Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="tiatoolbox_tutorial.html">Whole Slide Image Classification Using PyTorch and TIAToolbox</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Audio</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_io_tutorial.html">Audio I/O</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_resampling_tutorial.html">Audio Resampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_data_augmentation_tutorial.html">Audio Data Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_feature_extractions_tutorial.html">Audio Feature Extractions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_feature_augmentation_tutorial.html">Audio Feature Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/audio_datasets_tutorial.html">Audio Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="speech_recognition_pipeline_tutorial.html">Speech Recognition with Wav2Vec2</a></li>
<li class="toctree-l1"><a class="reference internal" href="text_to_speech_with_torchaudio.html">Text-to-speech with Tacotron2</a></li>
<li class="toctree-l1"><a class="reference internal" href="forced_alignment_with_torchaudio_tutorial.html">Forced Alignment with Wav2Vec2</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Text</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/transformer_tutorial.html">Language Modeling with <code class="docutils literal notranslate"><span class="pre">nn.Transformer</span></code> and torchtext</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/bettertransformer_tutorial.html">Fast Transformer Inference with Better Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="char_rnn_classification_tutorial.html">NLP From Scratch: Classifying Names with a Character-Level RNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="char_rnn_generation_tutorial.html">NLP From Scratch: Generating Names with a Character-Level RNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="seq2seq_translation_tutorial.html">NLP From Scratch: Translation with a Sequence to Sequence Network and Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/text_sentiment_ngrams_tutorial.html">Text classification with the torchtext library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/translation_transformer.html">Language Translation with <code class="docutils literal notranslate"><span class="pre">nn.Transformer</span></code> and torchtext</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/torchtext_custom_dataset_tutorial.html">Preprocess custom text dataset using Torchtext</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Backends</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/onnx/intro_onnx.html">Introduction to ONNX</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reinforcement Learning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="reinforcement_q_learning.html">Reinforcement Learning (DQN) Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="reinforcement_ppo.html">Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="mario_rl_tutorial.html">Train a Mario-playing RL Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/pendulum.html">Pendulum: Writing your environment and transforms with TorchRL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deploying PyTorch Models in Production</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/onnx/intro_onnx.html">Introduction to ONNX</a></li>
<li class="toctree-l1"><a class="reference internal" href="flask_rest_api_tutorial.html">Deploying PyTorch in Python via a REST API with Flask</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/Intro_to_TorchScript_tutorial.html">Introduction to TorchScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_export.html">Loading a TorchScript Model in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/super_resolution_with_onnxruntime.html">(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="realtime_rpi.html">Real Time Inference on Raspberry Pi 4 (30 fps!)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Profiling PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/profiler.html">Profiling your PyTorch Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/hta_intro_tutorial.html">Introduction to Holistic Trace Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/hta_trace_diff_tutorial.html">Trace Diff using Holistic Trace Analysis</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Code Transforms with FX</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="fx_conv_bn_fuser.html">(beta) Building a Convolution/Batch Norm fuser in FX</a></li>
<li class="toctree-l1"><a class="reference internal" href="fx_profiling_tutorial.html">(beta) Building a Simple CPU Performance Profiler with FX</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Frontend APIs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="memory_format_tutorial.html">(beta) Channels Last Memory Format in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="forward_ad_usage.html">Forward-mode Automatic Differentiation (Beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="jacobians_hessians.html">Jacobians, Hessians, hvp, vhp, and more: composing function transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="ensembling.html">Model ensembling</a></li>
<li class="toctree-l1"><a class="reference internal" href="per_sample_grads.html">Per-sample-gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_frontend.html">Using the PyTorch C++ Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/torch-script-parallelism.html">Dynamic Parallelism in TorchScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_autograd.html">Autograd in C++ Frontend</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Extending PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="custom_function_double_backward_tutorial.html">Double Backward with Custom Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom_function_conv_bn_tutorial.html">Fusing Convolution and Batch Norm using Custom Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/cpp_extension.html">Custom C++ and CUDA Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/torch_script_custom_ops.html">Extending TorchScript with Custom C++ Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/torch_script_custom_classes.html">Extending TorchScript with Custom C++ Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/dispatcher.html">Registering a Dispatched Operator in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/extend_dispatcher.html">Extending dispatcher for a new backend in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/privateuseone.html">Facilitating New Backend Integration by PrivateUse1</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Optimization</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../beginner/profiler.html">Profiling your PyTorch Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard_profiler_tutorial.html">PyTorch Profiler With TensorBoard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/hyperparameter_tuning_tutorial.html">Hyperparameter tuning with Ray Tune</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/vt_tutorial.html">Optimizing Vision Transformer Model for Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="parametrizations.html">Parametrizations Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="pruning_tutorial.html">Pruning Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/dynamic_quantization_tutorial.html">(beta) Dynamic Quantization on an LSTM Word Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="dynamic_quantization_bert_tutorial.html">(beta) Dynamic Quantization on BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantized_transfer_learning_tutorial.html">(beta) Quantized Transfer Learning for Computer Vision Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/static_quantization_tutorial.html">(beta) Static Quantization with Eager Mode in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchserve_with_ipex.html">Grokking PyTorch Intel CPU performance from first principles</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Grokking PyTorch Intel CPU performance from first principles (Part 2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="nvfuser_intro_tutorial.html">Getting Started - Accelerate Your Scripts with nvFuser</a></li>
<li class="toctree-l1"><a class="reference internal" href="ax_multiobjective_nas_tutorial.html">Multi-Objective NAS with Ax</a></li>
<li class="toctree-l1"><a class="reference internal" href="torch_compile_tutorial.html">Introduction to <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="inductor_debug_cpu.html">Inductor CPU backend debugging and profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="scaled_dot_product_attention_tutorial.html">(Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="scaled_dot_product_attention_tutorial.html#using-sdpa-with-torch-compile">Using SDPA with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="scaled_dot_product_attention_tutorial.html#conclusion">Conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/knowledge_distillation_tutorial.html">Knowledge Distillation Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Parallel and Distributed Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../distributed/home.html">Distributed and Parallel Training Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/dist_overview.html">PyTorch Distributed Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/ddp_series_intro.html">Distributed Data Parallel in PyTorch - Video Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_parallel_tutorial.html">Single-Machine Model Parallel Best Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddp_tutorial.html">Getting Started with Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="dist_tuto.html">Writing Distributed Applications with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="FSDP_tutorial.html">Getting Started with Fully Sharded Data Parallel(FSDP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="FSDP_adavnced_tutorial.html">Advanced Model Training with Fully Sharded Data Parallel (FSDP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="process_group_cpp_extension_tutorial.html">Customize Process Group Backends Using Cpp Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_tutorial.html">Getting Started with Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_param_server_tutorial.html">Implementing a Parameter Server Using Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="dist_pipeline_parallel_tutorial.html">Distributed Pipeline Parallelism Using RPC</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc_async_execution.html">Implementing Batch RPC Processing Using Asynchronous Executions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/rpc_ddp_tutorial.html">Combining Distributed DataParallel with Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="pipeline_tutorial.html">Training Transformer models using Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/ddp_pipeline.html">Training Transformer models using Distributed Data Parallel and Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/generic_join.html">Distributed Training with Uneven Inputs Using the Join Context Manager</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Mobile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/deeplabv3_on_ios.html">Image Segmentation DeepLabV3 on iOS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../beginner/deeplabv3_on_android.html">Image Segmentation DeepLabV3 on Android</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Recommendation Systems</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="torchrec_tutorial.html">Introduction to TorchRec</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/sharding.html">Exploring TorchRec sharding</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multimodality</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../beginner/flava_finetuning_tutorial.html">TorchMultimodal Tutorial: Finetuning FLAVA</a></li>
</ul>
</div>
</div>
</nav>
<div class="pytorch-container">
<div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
<div class="pytorch-breadcrumbs-wrapper">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="pytorch-breadcrumbs">
<li>
<a href="../index.html">
          
            Tutorials
          
        </a> &gt;
      </li>
<li>Grokking PyTorch Intel CPU performance from first principles (Part 2)</li>
<li class="pytorch-breadcrumbs-aside">
<a href="../_sources/intermediate/torchserve_with_ipex_2.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"/></a>
</li>
</ul>
</div>
</div>
<div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
</div>
<section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
<div class="pytorch-content-left">
<div class="pytorch-call-to-action-links">
<div id="tutorial-type">intermediate/torchserve_with_ipex_2</div>
<div id="google-colab-link">
<img class="call-to-action-img" src="../_static/images/pytorch-colab.svg">
<div class="call-to-action-desktop-view">Run in Google Colab</div>
<div class="call-to-action-mobile-view">Colab</div>
</img></div>
<div id="download-notebook-link">
<img class="call-to-action-notebook-img" src="../_static/images/pytorch-download.svg"/>
<div class="call-to-action-desktop-view">Download Notebook</div>
<div class="call-to-action-mobile-view">Notebook</div>
</div>
<div id="github-view-link">
<img class="call-to-action-img" src="../_static/images/pytorch-github.svg"/>
<div class="call-to-action-desktop-view">View on GitHub</div>
<div class="call-to-action-mobile-view">GitHub</div>
</div>
</div>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe height="0" src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" style="display:none;visibility:hidden" width="0"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<div class="rst-content">
<div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
<div class="section" id="grokking-pytorch-intel-cpu-performance-from-first-principles-part-2">
<h1>Grokking PyTorch Intel CPU performance from first principles (Part 2)<a class="headerlink" href="#grokking-pytorch-intel-cpu-performance-from-first-principles-part-2" title="Permalink to this heading">¶</a></h1>
<p>Authors: <a class="reference external" href="https://github.com/min-jean-cho">Min Jean Cho</a>, <a class="reference external" href="https://github.com/jingxu10">Jing Xu</a>, <a class="reference external" href="https://github.com/msaroufim">Mark Saroufim</a></p>
<p>In the <a class="reference external" href="https://pytorch.org/tutorials/intermediate/torchserve_with_ipex.html">Grokking PyTorch Intel CPU Performance From First Principles</a> tutorial
, we have introduced how to tune CPU runtime configurations, how to profile them, and how to integrate them into <a class="reference external" href="https://github.com/pytorch/serve">TorchServe</a> for optimized CPU performance.</p>
<p>In this tutorial, we will demonstrate boosting performance with memory allocator via the <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/blob/master/docs/tutorials/performance_tuning/launch_script.md">Intel® Extension for PyTorch* Launcher</a>
, and optimized kernels on CPU via <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch">Intel® Extension for PyTorch*</a>
, and apply them to TorchServe showcasing 7.71x throughput speedup for ResNet50 and 2.20x throughput speedup for BERT.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/1.png"><img alt="../_images/1.png" src="../_images/1.png" style="width: 100%;"/></a>
</div>
<div class="section" id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this heading">¶</a></h2>
<p>Throughout this tutorial, we will use <a class="reference external" href="https://www.intel.com/content/www/us/en/develop/documentation/vtune-cookbook/top/methodologies/top-down-microarchitecture-analysis-method.html">Top-down Microarchitecture Analysis (TMA)</a> to profile and show that the Back End Bound (Memory Bound, Core Bound) is often the primary bottleneck for under-optimized or under-tuned deep learning workloads, and demonstrate optimization techniques via Intel® Extension for PyTorch* for improving Back End Bound. We will use  <a class="reference external" href="https://github.com/andikleen/pmu-tools/wiki/toplev-manual">toplev</a>, a tool part of <a class="reference external" href="https://github.com/andikleen/pmu-tools">pmu-tools</a> built on top of <a class="reference external" href="https://man7.org/linux/man-pages/man1/perf.1.html">Linux perf</a>, for TMA.</p>
<p>We will also use <a class="reference external" href="https://github.com/pytorch/pytorch/issues/41001">Intel® VTune™ Profiler’s Instrumentation and Tracing Technology (ITT)</a> to profile at finer granularity.</p>
<div class="section" id="top-down-microarchitecture-analysis-method-tma">
<h3>Top-down Microarchitecture Analysis Method (TMA)<a class="headerlink" href="#top-down-microarchitecture-analysis-method-tma" title="Permalink to this heading">¶</a></h3>
<p>When tuning CPU for optimal performance, it’s useful to know where the bottleneck is. Most CPU cores have on-chip Performance Monitoring Units (PMUs). PMUs are dedicated pieces of logic within a CPU core that count specific hardware events as they occur on the system. Examples of these events may be Cache Misses or Branch Mispredictions. PMUs are used for Top-down Microarchitecture Analysis (TMA) to identify the bottlenecks. TMA consists of hierarchical levels as shown:</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/26.png"><img alt="../_images/26.png" src="../_images/26.png" style="width: 100%;"/></a>
</div>
<p>The top level, level-1, metrics collect <em>Retiring</em>, <em>Bad Speculation</em>, <em>Front End Bound</em>, <em>Back End Bound</em>. The pipeline of CPU can conceptually be simplified and divided into two: the frontend and the backend. The <em>frontend</em> is responsible for fetching the program code and decoding them into low-level hardware operations called micro-ops (uOps). The uOps are then fed to the <em>backend</em> in a process called allocation. Once allocated, the backend is responsible for executing the uOp in an available execution unit. A completion of uOp’s execution is called <em>retirement</em>. In contrast, a <em>bad speculation</em> is when speculatively fetched uOps are canceled before retiring such as in the case of mispredicted branches. Each of these metrics can further be broken down in the subsequent levels to pinpoint the bottleneck.</p>
<div class="section" id="tune-for-the-back-end-bound">
<h4>Tune for the Back End Bound<a class="headerlink" href="#tune-for-the-back-end-bound" title="Permalink to this heading">¶</a></h4>
<p>The majority of untuned deep learning workloads will be Back End Bound. Resolving Back End bound is often resolving sources of latency causing retirement to take longer than necessary. As shown above, Back End Bound has two sub-metrics – Core Bound and Memory Bound.</p>
<p>Memory Bound stalls have causes related to the memory subsystem. For example, last-level cache (LLC or L3 cache) miss causing access to DRAM. Scaling deep learning models often requires significant compute. And high compute utilization requires that data is available when the execution units need it to execute the uOps. This requires prefetching the data and reusing the data in cache instead of fetching that same data multiple times from main memory which causes execution units to be starved while data is being returned. Throughout this tutorial, we wll show that a more efficient memory allocator, operator fusion, memory layout format optimization reduce overhead on Memory Bound with better cache locality.</p>
<p>Core Bound stalls indicate sub-optimal use of available execution units while there are no uncompleted memory accesses. For example, several general matrix-matrix multiplication (GEMM) instructions in a row competing for fused-multiply-add (FMA) or dot-product (DP) execution units could cause Core Bound stalls. Key deep learning kernels, including the DP kernels, have been well optimized by <a class="reference external" href="https://github.com/oneapi-src/oneDNN">oneDNN library</a> (oneAPI Deep Neural Network Library), reducing overhead on Core Bound.</p>
<p>Operations like GEMM, convolution, deconvolution are compute-intensive. While operations like pooling, batch normalization, activation functions like ReLU are memory-bound.</p>
</div>
</div>
<div class="section" id="intel-vtune-profiler-s-instrumentation-and-tracing-technology-itt">
<h3>Intel® VTune™ Profiler’s Instrumentation and Tracing Technology (ITT)<a class="headerlink" href="#intel-vtune-profiler-s-instrumentation-and-tracing-technology-itt" title="Permalink to this heading">¶</a></h3>
<p>The ITT APIs of Intel® VTune Profiler is a useful tool to annotate a region of your workload for tracing to profile and visualize at a finer granularity of your annotation – OP/function/sub-function granularity. By annotating at the granularity of your PyTorch model’s OPs, Intel® VTune Profiler’s ITT enables op-level profiling. Intel® VTune Profiler’s ITT has been integrated into <a class="reference external" href="https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html#autograd-profiler">PyTorch Autograd Profiler</a>. <sup>1</sup></p>
<ol class="arabic simple">
<li><p>The feature has to be explicitly enabled by <em>with torch.autograd.profiler.emit_itt()</em>.</p></li>
</ol>
</div>
</div>
<div class="section" id="torchserve-with-intel-extension-for-pytorch">
<h2>TorchServe with Intel® Extension for PyTorch*<a class="headerlink" href="#torchserve-with-intel-extension-for-pytorch" title="Permalink to this heading">¶</a></h2>
<p><a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch">Intel® Extension for PyTorch*</a> is a Python package to extend PyTorch with optimizations for extra performance boost on Intel hardware.</p>
<p>Intel® Extension for PyTorch* has already been integrated into TorchServe to improve the performance out-of-box. <sup>2</sup> For custom handler scripts, we recommend adding the <em>intel_extension_for_pytorch</em> package in.</p>
<ol class="arabic simple" start="2">
<li><p>The feature has to be explicitly enabled by setting <em>ipex_enable=true</em> in <em>config.properties</em>.</p></li>
</ol>
<p>Throughout this section, we will show that Back End Bound is often the primary bottleneck for under-optimized or under-tuned deep learning workloads, and demonstrate optimization techniques via Intel® Extension for PyTorch* for improving Back End Bound, which has two submetrics - Memory Bound, and Core Bound. A more efficient memory allocator, operator fusion, memory layout format optimization improve Memory Bound. Ideally, Memory Bound can be improved to Core Bound by optimized operators and better cache locality. And key deep learning primitives, such as convolution, matrix multiplication, dot-product, have been well optimized by Intel® Extension for PyTorch* and oneDNN library, improving Core Bound.</p>
<div class="section" id="leveraging-advanced-launcher-configuration-memory-allocator">
<h3>Leveraging Advanced Launcher Configuration: Memory Allocator<a class="headerlink" href="#leveraging-advanced-launcher-configuration-memory-allocator" title="Permalink to this heading">¶</a></h3>
<p>Memory allocator plays an important role from performance perspective. A more efficient memory usage reduces overhead on unnecessary memory allocations or destructions, and thus faster execution. For deep learning workloads in practice, especially those running on large multi-core systems or servers like TorchServe, TCMalloc, or JeMalloc can generally get better memory usage than the default PyTorch memory allocator, PTMalloc.</p>
<div class="section" id="tcmalloc-jemalloc-ptmalloc">
<h4>TCMalloc, JeMalloc, PTMalloc<a class="headerlink" href="#tcmalloc-jemalloc-ptmalloc" title="Permalink to this heading">¶</a></h4>
<p>Both TCMalloc and JeMalloc use thread-local caches to reduce overhead on thread synchronization, and lock contention by using spinlocks and per-thread arenas respectively. TCMalloc and JeMalloc reduce overhead on unnecessary memory allocation and deallocation. Both allocators categorize memory allocations by sizes to reduce overhead on memory fragmentation.</p>
<p>With the launcher, users can easily experiment with different memory allocators by choosing one of the three launcher knobs <em>–enable_tcmalloc</em> (TCMalloc), <em>–enable_jemalloc</em> (JeMalloc), <em>–use_default_allocator</em> (PTMalloc).</p>
<div class="section" id="exercise">
<h5>Exercise<a class="headerlink" href="#exercise" title="Permalink to this heading">¶</a></h5>
<p>Let’s profile PTMalloc vs. JeMalloc.</p>
<p>We will use the launcher to designate the memory allocator, and to bind the workload to physical cores of the first socket to avoid any NUMA complication – to profile the effect of memory allocator only.</p>
<p>The following example measures the average inference time of ResNet50:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision.models</span> <span class="k">as</span> <span class="nn">models</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>

<span class="c1"># warm up</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># measure</span>
<span class="c1"># Intel® VTune Profiler's ITT context manager</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">emit_itt</span><span class="p">():</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
   <span class="c1"># Intel® VTune Profiler's ITT to annotate each step</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">itt</span><span class="o">.</span><span class="n">range_push</span><span class="p">(</span><span class="s1">'step_</span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
        <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">itt</span><span class="o">.</span><span class="n">range_pop</span><span class="p">()</span>
    <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'Inference took </span><span class="si">{:.2f}</span><span class="s1"> ms in average'</span><span class="o">.</span><span class="n">format</span><span class="p">((</span><span class="n">end</span><span class="o">-</span><span class="n">start</span><span class="p">)</span><span class="o">/</span><span class="mi">100</span><span class="o">*</span><span class="mi">1000</span><span class="p">))</span>
</pre></div>
</div>
<p>Let’s collect level-1 TMA metrics.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/32.png"><img alt="../_images/32.png" src="../_images/32.png" style="width: 100%;"/></a>
</div>
<p>Level-1 TMA shows that both PTMalloc and JeMalloc are bounded by the backend. More than half of the execution time was stalled by the backend. Let’s go one level deeper.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/41.png"><img alt="../_images/41.png" src="../_images/41.png" style="width: 100%;"/></a>
</div>
<p>Level-2 TMA shows that the Back End Bound was caused by Memory Bound. Let’s go one level deeper.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/51.png"><img alt="../_images/51.png" src="../_images/51.png" style="width: 100%;"/></a>
</div>
<p>Most of the metrics under the Memory Bound identify which level of the memory hierarchy from the L1 cache to main memory is the bottleneck. A hotspot bounded at a given level indicates that most of the data was being retrieved from that cache or memory-level. Optimizations should focus on moving data closer to the core. Level-3 TMA shows that PTMalloc was bottlenecked by DRAM Bound. On the other hand, JeMalloc was bottlenecked by L1 Bound – JeMalloc moved data closer to the core, and thus faster execution.</p>
<p>Let’s look at Intel® VTune Profiler ITT trace. In the example script, we have annotated each <em>step_x</em> of the inference loop.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/61.png"><img alt="../_images/61.png" src="../_images/61.png" style="width: 100%;"/></a>
</div>
<p>Each step is traced in the timeline graph. The duration of model inference on the last step (step_99) decreased from 304.308 ms to 261.843 ms.</p>
</div>
<div class="section" id="exercise-with-torchserve">
<h5>Exercise with TorchServe<a class="headerlink" href="#exercise-with-torchserve" title="Permalink to this heading">¶</a></h5>
<p>Let’s profile PTMalloc vs. JeMalloc with TorchServe.</p>
<p>We will use <a class="reference external" href="https://github.com/pytorch/serve/tree/master/benchmarks#benchmarking-with-apache-bench">TorchServe apache-bench benchmarking</a> with ResNet50 FP32, batch size 32, concurrency 32, requests 8960. All other parameters are the same as the <a class="reference external" href="https://github.com/pytorch/serve/tree/master/benchmarks#benchmark-parameters">default parameters</a>.</p>
<p>As in the previous exercise, we will use the launcher to designate the memory allocator, and to bind the workload to physical cores of the first socket. To do so, user simply needs to add a few lines in <a class="reference external" href="https://pytorch.org/serve/configuration.html#config-properties-file">config.properties</a>:</p>
<p>PTMalloc</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cpu_launcher_enable</span><span class="o">=</span><span class="n">true</span>
<span class="n">cpu_launcher_args</span><span class="o">=--</span><span class="n">node_id</span> <span class="mi">0</span> <span class="o">--</span><span class="n">use_default_allocator</span>
</pre></div>
</div>
<p>JeMalloc</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cpu_launcher_enable</span><span class="o">=</span><span class="n">true</span>
<span class="n">cpu_launcher_args</span><span class="o">=--</span><span class="n">node_id</span> <span class="mi">0</span> <span class="o">--</span><span class="n">enable_jemalloc</span>
</pre></div>
</div>
<p>Let’s collect level-1 TMA metrics.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/71.png"><img alt="../_images/71.png" src="../_images/71.png" style="width: 100%;"/></a>
</div>
<p>Let’s go one level deeper.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/81.png"><img alt="../_images/81.png" src="../_images/81.png" style="width: 100%;"/></a>
</div>
<p>Let’s use Intel® VTune Profiler ITT to annotate <a class="reference external" href="https://github.com/pytorch/serve/blob/master/ts/torch_handler/base_handler.py#L188">TorchServe inference scope</a> to profile at inference-level granularity. As <a class="reference external" href="https://github.com/pytorch/serve/blob/master/docs/internals.md#torchserve-architecture">TorchServe Architecture</a> consists of several sub-components, including the Java frontend for handling request/response, and the Python backend for running the actual inference on the models, it is helpful to use Intel® VTune Profiler ITT to limit the collection of trace data at inference-level.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/9.png"><img alt="../_images/9.png" src="../_images/9.png" style="width: 100%;"/></a>
</div>
<p>Each inference call is traced in the timeline graph. The duration of the last model inference decreased from 561.688 ms to 251.287 ms - 2.2x speedup.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/101.png"><img alt="../_images/101.png" src="../_images/101.png" style="width: 100%;"/></a>
</div>
<p>The timeline graph can be expanded to see op-level profiling results. The duration of <em>aten::conv2d</em> decreased from 16.401 ms to 6.392 ms - 2.6x speedup.</p>
<p>In this section, we have demonstrated that JeMalloc can give better performance than the default PyTorch memory allocator, PTMalloc, with efficient thread-local caches improving Back-End-Bound.</p>
</div>
</div>
</div>
<div class="section" id="id1">
<h3>Intel® Extension for PyTorch*<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h3>
<p>The three major <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch">Intel® Extension for PyTorch*</a> optimization techniques, Operator, Graph, Runtime, are as shown:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 46%"/>
<col style="width: 33%"/>
<col style="width: 21%"/>
</colgroup>
<thead>
<tr class="row-odd"><th class="head" colspan="3"><p>Intel® Extension for PyTorch* Optimization Techniques</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Operator</p></td>
<td><p>Graph</p></td>
<td><p>Runtime</p></td>
</tr>
<tr class="row-odd"><td><ul class="simple">
<li><p>Vectorization and Multi-threading</p></li>
<li><p>Low-precision BF16/INT8 compute</p></li>
<li><p>Data layout optimization for better cache locality</p></li>
</ul>
</td>
<td><ul class="simple">
<li><p>Constant folding to reduce compute</p></li>
<li><p>Op fusion for better cache locality</p></li>
</ul>
</td>
<td><ul class="simple">
<li><p>Thread affinitization</p></li>
<li><p>Memory buffer pooling</p></li>
<li><p>GPU runtime</p></li>
<li><p>Launcher</p></li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="section" id="operator-optimization">
<h4>Operator Optimization<a class="headerlink" href="#operator-optimization" title="Permalink to this heading">¶</a></h4>
<p>Optimized operators and kernels are registered through PyTorch dispatching mechanism. These operators and kernels are accelerated from native vectorization feature and matrix calculation feature of Intel hardware. During execution, Intel® Extension for PyTorch* intercepts invocation of ATen operators, and replaces the original ones with these optimized ones. Popular operators like Convolution, Linear have been optimized in Intel® Extension for PyTorch*.</p>
<div class="section" id="id2">
<h5>Exercise<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h5>
<p>Let’s profile optimized operator with Intel® Extension for PyTorch*. We will compare with and without the lines in code changes.</p>
<p>As in the previous exercises, we will bind the workload to physical cores of the first socket.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1">#################### code changes ####################</span>
<span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="c1">######################################################</span>

<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>The model consists of two operations—Conv2d and ReLU. By printing the model object, we get the following output.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/11.png"><img alt="../_images/11.png" src="../_images/11.png" style="width: 60%;"/></a>
</div>
<p>Let’s collect level-1 TMA metrics.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/121.png"><img alt="../_images/121.png" src="../_images/121.png" style="width: 100%;"/></a>
</div>
<p>Notice the Back End Bound reduced from 68.9 to 38.5 – 1.8x speedup.</p>
<p>Additionally, let’s profile with PyTorch Profiler.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/131.png"><img alt="../_images/131.png" src="../_images/131.png" style="width: 100%;"/></a>
</div>
<p>Notice the CPU time reduced from 851 us to 310 us – 2.7X speedup.</p>
</div>
</div>
<div class="section" id="graph-optimization">
<h4>Graph Optimization<a class="headerlink" href="#graph-optimization" title="Permalink to this heading">¶</a></h4>
<p>It is highly recommended for users to take advantage of Intel® Extension for PyTorch* with <a class="reference external" href="https://pytorch.org/docs/stable/jit.html">TorchScript</a> for further graph optimizations. To optimize performance further with TorchScript, Intel® Extension for PyTorch* supports oneDNN fusion of frequently used FP32/BF16 operator patterns, like Conv2D+ReLU, Linear+ReLU, and more to reduce operator/kernel invocation overheads, and for better cache locality. Some operator fusions allow to maintain temporary calculations, data type conversions, data layouts for better cache locality. As well as for INT8, Intel® Extension for PyTorch* has built-in quantization recipes to deliver good statistical accuracy for popular DL workloads including CNN, NLP and recommendation models. The quantized model is then optimized with oneDNN fusion support.</p>
<div class="section" id="id3">
<h5>Exercise<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h5>
<p>Let’s profile FP32 graph optimization with TorchScript.</p>
<p>As in the previous exercises, we will bind the workload to physical cores of the first socket.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1">#################### code changes ####################</span>
<span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="c1">######################################################</span>

<span class="c1"># torchscript</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>Let’s collect level-1 TMA metrics.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/141.png"><img alt="../_images/141.png" src="../_images/141.png" style="width: 100%;"/></a>
</div>
<p>Notice the Back End Bound reduced from 67.1 to 37.5 – 1.8x speedup.</p>
<p>Additionally, let’s profile with PyTorch Profiler.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/151.png"><img alt="../_images/151.png" src="../_images/151.png" style="width: 100%;"/></a>
</div>
<p>Notice that with Intel® Extension for PyTorch*  Conv + ReLU operators are fused, and the CPU time reduced from 803 us to 248 us – 3.2X speedup. The oneDNN eltwise post-op enables fusing a primitive with an elementwise primitive. This is one of the most popular kinds of fusion: an eltwise (typically an activation function such as ReLU) with preceding convolution or inner product. Have a look at the oneDNN verbose log shown in the next section.</p>
</div>
</div>
<div class="section" id="channels-last-memory-format">
<h4>Channels Last Memory Format<a class="headerlink" href="#channels-last-memory-format" title="Permalink to this heading">¶</a></h4>
<p>When invoking <em>ipex.optimize</em> on model, Intel® Extension for PyTorch* automatically converts the model to optimized memory format, channels last. Channels last is a memory format that is more friendly to Intel Architecture. Compared to PyTorch default channels first NCHW (batch, channels, height, width) memory format, channels last NHWC (batch, height, width, channels) memory format generally accelerates convolutional neural networks with better cache locality.</p>
<p>One thing to note is that it is expensive to convert memory format. So it’s better to convert the memory format prior to deployment once, and keep the memory format conversion minimum during deployment. As the data propagates through model’s layers the channels last memory format is preserved through consecutive channels last supported layers (for example, Conv2d -&gt; ReLU -&gt; Conv2d) and conversions are only made in between channels last unsupported layers. See <a class="reference external" href="https://www.intel.com/content/www/us/en/develop/documentation/onednn-developer-guide-and-reference/top/programming-model/memory-format-propagation.html">Memory Format Propagation</a> for more details.</p>
<div class="section" id="id4">
<h5>Exercise<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h5>
<p>Let’s demonstrate channels last optimization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>
<span class="c1">############################### code changes ###############################</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">disable_auto_channels_last</span><span class="p">()</span> <span class="c1"># omit this line for channels_last (default)</span>
<span class="c1">############################################################################</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>We will use <a class="reference external" href="https://oneapi-src.github.io/oneDNN/dev_guide_verbose.html">oneDNN verbose mode</a>, a tool to help collect information at oneDNN graph level such as operator fusions, kernel execution time spent on executing oneDNN primitives. For more information, refer to the <a class="reference external" href="https://oneapi-src.github.io/oneDNN/index.html">oneDNN Documentation</a>.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/161.png"><img alt="../_images/161.png" src="../_images/161.png" style="width: 15%;"/></a>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/171.png"><img alt="../_images/171.png" src="../_images/171.png" style="width: 100%;"/></a>
</div>
<p>Above is oneDNN verbose from channels first. We can verify that there are reorders from weight and data, then do computation, and finally reorder output back.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/181.png"><img alt="../_images/181.png" src="../_images/181.png" style="width: 80%;"/></a>
</div>
<p>Above is oneDNN verbose from channels last. We can verify that channels last memory format avoids unnecessary reorders.</p>
</div>
</div>
</div>
<div class="section" id="performance-boost-with-intel-extension-for-pytorch">
<h3>Performance Boost with Intel® Extension for PyTorch*<a class="headerlink" href="#performance-boost-with-intel-extension-for-pytorch" title="Permalink to this heading">¶</a></h3>
<p>Below summarizes performance boost of TorchServe with Intel® Extension for PyTorch* for ResNet50 and BERT-base-uncased.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/191.png"><img alt="../_images/191.png" src="../_images/191.png" style="width: 100%;"/></a>
</div>
</div>
<div class="section" id="id5">
<h3>Exercise with TorchServe<a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h3>
<p>Let’s profile Intel® Extension for PyTorch* optimizations with TorchServe.</p>
<p>We will use <a class="reference external" href="https://github.com/pytorch/serve/tree/master/benchmarks#benchmarking-with-apache-bench">TorchServe apache-bench benchmarking</a> with ResNet50 FP32 TorchScript, batch size 32, concurrency 32, requests 8960. All other parameters are the same as the <a class="reference external" href="https://github.com/pytorch/serve/tree/master/benchmarks#benchmark-parameters">default parameters</a>.</p>
<p>As in the previous exercise, we will use the launcher to bind the workload to physical cores of the first socket. To do so, user simply needs to add a few lines in <a class="reference external" href="https://github.com/pytorch/serve/tree/master/benchmarks#benchmark-parameters">config.properties</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cpu_launcher_enable</span><span class="o">=</span><span class="n">true</span>
<span class="n">cpu_launcher_args</span><span class="o">=--</span><span class="n">node_id</span> <span class="mi">0</span>
</pre></div>
</div>
<p>Let’s collect level-1 TMA metrics.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/20.png"><img alt="../_images/20.png" src="../_images/20.png" style="width: 100%;"/></a>
</div>
<p>Level-1 TMA shows that both are bounded by the backend. As discussed earlier, the majority of untuned deep learning workloads will be Back End Bound. Notice the Back End Bound reduced from 70.0 to 54.1. Let’s go one level deeper.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/211.png"><img alt="../_images/211.png" src="../_images/211.png" style="width: 100%;"/></a>
</div>
<p>As discussed earlier, Back End Bound has two submetrics – Memory Bound and Core Bound. Memory Bound indicates the workload is under-optimized or under-utilized, and ideally memory-bound operations can be improved to core-bound by optimizing the OPs and improving cache locality. Level-2 TMA shows that the Back End Bound improved from Memory Bound to Core Bound. Let’s go one level deeper.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/221.png"><img alt="../_images/221.png" src="../_images/221.png" style="width: 100%;"/></a>
</div>
<p>Scaling deep learning models for production on a model serving framework like TorchServe requires high compute utilization. This requires that data is available through prefetching and reusing the data in cache when the execution units need it to execute the uOps. Level-3 TMA shows that the Back End Memory Bound improved from DRAM Bound to Core Bound.</p>
<p>As in the previous exercise with TorchServe, let’s use Intel® VTune Profiler ITT to annotate <a class="reference external" href="https://github.com/pytorch/serve/blob/master/ts/torch_handler/base_handler.py#L188">TorchServe inference scope</a> to profile at inference-level granularity.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/231.png"><img alt="../_images/231.png" src="../_images/231.png" style="width: 100%;"/></a>
</div>
<p>Each inference call is traced in the timeline graph. The duration of the last inference call decreased from 215.731 ms to 95.634 ms - 2.3x speedup.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/241.png"><img alt="../_images/241.png" src="../_images/241.png" style="width: 100%;"/></a>
</div>
<p>The timeline graph can be expanded to see op-level profiling results. Notice that Conv + ReLU has been fused, and the duration decreased from 6.393 ms + 1.731 ms to 3.408 ms - 2.4x speedup.</p>
</div>
</div>
<div class="section" id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">¶</a></h2>
<p>In this tutorial, we have used Top-down Microarchitecture Analysis (TMA) and Intel® VTune™ Profiler’s Instrumentation and Tracing Technology (ITT) to demonstrate that</p>
<ul class="simple">
<li><p>Often the primary bottleneck of under-optimized or under-tuned deep learning workloads are Back End Bound, which has two submetrics, Memory Bound and Core Bound.</p></li>
<li><p>A more efficient memory allocator, operator fusion, memory layout format optimization by Intel® Extension for PyTorch* improve Memory Bound.</p></li>
<li><p>Key deep learning primitives, such as convolution, matrix multiplication, dot-product, etc have been well optimized by Intel® Extension for PyTorch* and oneDNN library, improving Core Bound.</p></li>
<li><p>Intel® Extension for PyTorch* has been integrated into TorchServe with an ease-of-use API.</p></li>
<li><p>TorchServe with Intel® Extension for PyTorch* shows 7.71x throughput speedup for ResNet50, and 2.20x throughput speedup for BERT.</p></li>
</ul>
</div>
<div class="section" id="related-readings">
<h2>Related Readings<a class="headerlink" href="#related-readings" title="Permalink to this heading">¶</a></h2>
<p><a class="reference external" href="https://www.intel.com/content/www/us/en/develop/documentation/vtune-cookbook/top/methodologies/top-down-microarchitecture-analysis-method.html">Top-down Microarchitecture Analysis Method</a></p>
<p><a class="reference external" href="https://easyperf.net/blog/2019/02/09/Top-Down-performance-analysis-methodology">Top-Down performance analysis methodology</a></p>
<p><a class="reference external" href="https://medium.com/pytorch/accelerating-pytorch-with-intel-extension-for-pytorch-3aef51ea3722">Accelerating PyTorch with Intel® Extension for PyTorch*</a></p>
</div>
<div class="section" id="acknowledgement">
<h2>Acknowledgement<a class="headerlink" href="#acknowledgement" title="Permalink to this heading">¶</a></h2>
<p>We would like to thank Ashok Emani (Intel) and Jiong Gong (Intel) for their immense guidance and support, and thorough feedback and reviews throughout many steps of this tutorial. We would also like to thank Hamid Shojanazeri (Meta) and Li Ning (AWS) for their helpful feedback in code review and the tutorial.</p>
</div>
</div>
</article>
</div>
<footer>
<div aria-label="footer navigation" class="rst-footer-buttons" role="navigation">
<a accesskey="n" class="btn btn-neutral float-right" href="nvfuser_intro_tutorial.html" rel="next" title="Getting Started - Accelerate Your Scripts with nvFuser">Next <img class="next-page" src="../_static/images/chevron-right-orange.svg"/></a>
<a accesskey="p" class="btn btn-neutral" href="torchserve_with_ipex.html" rel="prev" title="Grokking PyTorch Intel CPU performance from first principles"><img class="previous-page" src="../_static/images/chevron-right-orange.svg"/> Previous</a>
</div>
<hr class="rating-hr hr-top"/>
<div class="rating-container">
<div class="rating-prompt">Rate this Tutorial</div>
<div class="stars-outer">
<i class="far fa-star" data-behavior="tutorial-rating" data-count="1" title="1 Star"></i>
<i class="far fa-star" data-behavior="tutorial-rating" data-count="2" title="2 Stars"></i>
<i class="far fa-star" data-behavior="tutorial-rating" data-count="3" title="3 Stars"></i>
<i class="far fa-star" data-behavior="tutorial-rating" data-count="4" title="4 Stars"></i>
<i class="far fa-star" data-behavior="tutorial-rating" data-count="5" title="5 Stars"></i>
</div>
</div>
<hr class="rating-hr hr-bottom">
<div role="contentinfo">
<p>
        © Copyright 2024, PyTorch.

    </p>
</div>
<div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
</hr></footer>
</div>
<script>
if((window.location.href.indexOf("/prototype/")!= -1) && (window.location.href.indexOf("/prototype/prototype_index")< 1))
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-flask" aria-hidden="true">&nbsp</i> This tutorial describes a prototype feature. Prototype features are typically not available as part of binary distributions like PyPI or Conda, except sometimes behind run-time flags, and are at an early stage for feedback and testing.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  } 
</script>
</div>
<div class="pytorch-content-right" id="pytorch-content-right">
<div class="pytorch-right-menu" id="pytorch-right-menu">
<div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
<ul>
<li><a class="reference internal" href="#">Grokking PyTorch Intel CPU performance from first principles (Part 2)</a><ul>
<li><a class="reference internal" href="#prerequisites">Prerequisites</a><ul>
<li><a class="reference internal" href="#top-down-microarchitecture-analysis-method-tma">Top-down Microarchitecture Analysis Method (TMA)</a><ul>
<li><a class="reference internal" href="#tune-for-the-back-end-bound">Tune for the Back End Bound</a></li>
</ul>
</li>
<li><a class="reference internal" href="#intel-vtune-profiler-s-instrumentation-and-tracing-technology-itt">Intel® VTune™ Profiler’s Instrumentation and Tracing Technology (ITT)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#torchserve-with-intel-extension-for-pytorch">TorchServe with Intel® Extension for PyTorch*</a><ul>
<li><a class="reference internal" href="#leveraging-advanced-launcher-configuration-memory-allocator">Leveraging Advanced Launcher Configuration: Memory Allocator</a><ul>
<li><a class="reference internal" href="#tcmalloc-jemalloc-ptmalloc">TCMalloc, JeMalloc, PTMalloc</a><ul>
<li><a class="reference internal" href="#exercise">Exercise</a></li>
<li><a class="reference internal" href="#exercise-with-torchserve">Exercise with TorchServe</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#id1">Intel® Extension for PyTorch*</a><ul>
<li><a class="reference internal" href="#operator-optimization">Operator Optimization</a><ul>
<li><a class="reference internal" href="#id2">Exercise</a></li>
</ul>
</li>
<li><a class="reference internal" href="#graph-optimization">Graph Optimization</a><ul>
<li><a class="reference internal" href="#id3">Exercise</a></li>
</ul>
</li>
<li><a class="reference internal" href="#channels-last-memory-format">Channels Last Memory Format</a><ul>
<li><a class="reference internal" href="#id4">Exercise</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#performance-boost-with-intel-extension-for-pytorch">Performance Boost with Intel® Extension for PyTorch*</a></li>
<li><a class="reference internal" href="#id5">Exercise with TorchServe</a></li>
</ul>
</li>
<li><a class="reference internal" href="#conclusion">Conclusion</a></li>
<li><a class="reference internal" href="#related-readings">Related Readings</a></li>
<li><a class="reference internal" href="#acknowledgement">Acknowledgement</a></li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</section>
</div>
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript"></script>
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/katex.min.js"></script>
<script src="../_static/auto-render.min.js"></script>
<script src="../_static/katex_autorenderer.js"></script>
<script src="../_static/design-tabs.js"></script>
<script src="../_static/js/vendor/popper.min.js" type="text/javascript"></script>
<script src="../_static/js/vendor/bootstrap.min.js" type="text/javascript"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<script>

// Helper function to make it easier to call dataLayer.push() 
function gtag(){window.dataLayer.push(arguments);}

//add microsoft link

if(window.location.href.indexOf("/beginner/basics/")!= -1)
{
  var url="https://docs.microsoft.com/learn/paths/pytorch-fundamentals/?wt.mc_id=aiml-7486-cxa";
  switch(window.location.pathname.split("/").pop().replace('.html',''))
  {
    case"quickstart_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/9-quickstart?WT.mc_id=aiml-7486-cxa";
      break;
    case"tensorqs_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/2-tensors?WT.mc_id=aiml-7486-cxa";
      break;
    case"data_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/3-data?WT.mc_id=aiml-7486-cxa";
      break;
    case"transforms_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/4-transforms?WT.mc_id=aiml-7486-cxa";
      break;
    case"buildmodel_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/5-model?WT.mc_id=aiml-7486-cxa";
      break;
    case"autogradqs_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/6-autograd?WT.mc_id=aiml-7486-cxa";
      break;
    case"optimization_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/7-optimization?WT.mc_id=aiml-7486-cxa";
      break;
    case"saveloadrun_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/8-inference?WT.mc_id=aiml-7486-cxa";
    }
    
    $(".pytorch-call-to-action-links").children().first().before("<a href="+url+' data-behavior="call-to-action-event" data-response="Run in Microsoft Learn" target="_blank"><div id="microsoft-learn-link" style="padding-bottom: 0.625rem;border-bottom: 1px solid #f3f4f7;padding-right: 2.5rem;display: -webkit-box;  display: -ms-flexbox; display: flex; -webkit-box-align: center;-ms-flex-align: center;align-items: center;"><img class="call-to-action-img" src="../../_static/images/microsoft-logo.svg"/><div class="call-to-action-desktop-view">Run in Microsoft Learn</div><div class="call-to-action-mobile-view">Learn</div></div></a>')
  }

  !function(f,b,e,v,n,t,s)
  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
  n.callMethod.apply(n,arguments):n.queue.push(arguments)};
  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
  n.queue=[];t=b.createElement(e);t.async=!0;
  t.src=v;s=b.getElementsByTagName(e)[0];
  s.parentNode.insertBefore(t,s)}(window,document,'script',
  'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');

  $("[data-behavior='call-to-action-event']").on('click', function(){
    fbq('trackCustom', "Download", {
      tutorialTitle: $('h1:first').text(),
      downloadLink: this.href,
      tutorialLink: window.location.href,
      downloadTitle: $(this).attr("data-response")
    });
    gtag('event', 'click', {
      'event_category': $(this).attr("data-response"),
      'event_label': $("h1").first().text(),
      'tutorial_link': window.location.href
    });
   });

   $("[data-behavior='tutorial-rating']").on('click', function(){
    fbq('trackCustom', "Tutorial Rating", {
      tutorialLink: window.location.href,
      tutorialTitle: $('h1:first').text(),
      rating: $(this).attr("data-count")
    });

    gtag('event', 'click', {
      'event_category': 'Tutorial Rating',
      'event_label': $("h1").first().text(),
      'value': $(this).attr("data-count")
    });
   });

   if (location.pathname == "/") {
     $(".rating-container").hide();
     $(".hr-bottom").hide();
   }


</script>
<noscript>
<img height="1" src="https://www.facebook.com/tr?id=243028289693773&amp;ev=PageView
  &amp;noscript=1" width="1">
</img></noscript>
<script type="text/javascript">
  var collapsedSections = ['PyTorch Recipes', 'Learning PyTorch', 'Image and Video', 'Audio', 'Text', 'Backends', 'Reinforcement Learning', 'Deploying PyTorch Models in Production', 'Code Transforms with FX', 'Frontend APIs', 'Extending PyTorch', 'Model Optimization', 'Parallel and Distributed Training', 'Mobile'];
</script>
<img alt="" height="1" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0" style="border-style:none;" width="1">
<!-- Begin Footer -->
<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
<div class="container">
<div class="row">
<div class="col-md-4 text-center">
<h2>Docs</h2>
<p>Access comprehensive developer documentation for PyTorch</p>
<a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
</div>
<div class="col-md-4 text-center">
<h2>Tutorials</h2>
<p>Get in-depth tutorials for beginners and advanced developers</p>
<a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
</div>
<div class="col-md-4 text-center">
<h2>Resources</h2>
<p>Find development resources and get your questions answered</p>
<a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
</div>
</div>
</div>
</div>
<footer class="site-footer">
<div class="container footer-container">
<div class="footer-logo-wrapper">
<a class="footer-logo" href="https://pytorch.org/"></a>
</div>
<div class="footer-links-wrapper">
<div class="footer-links-col">
<ul>
<li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
<li><a href="https://pytorch.org/get-started">Get Started</a></li>
<li><a href="https://pytorch.org/features">Features</a></li>
<li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
<li><a href="https://pytorch.org/blog/">Blog</a></li>
<li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
</ul>
</div>
<div class="footer-links-col">
<ul>
<li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
<li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
<li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
<li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
<li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
<li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
</ul>
</div>
<div class="footer-links-col">
<ul>
<li class="list-title">Stay up to date</li>
<li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
<li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
<li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
<li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
</ul>
</div>
<div class="footer-links-col">
<ul>
<li class="list-title">PyTorch Podcasts</li>
<li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
<li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
<li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
<li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
</ul>
</div>
</div>
<div class="privacy-policy">
<ul>
<li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
<li class="privacy-policy-links">|</li>
<li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
</ul>
</div>
<div class="copyright">
<p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
</div>
</div>
</footer>
<div class="cookie-banner-wrapper">
<div class="container">
<p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
<img class="close-button" src="../_static/images/pytorch-x.svg"/>
</div>
</div>
<!-- End Footer -->
<!-- Begin Mobile Menu -->
<div class="mobile-main-menu">
<div class="container-fluid">
<div class="container">
<div class="mobile-main-menu-header-container">
<a aria-label="PyTorch" class="header-logo" href="https://pytorch.org/"></a>
<a class="main-menu-close-button" data-behavior="close-mobile-menu" href="#"></a>
</div>
</div>
</div>
<div class="mobile-main-menu-links-container">
<div class="main-menu">
<ul>
<li>
<a href="https://pytorch.org/get-started">Get Started</a>
</li>
<li>
<a href="https://pytorch.org/ecosystem">Ecosystem</a>
</li>
<li>
<a href="">Mobile</a>
</li>
<li>
<a href="https://pytorch.org/blog/">Blog</a>
</li>
<li class="active">
<a href="https://pytorch.org/tutorials">Tutorials</a>
</li>
<li class="resources-mobile-menu-title">
            Docs
          </li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
</li>
<li>
<a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
</li>
<li>
<a href="https://pytorch.org/text/stable/index.html">torchtext</a>
</li>
<li>
<a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
</li>
<li>
<a href="https://pytorch.org/torcharrow">torcharrow</a>
</li>
<li>
<a href="https://pytorch.org/data">TorchData</a>
</li>
<li>
<a href="https://pytorch.org/torchrec">TorchRec</a>
</li>
<li>
<a href="https://pytorch.org/serve/">TorchServe</a>
</li>
<li>
<a href="https://pytorch.org/torchx/">TorchX</a>
</li>
<li>
<a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
</li>
</ul>
<li class="resources-mobile-menu-title">
            Resources
          </li>
<ul class="resources-mobile-menu-items">
<li>
<a href="https://pytorch.org/features">About</a>
</li>
<li>
<a href="https://pytorch.org/foundation">PyTorch Foundation</a>
</li>
<li>
<a href="https://pytorch.org/#community-module">Community</a>
</li>
<li>
<a href="https://pytorch.org/community-stories">Community Stories</a>
</li>
<li>
<a href="https://pytorch.org/resources">Developer Resources</a>
</li>
<li>
<a href="https://pytorch.org/events">Events</a>
</li>
<li>
<a href="https://discuss.pytorch.org/">Forums</a>
</li>
<li>
<a href="https://pytorch.org/hub">Models (Beta)</a>
</li>
</ul>
<li>
<a href="https://github.com/pytorch/pytorch">Github</a>
</li>
</ul>
</div>
</div>
</div>
<!-- End Mobile Menu -->
<script src="../_static/js/vendor/anchor.min.js" type="text/javascript"></script>
<script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</img></body>
</html>