


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>PyTorch Benchmark &mdash; PyTorch Tutorials 2.2.0+cu121 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom2.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch">
                  <span class="dropdown-title">ExecuTorch</span>
                </a>
              </div>
            </div>  
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  2.2.0+cu121
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Tutorials" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">PyTorch Recipes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../recipes_index.html">See All Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../prototype/prototype_index.html">See All Prototype Recipes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Introduction to PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/intro.html">Learn the Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/quickstart_tutorial.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/tensorqs_tutorial.html">Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/data_tutorial.html">Datasets &amp; DataLoaders</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/transforms_tutorial.html">Transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/buildmodel_tutorial.html">Build the Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/autogradqs_tutorial.html">Automatic Differentiation with <code class="docutils literal notranslate"><span class="pre">torch.autograd</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/optimization_tutorial.html">Optimizing Model Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/saveloadrun_tutorial.html">Save and Load the Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Introduction to PyTorch on YouTube</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introyt.html">Introduction to PyTorch - YouTube Series</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introyt/introyt1_tutorial.html">Introduction to PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introyt/tensors_deeper_tutorial.html">Introduction to PyTorch Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introyt/autogradyt_tutorial.html">The Fundamentals of Autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introyt/modelsyt_tutorial.html">Building Models with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introyt/tensorboardyt_tutorial.html">PyTorch TensorBoard Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introyt/trainingyt.html">Training with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introyt/captumyt.html">Model Understanding with Captum</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Learning PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/deep_learning_60min_blitz.html">Deep Learning with PyTorch: A 60 Minute Blitz</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/pytorch_with_examples.html">Learning PyTorch with Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/nn_tutorial.html">What is <cite>torch.nn</cite> <em>really</em>?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/tensorboard_tutorial.html">Visualizing Models, Data, and Training with TensorBoard</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Image and Video</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/torchvision_tutorial.html">TorchVision Object Detection Finetuning Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/transfer_learning_tutorial.html">Transfer Learning for Computer Vision Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/fgsm_tutorial.html">Adversarial Example Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/dcgan_faces_tutorial.html">DCGAN Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/spatial_transformer_tutorial.html">Spatial Transformer Networks Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/vt_tutorial.html">Optimizing Vision Transformer Model for Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/tiatoolbox_tutorial.html">Whole Slide Image Classification Using PyTorch and TIAToolbox</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Audio</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/audio_io_tutorial.html">Audio I/O</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/audio_resampling_tutorial.html">Audio Resampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/audio_data_augmentation_tutorial.html">Audio Data Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/audio_feature_extractions_tutorial.html">Audio Feature Extractions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/audio_feature_augmentation_tutorial.html">Audio Feature Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/audio_datasets_tutorial.html">Audio Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/speech_recognition_pipeline_tutorial.html">Speech Recognition with Wav2Vec2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/text_to_speech_with_torchaudio.html">Text-to-speech with Tacotron2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/forced_alignment_with_torchaudio_tutorial.html">Forced Alignment with Wav2Vec2</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Text</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/transformer_tutorial.html">Language Modeling with <code class="docutils literal notranslate"><span class="pre">nn.Transformer</span></code> and torchtext</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/bettertransformer_tutorial.html">Fast Transformer Inference with Better Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html">NLP From Scratch: Classifying Names with a Character-Level RNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/char_rnn_generation_tutorial.html">NLP From Scratch: Generating Names with a Character-Level RNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html">NLP From Scratch: Translation with a Sequence to Sequence Network and Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/text_sentiment_ngrams_tutorial.html">Text classification with the torchtext library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/translation_transformer.html">Language Translation with <code class="docutils literal notranslate"><span class="pre">nn.Transformer</span></code> and torchtext</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/torchtext_custom_dataset_tutorial.html">Preprocess custom text dataset using Torchtext</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Backends</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/onnx/intro_onnx.html">Introduction to ONNX</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reinforcement Learning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/reinforcement_q_learning.html">Reinforcement Learning (DQN) Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/reinforcement_ppo.html">Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/mario_rl_tutorial.html">Train a Mario-playing RL Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/pendulum.html">Pendulum: Writing your environment and transforms with TorchRL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deploying PyTorch Models in Production</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/onnx/intro_onnx.html">Introduction to ONNX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/flask_rest_api_tutorial.html">Deploying PyTorch in Python via a REST API with Flask</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/Intro_to_TorchScript_tutorial.html">Introduction to TorchScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/cpp_export.html">Loading a TorchScript Model in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/super_resolution_with_onnxruntime.html">(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/realtime_rpi.html">Real Time Inference on Raspberry Pi 4 (30 fps!)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Profiling PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/profiler.html">Profiling your PyTorch Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/hta_intro_tutorial.html">Introduction to Holistic Trace Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/hta_trace_diff_tutorial.html">Trace Diff using Holistic Trace Analysis</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Code Transforms with FX</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/fx_conv_bn_fuser.html">(beta) Building a Convolution/Batch Norm fuser in FX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/fx_profiling_tutorial.html">(beta) Building a Simple CPU Performance Profiler with FX</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Frontend APIs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/memory_format_tutorial.html">(beta) Channels Last Memory Format in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/forward_ad_usage.html">Forward-mode Automatic Differentiation (Beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/jacobians_hessians.html">Jacobians, Hessians, hvp, vhp, and more: composing function transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/ensembling.html">Model ensembling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/per_sample_grads.html">Per-sample-gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/cpp_frontend.html">Using the PyTorch C++ Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/torch-script-parallelism.html">Dynamic Parallelism in TorchScript</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/cpp_autograd.html">Autograd in C++ Frontend</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Extending PyTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/custom_function_double_backward_tutorial.html">Double Backward with Custom Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/custom_function_conv_bn_tutorial.html">Fusing Convolution and Batch Norm using Custom Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/cpp_extension.html">Custom C++ and CUDA Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/torch_script_custom_ops.html">Extending TorchScript with Custom C++ Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/torch_script_custom_classes.html">Extending TorchScript with Custom C++ Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/dispatcher.html">Registering a Dispatched Operator in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/extend_dispatcher.html">Extending dispatcher for a new backend in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/privateuseone.html">Facilitating New Backend Integration by PrivateUse1</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/profiler.html">Profiling your PyTorch Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/tensorboard_profiler_tutorial.html">PyTorch Profiler With TensorBoard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/hyperparameter_tuning_tutorial.html">Hyperparameter tuning with Ray Tune</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/vt_tutorial.html">Optimizing Vision Transformer Model for Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/parametrizations.html">Parametrizations Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/pruning_tutorial.html">Pruning Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/dynamic_quantization_tutorial.html">(beta) Dynamic Quantization on an LSTM Word Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/dynamic_quantization_bert_tutorial.html">(beta) Dynamic Quantization on BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/quantized_transfer_learning_tutorial.html">(beta) Quantized Transfer Learning for Computer Vision Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/static_quantization_tutorial.html">(beta) Static Quantization with Eager Mode in PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/torchserve_with_ipex.html">Grokking PyTorch Intel CPU performance from first principles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/torchserve_with_ipex_2.html">Grokking PyTorch Intel CPU performance from first principles (Part 2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/nvfuser_intro_tutorial.html">Getting Started - Accelerate Your Scripts with nvFuser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/ax_multiobjective_nas_tutorial.html">Multi-Objective NAS with Ax</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/torch_compile_tutorial.html">Introduction to <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/inductor_debug_cpu.html">Inductor CPU backend debugging and profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/scaled_dot_product_attention_tutorial.html">(Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/scaled_dot_product_attention_tutorial.html#using-sdpa-with-torch-compile">Using SDPA with <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/scaled_dot_product_attention_tutorial.html#conclusion">Conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/knowledge_distillation_tutorial.html">Knowledge Distillation Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Parallel and Distributed Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../distributed/home.html">Distributed and Parallel Training Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/dist_overview.html">PyTorch Distributed Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/ddp_series_intro.html">Distributed Data Parallel in PyTorch - Video Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/model_parallel_tutorial.html">Single-Machine Model Parallel Best Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/ddp_tutorial.html">Getting Started with Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/dist_tuto.html">Writing Distributed Applications with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/FSDP_tutorial.html">Getting Started with Fully Sharded Data Parallel(FSDP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/FSDP_adavnced_tutorial.html">Advanced Model Training with Fully Sharded Data Parallel (FSDP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/process_group_cpp_extension_tutorial.html">Customize Process Group Backends Using Cpp Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/rpc_tutorial.html">Getting Started with Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/rpc_param_server_tutorial.html">Implementing a Parameter Server Using Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/dist_pipeline_parallel_tutorial.html">Distributed Pipeline Parallelism Using RPC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/rpc_async_execution.html">Implementing Batch RPC Processing Using Asynchronous Executions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/rpc_ddp_tutorial.html">Combining Distributed DataParallel with Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/pipeline_tutorial.html">Training Transformer models using Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/ddp_pipeline.html">Training Transformer models using Distributed Data Parallel and Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/generic_join.html">Distributed Training with Uneven Inputs Using the Join Context Manager</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Mobile</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/deeplabv3_on_ios.html">Image Segmentation DeepLabV3 on iOS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/deeplabv3_on_android.html">Image Segmentation DeepLabV3 on Android</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Recommendation Systems</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/torchrec_tutorial.html">Introduction to TorchRec</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/sharding.html">Exploring TorchRec sharding</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multimodality</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/flava_finetuning_tutorial.html">TorchMultimodal Tutorial: Finetuning FLAVA</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../index.html">
          
            Tutorials
          
        </a> &gt;
      </li>

        
      <li>PyTorch Benchmark</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../../_sources/recipes/recipes/benchmark.rst.txt" rel="nofollow"><img src="../../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <div class="pytorch-call-to-action-links">
            <div id="tutorial-type">recipes/recipes/benchmark</div>

            <div id="google-colab-link">
              <img class="call-to-action-img" src="../../_static/images/pytorch-colab.svg"/>
              <div class="call-to-action-desktop-view">Run in Google Colab</div>
              <div class="call-to-action-mobile-view">Colab</div>
            </div>
            <div id="download-notebook-link">
              <img class="call-to-action-notebook-img" src="../../_static/images/pytorch-download.svg"/>
              <div class="call-to-action-desktop-view">Download Notebook</div>
              <div class="call-to-action-mobile-view">Notebook</div>
            </div>
            <div id="github-view-link">
              <img class="call-to-action-img" src="../../_static/images/pytorch-github.svg"/>
              <div class="call-to-action-desktop-view">View on GitHub</div>
              <div class="call-to-action-mobile-view">GitHub</div>
            </div>
          </div>

        

          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-recipes-recipes-benchmark-py"><span class="std std-ref">here</span></a>
to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="pytorch-benchmark">
<span id="sphx-glr-recipes-recipes-benchmark-py"></span><h1>PyTorch Benchmark<a class="headerlink" href="#pytorch-benchmark" title="Permalink to this heading">Â¶</a></h1>
<p>This recipe provides a quick-start guide to using PyTorch
<code class="docutils literal notranslate"><span class="pre">benchmark</span></code> module to measure and compare code performance.</p>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">Â¶</a></h2>
<p>Benchmarking is an important step in writing code. It helps
us validate that our code meets performance expectations,
compare different approaches to solving the same problem and
prevent performance regressions.</p>
<p>There are many options when it comes to benchmarking PyTorch code
including the Python builtin <code class="docutils literal notranslate"><span class="pre">timeit</span></code> module. However, benchmarking
PyTorch code has many caveats that can be easily overlooked such as
managing the number of threads and synchronizing CUDA devices. Moreover,
generating Tensor inputs for benchmarking can be quite tedious.</p>
<p>This recipe demonstrates how to use PyTorch <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> module to avoid
common mistakes while making it easier to compare performance of
different code, generate input for benchmarking and more.</p>
</div>
<div class="section" id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this heading">Â¶</a></h2>
<p>Before we begin, install <code class="docutils literal notranslate"><span class="pre">torch</span></code> if it isnâ€™t already available.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">torch</span>
</pre></div>
</div>
</div>
<div class="section" id="steps">
<h2>Steps<a class="headerlink" href="#steps" title="Permalink to this heading">Â¶</a></h2>
<ol class="arabic simple">
<li><p>Defining functions to benchmark</p></li>
<li><p>Benchmarking with <code class="docutils literal notranslate"><span class="pre">timeit.Timer</span></code></p></li>
<li><p>Benchmarking with <code class="docutils literal notranslate"><span class="pre">torch.utils.benchmark.Timer</span></code></p></li>
<li><p>Benchmarking with <code class="docutils literal notranslate"><span class="pre">Blocked</span> <span class="pre">Autorange</span></code></p></li>
<li><p>Comparing benchmark results</p></li>
<li><p>Saving/Loading benchmark results</p></li>
<li><p>Generating inputs with <code class="docutils literal notranslate"><span class="pre">Fuzzed</span> <span class="pre">Parameters</span></code></p></li>
<li><p>Collecting instruction counts with <code class="docutils literal notranslate"><span class="pre">Callgrind</span></code></p></li>
</ol>
<div class="section" id="defining-functions-to-benchmark">
<h3>1. Defining functions to benchmark<a class="headerlink" href="#defining-functions-to-benchmark" title="Permalink to this heading">Â¶</a></h3>
<p>As of the time of this writing, <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.dot.html?highlight=dot#torch.dot">torch.dot</a>
does not support batched mode, so we will compare two approaches to
implementing it using existing <code class="docutils literal notranslate"><span class="pre">torch</span></code> operators: one approach uses a
combination of <code class="docutils literal notranslate"><span class="pre">mul</span></code> and <code class="docutils literal notranslate"><span class="pre">sum</span></code> while the other reduces the problem to <code class="docutils literal notranslate"><span class="pre">bmm</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>


<span class="k">def</span> <span class="nf">batched_dot_mul_sum</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;Computes batched dot by multiplying and summing&#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="n">a</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">batched_dot_bmm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;Computes batched dot by reducing to ``bmm``&#39;&#39;&#39;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <a href="https://pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm" title="torch.bmm" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">bmm</span></a><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>


<span class="c1"># Input for benchmarking</span>
<span class="n">x</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>

<span class="c1"># Ensure that both functions compute the same output</span>
<span class="k">assert</span> <span class="n">batched_dot_mul_sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">batched_dot_bmm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="benchmarking-with-timeit-timer">
<h3>2. Benchmarking with <code class="docutils literal notranslate"><span class="pre">timeit.Timer</span></code><a class="headerlink" href="#benchmarking-with-timeit-timer" title="Permalink to this heading">Â¶</a></h3>
<p>First, letâ€™s benchmark the code using Pythonâ€™s builtin <code class="docutils literal notranslate"><span class="pre">timeit</span></code> module.
We keep the benchmark code simple here so we can compare the defaults
of <code class="docutils literal notranslate"><span class="pre">timeit</span></code> and <code class="docutils literal notranslate"><span class="pre">torch.utils.benchmark</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">timeit</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_mul_sum(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_mul_sum&#39;</span><span class="p">,</span>
    <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">})</span>

<span class="n">t1</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_bmm(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_bmm&#39;</span><span class="p">,</span>
    <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">})</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;mul_sum(x, x):  </span><span class="si">{</span><span class="n">t0</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">1e6</span><span class="si">:</span><span class="s1">&gt;5.1f</span><span class="si">}</span><span class="s1"> us&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;bmm(x, x):      </span><span class="si">{</span><span class="n">t1</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">1e6</span><span class="si">:</span><span class="s1">&gt;5.1f</span><span class="si">}</span><span class="s1"> us&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id1">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id1" title="Permalink to this code">Â¶</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> mul_sum(x, x):  111.6 us
 bmm(x, x):       70.0 us
</pre></div>
</div>
</div>
</div>
<div class="section" id="benchmarking-with-torch-utils-benchmark-timer">
<h3>3. Benchmarking with <code class="docutils literal notranslate"><span class="pre">torch.utils.benchmark.Timer</span></code><a class="headerlink" href="#benchmarking-with-torch-utils-benchmark-timer" title="Permalink to this heading">Â¶</a></h3>
<p>PyTorch <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> module was designed to be familiar to those who
have used the <code class="docutils literal notranslate"><span class="pre">timeit</span></code> module before. However, its defaults make it
easier and safer to use for benchmarking PyTorch code. Letâ€™s first
compare the same basic API as above.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.utils.benchmark</span> <span class="k">as</span> <span class="nn">benchmark</span>

<span class="n">t0</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer" title="torch.utils.benchmark.Timer" class="sphx-glr-backref-module-torch-utils-benchmark sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span></a><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_mul_sum(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_mul_sum&#39;</span><span class="p">,</span>
    <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">})</span>

<span class="n">t1</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer" title="torch.utils.benchmark.Timer" class="sphx-glr-backref-module-torch-utils-benchmark sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span></a><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_bmm(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_bmm&#39;</span><span class="p">,</span>
    <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">})</span>

<span class="nb">print</span><span class="p">(</span><span class="n">t0</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t1</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id2">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id2" title="Permalink to this code">Â¶</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d0f0&gt;
 batched_dot_mul_sum(x, x)
 setup: from __main__ import batched_dot_mul_sum
   379.29 us
   1 measurement, 100 runs , 1 thread
 &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb103d67048&gt;
 batched_dot_bmm(x, x)
 setup: from __main__ import batched_dot_bmm
   716.42 us
   1 measurement, 100 runs , 1 thread
</pre></div>
</div>
</div>
<p>Even though the APIs are the same for the basic functionality, there
are some important differences. <code class="docutils literal notranslate"><span class="pre">benchmark.Timer.timeit()</span></code> returns the
time per run as opposed to the total runtime like <code class="docutils literal notranslate"><span class="pre">timeit.Timer.timeit()</span></code>
does. PyTorch <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> module also provides formatted string
representations for printing the results.</p>
<p>Another important difference, and the reason why the results diverge
is that PyTorch benchmark module runs in a single thread by default.
We can change the number of threads with the <code class="docutils literal notranslate"><span class="pre">num_threads</span></code> argument.</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.utils.benchmark.Timer</span></code> takes several additional arguments
including: <code class="docutils literal notranslate"><span class="pre">label</span></code>, <code class="docutils literal notranslate"><span class="pre">sub_label</span></code>, <code class="docutils literal notranslate"><span class="pre">description</span></code> and <code class="docutils literal notranslate"><span class="pre">env</span></code> which change
the __repr__ of the measurement object returned and are used for
grouping the results (more on this later).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">num_threads</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.get_num_threads.html#torch.get_num_threads" title="torch.get_num_threads" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">get_num_threads</span></a><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Benchmarking on </span><span class="si">{</span><span class="n">num_threads</span><span class="si">}</span><span class="s1"> threads&#39;</span><span class="p">)</span>

<span class="n">t0</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer" title="torch.utils.benchmark.Timer" class="sphx-glr-backref-module-torch-utils-benchmark sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span></a><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_mul_sum(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_mul_sum&#39;</span><span class="p">,</span>
    <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">},</span>
    <span class="n">num_threads</span><span class="o">=</span><span class="n">num_threads</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Multithreaded batch dot&#39;</span><span class="p">,</span>
    <span class="n">sub_label</span><span class="o">=</span><span class="s1">&#39;Implemented using mul and sum&#39;</span><span class="p">)</span>

<span class="n">t1</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer" title="torch.utils.benchmark.Timer" class="sphx-glr-backref-module-torch-utils-benchmark sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span></a><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_bmm(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_bmm&#39;</span><span class="p">,</span>
    <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">},</span>
    <span class="n">num_threads</span><span class="o">=</span><span class="n">num_threads</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Multithreaded batch dot&#39;</span><span class="p">,</span>
    <span class="n">sub_label</span><span class="o">=</span><span class="s1">&#39;Implemented using bmm&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">t0</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t1</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id3">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id3" title="Permalink to this code">Â¶</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> Benchmarking on 40 threads
 &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb103d54080&gt;
 Multithreaded batch dot: Implemented using mul and sum
 setup: from __main__ import batched_dot_mul_sum
   118.47 us
   1 measurement, 100 runs , 40 threads
 &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb16935d2e8&gt;
 Multithreaded batch dot: Implemented using bmm
 setup: from __main__ import batched_dot_bmm
   68.21 us
   1 measurement, 100 runs , 40 threads
</pre></div>
</div>
</div>
<p>Running <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> with all threads available gives similar results
as the <code class="docutils literal notranslate"><span class="pre">timeit</span></code> module. More importantly, which version is faster
depends on how many threads we run the code with. This is why itâ€™s
important to benchmark the code with thread settings that are
representative of real use cases. Another important thing to remember
is to synchronize CPU and CUDA when benchmarking on the GPU. Letâ€™s run
the above benchmarks again on a CUDA tensor and see what happens.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn" title="torch.randn" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span></a><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>

<span class="n">t0</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_mul_sum(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_mul_sum&#39;</span><span class="p">,</span>
    <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">})</span>

<span class="n">t1</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">Timer</span><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_bmm(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_bmm&#39;</span><span class="p">,</span>
    <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">})</span>

<span class="c1"># Ran each twice to show difference before/after warm-up</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;mul_sum(x, x):  </span><span class="si">{</span><span class="n">t0</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">1e6</span><span class="si">:</span><span class="s1">&gt;5.1f</span><span class="si">}</span><span class="s1"> us&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;mul_sum(x, x):  </span><span class="si">{</span><span class="n">t0</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">1e6</span><span class="si">:</span><span class="s1">&gt;5.1f</span><span class="si">}</span><span class="s1"> us&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;bmm(x, x):      </span><span class="si">{</span><span class="n">t1</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">1e6</span><span class="si">:</span><span class="s1">&gt;5.1f</span><span class="si">}</span><span class="s1"> us&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;bmm(x, x):      </span><span class="si">{</span><span class="n">t1</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">1e6</span><span class="si">:</span><span class="s1">&gt;5.1f</span><span class="si">}</span><span class="s1"> us&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id4">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id4" title="Permalink to this code">Â¶</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> mul_sum(x, x):   27.6 us
 mul_sum(x, x):   25.3 us
 bmm(x, x):      2775.5 us
 bmm(x, x):       22.4 us
</pre></div>
</div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">t0</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer" title="torch.utils.benchmark.Timer" class="sphx-glr-backref-module-torch-utils-benchmark sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span></a><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_mul_sum(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_mul_sum&#39;</span><span class="p">,</span>
    <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">})</span>

<span class="n">t1</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer" title="torch.utils.benchmark.Timer" class="sphx-glr-backref-module-torch-utils-benchmark sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span></a><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_bmm(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_bmm&#39;</span><span class="p">,</span>
    <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">})</span>

<span class="c1"># Run only once since benchmark module does warm-up for us</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t0</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">t1</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id5">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id5" title="Permalink to this code">Â¶</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d080&gt;
 batched_dot_mul_sum(x, x)
 setup: from __main__ import batched_dot_mul_sum
   232.93 us
   1 measurement, 100 runs , 1 thread
 &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d0f0&gt;
 batched_dot_bmm(x, x)
 setup: from __main__ import batched_dot_bmm
   181.04 us
   1 measurement, 100 runs , 1 thread
</pre></div>
</div>
</div>
<p>The results reveal something interesting. The first run of the <code class="docutils literal notranslate"><span class="pre">bmm</span></code>
version using the <code class="docutils literal notranslate"><span class="pre">timeit</span></code> module takes much longer than the second
run. This is because <code class="docutils literal notranslate"><span class="pre">bmm</span></code> calls into <cite>cuBLAS</cite> which needs to be
loaded the first time itâ€™s called which takes some time. This is why
itâ€™s important to do a warm-up run before benchmarking, luckily for
us, PyTorchâ€™s <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> module takes care of that.</p>
<p>The difference in the results between <code class="docutils literal notranslate"><span class="pre">timeit</span></code> and <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> modules
is because the <cite>timeit</cite> module is not synchronizing CUDA and is thus only
timing the time to launch the kernel. PyTorchâ€™s <code class="docutils literal notranslate"><span class="pre">benchmark</span></code> module does
the synchronization for us.</p>
</div>
<div class="section" id="benchmarking-with-blocked-autorange">
<h3>4. Benchmarking with <cite>Blocked Autorange</cite><a class="headerlink" href="#benchmarking-with-blocked-autorange" title="Permalink to this heading">Â¶</a></h3>
<p>While <code class="docutils literal notranslate"><span class="pre">timeit.Timer.autorange</span></code> takes a single continuous measurement
of at least 0.2 seconds, <cite>torch.utils.benchmark.blocked_autorange</cite>
takes many measurements whose times total at least 0.2 seconds (which
can be changed by the <cite>min_run_time</cite> parameter) subject to the constraint
that timing overhead is a small fraction of the overall measurement.
This is accomplished by first running with an increasing number of runs
per loop until the runtime is much larger than measurement overhead
(which also serves as a warm up), and then taking measurements until
the target time is reached. This has the useful properties that it wastes
less data and allows us to compute statistics to estimate the reliability
of the measurements.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">m0</span> <span class="o">=</span> <span class="n">t0</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">()</span>
<span class="n">m1</span> <span class="o">=</span> <span class="n">t1</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">m0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m1</span><span class="p">)</span>
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id6">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id6" title="Permalink to this code">Â¶</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d0f0&gt;
 batched_dot_mul_sum(x, x)
 setup: from __main__ import batched_dot_mul_sum
   231.79 us
   1 measurement, 1000 runs , 1 thread
 &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d080&gt;
 batched_dot_bmm(x, x)
 setup: from __main__ import batched_dot_bmm
   Median: 162.08 us
   2 measurements, 1000 runs per measurement, 1 thread
</pre></div>
</div>
</div>
<p>We can also inspect the individual statistics from the returned
measurements object.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean:   </span><span class="si">{</span><span class="n">m0</span><span class="o">.</span><span class="n">mean</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">1e6</span><span class="si">:</span><span class="s2">6.2f</span><span class="si">}</span><span class="s2"> us&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Median: </span><span class="si">{</span><span class="n">m0</span><span class="o">.</span><span class="n">median</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">1e6</span><span class="si">:</span><span class="s2">6.2f</span><span class="si">}</span><span class="s2"> us&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id7">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id7" title="Permalink to this code">Â¶</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> Mean:   231.79 us
 Median: 231.79 us
</pre></div>
</div>
</div>
</div>
<div class="section" id="comparing-benchmark-results">
<h3>5. Comparing benchmark results<a class="headerlink" href="#comparing-benchmark-results" title="Permalink to this heading">Â¶</a></h3>
<p>So far weâ€™ve been comparing our two versions of batched dot against a
single input. In practice, we want to try a combination of inputs as
well as different number of threads. The <code class="docutils literal notranslate"><span class="pre">Compare</span></code> class helps display
the results of many measurements in a formatted table. It uses the
annotations described above (<cite>label</cite>, <cite>sub_label</cite>, <cite>num_threads</cite>, etc.) as
well as <cite>description</cite> to group and organize the table. Letâ€™s use
<code class="docutils literal notranslate"><span class="pre">Compare</span></code> to see how our functions perform for different input sizes
and number of threads.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">product</span>

<span class="c1"># Compare takes a list of measurements which we&#39;ll save in results.</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">10000</span><span class="p">]</span>
<span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">product</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">sizes</span><span class="p">):</span>
    <span class="c1"># label and sub_label are the rows</span>
    <span class="c1"># description is the column</span>
    <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Batched dot&#39;</span>
    <span class="n">sub_label</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;[</span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s1">]&#39;</span>
    <span class="n">x</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones" title="torch.ones" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span></a><span class="p">((</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">num_threads</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">]:</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer" title="torch.utils.benchmark.Timer" class="sphx-glr-backref-module-torch-utils-benchmark sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span></a><span class="p">(</span>
            <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_mul_sum(x, x)&#39;</span><span class="p">,</span>
            <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_mul_sum&#39;</span><span class="p">,</span>
            <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">},</span>
            <span class="n">num_threads</span><span class="o">=</span><span class="n">num_threads</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">,</span>
            <span class="n">sub_label</span><span class="o">=</span><span class="n">sub_label</span><span class="p">,</span>
            <span class="n">description</span><span class="o">=</span><span class="s1">&#39;mul/sum&#39;</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">(</span><span class="n">min_run_time</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer" title="torch.utils.benchmark.Timer" class="sphx-glr-backref-module-torch-utils-benchmark sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span></a><span class="p">(</span>
            <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_bmm(x, x)&#39;</span><span class="p">,</span>
            <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_bmm&#39;</span><span class="p">,</span>
            <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">},</span>
            <span class="n">num_threads</span><span class="o">=</span><span class="n">num_threads</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">,</span>
            <span class="n">sub_label</span><span class="o">=</span><span class="n">sub_label</span><span class="p">,</span>
            <span class="n">description</span><span class="o">=</span><span class="s1">&#39;bmm&#39;</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">(</span><span class="n">min_run_time</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="n">compare</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">Compare</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
<span class="n">compare</span><span class="o">.</span><span class="n">print</span><span class="p">()</span>
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id8">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id8" title="Permalink to this code">Â¶</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> [--------------- Batched dot ----------------]
                       |  mul/sum   |    bmm
 1 threads: -----------------------------------
       [1, 1]          |       5.9  |      11.2
       [1, 64]         |       6.4  |      11.4
       [1, 1024]       |       6.7  |      14.2
       [1, 10000]      |      10.2  |      23.7
       [64, 1]         |       6.3  |      11.5
       [64, 64]        |       8.6  |      15.4
       [64, 1024]      |      39.4  |     204.4
       [64, 10000]     |     274.9  |     748.5
       [1024, 1]       |       7.7  |      17.8
       [1024, 64]      |      40.3  |      76.4
       [1024, 1024]    |     432.4  |    2795.9
       [1024, 10000]   |   22657.3  |   11899.5
       [10000, 1]      |      16.9  |      74.8
       [10000, 64]     |     300.3  |     609.4
       [10000, 1024]   |   23098.6  |   27246.1
       [10000, 10000]  |  267073.7  |  118823.7
 4 threads: -----------------------------------
       [1, 1]          |       6.0  |      11.5
       [1, 64]         |       6.2  |      11.2
       [1, 1024]       |       6.8  |      14.3
       [1, 10000]      |      10.2  |      23.7
       [64, 1]         |       6.3  |      16.2
       [64, 64]        |       8.8  |      18.2
       [64, 1024]      |      41.5  |     189.1
       [64, 10000]     |      91.7  |     849.1
       [1024, 1]       |       7.6  |      17.4
       [1024, 64]      |      43.5  |      33.5
       [1024, 1024]    |     135.4  |    2782.3
       [1024, 10000]   |    7471.1  |   11874.0
       [10000, 1]      |      16.8  |      33.9
       [10000, 64]     |     118.7  |     173.2
       [10000, 1024]   |    7264.6  |   27824.7
       [10000, 10000]  |  100060.9  |  121499.0
 16 threads: ----------------------------------
       [1, 1]          |       6.0  |      11.3
       [1, 64]         |       6.2  |      11.2
       [1, 1024]       |       6.9  |      14.2
       [1, 10000]      |      10.3  |      23.8
       [64, 1]         |       6.4  |      24.1
       [64, 64]        |       9.0  |      23.8
       [64, 1024]      |      54.1  |     188.5
       [64, 10000]     |      49.9  |     748.0
       [1024, 1]       |       7.6  |      23.4
       [1024, 64]      |      55.5  |      28.2
       [1024, 1024]    |      66.9  |    2773.9
       [1024, 10000]   |    6111.5  |   12833.7
       [10000, 1]      |      16.9  |      27.5
       [10000, 64]     |      59.5  |      73.7
       [10000, 1024]   |    6295.9  |   27062.0
       [10000, 10000]  |   71804.5  |  120365.8
 32 threads: ----------------------------------
       [1, 1]          |       5.9  |      11.3
       [1, 64]         |       6.2  |      11.3
       [1, 1024]       |       6.7  |      14.2
       [1, 10000]      |      10.5  |      23.8
       [64, 1]         |       6.3  |      31.7
       [64, 64]        |       9.1  |      30.4
       [64, 1024]      |      72.0  |     190.4
       [64, 10000]     |     103.1  |     746.9
       [1024, 1]       |       7.6  |      28.4
       [1024, 64]      |      70.5  |      31.9
       [1024, 1024]    |      65.6  |    2804.6
       [1024, 10000]   |    6764.0  |   11871.4
       [10000, 1]      |      17.8  |      31.8
       [10000, 64]     |     110.3  |      56.0
       [10000, 1024]   |    6640.2  |   27592.2
       [10000, 10000]  |   73003.4  |  120083.2

 Times are in microseconds (us).
</pre></div>
</div>
</div>
<p>The results above indicate that the version which reduces to <code class="docutils literal notranslate"><span class="pre">bmm</span></code>
is better for larger tensors running on multiple threads, while for
smaller and/or single thread code, the other version is better.</p>
<p><code class="docutils literal notranslate"><span class="pre">Compare</span></code> also provides functions for changing the table format</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">compare</span><span class="o">.</span><span class="n">trim_significant_figures</span><span class="p">()</span>
<span class="n">compare</span><span class="o">.</span><span class="n">colorize</span><span class="p">()</span>
<span class="n">compare</span><span class="o">.</span><span class="n">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="saving-loading-benchmark-results">
<h3>6. Saving/Loading benchmark results<a class="headerlink" href="#saving-loading-benchmark-results" title="Permalink to this heading">Â¶</a></h3>
<p><cite>Measurements</cite> (and <code class="docutils literal notranslate"><span class="pre">CallgrindStats</span></code> which are described in section 8)
can be serialized by the <code class="docutils literal notranslate"><span class="pre">pickle</span></code> module. This makes A/B testing easy, as you can collect
measurements from two separate environments, pickle them, and then
load both in a single environment. Timer even takes an <cite>env</cite>
constructor argument so that such A/B testing works seamlessly.</p>
<p>Letâ€™s imagine that rather than two Python functions, the add/sum
and <code class="docutils literal notranslate"><span class="pre">bmm</span></code> approaches were in two different builds of PyTorch.
The example below demonstrates how one might A/B test them. For
simplicity, we only use a subset of shapes, and simply round trip
results through pickle rather than actually using multiple environments
and writing results to disk.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>

<span class="n">ab_test_results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">env</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;environment A: mul/sum&#39;</span><span class="p">,</span> <span class="s1">&#39;environment B: bmm&#39;</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">10000</span><span class="p">),</span> <span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">1</span><span class="p">)):</span>
        <span class="n">x</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones" title="torch.ones" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">ones</span></a><span class="p">((</span><span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
        <span class="n">dot_fn</span> <span class="o">=</span> <span class="p">(</span><span class="n">batched_dot_mul_sum</span> <span class="k">if</span> <span class="n">env</span> <span class="o">==</span> <span class="s1">&#39;environment A: mul/sum&#39;</span> <span class="k">else</span> <span class="n">batched_dot_bmm</span><span class="p">)</span>
        <span class="n">m</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer" title="torch.utils.benchmark.Timer" class="sphx-glr-backref-module-torch-utils-benchmark sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span></a><span class="p">(</span>
            <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot(x, x)&#39;</span><span class="p">,</span>
            <span class="nb">globals</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="s1">&#39;batched_dot&#39;</span><span class="p">:</span> <span class="n">dot_fn</span><span class="p">},</span>
            <span class="n">num_threads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Batched dot&#39;</span><span class="p">,</span>
            <span class="n">description</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;[</span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s1">]&#39;</span><span class="p">,</span>
            <span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">(</span><span class="n">min_run_time</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">ab_test_results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pickle</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">m</span><span class="p">))</span>

<span class="n">ab_results</span> <span class="o">=</span> <span class="p">[</span><span class="n">pickle</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ab_test_results</span><span class="p">]</span>
<span class="n">compare</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">Compare</span><span class="p">(</span><span class="n">ab_results</span><span class="p">)</span>
<span class="n">compare</span><span class="o">.</span><span class="n">trim_significant_figures</span><span class="p">()</span>
<span class="n">compare</span><span class="o">.</span><span class="n">colorize</span><span class="p">()</span>
<span class="n">compare</span><span class="o">.</span><span class="n">print</span><span class="p">()</span>
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id9">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id9" title="Permalink to this code">Â¶</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> [------------------------------------- Batched dot -------------------------------------]
                                                |  [1, 1]  |  [1024, 10000]  |  [10000, 1]
 1 threads: ------------------------------------------------------------------------------
   (environment A: mul/sum)  batched_dot(x, x)  |     7    |      36000      |      21
   (environment B: bmm)      batched_dot(x, x)  |    14    |      40000      |      85

 Times are in microseconds (us).
</pre></div>
</div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># And just to show that we can round trip all of the results from earlier:</span>
<span class="n">round_tripped_results</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">pickle</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">results</span><span class="p">))</span>
<span class="k">assert</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">benchmark</span><span class="o">.</span><span class="n">Compare</span><span class="p">(</span><span class="n">results</span><span class="p">))</span> <span class="o">==</span> <span class="nb">str</span><span class="p">(</span><span class="n">benchmark</span><span class="o">.</span><span class="n">Compare</span><span class="p">(</span><span class="n">round_tripped_results</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="section" id="generating-inputs-with-fuzzed-parameters">
<h3>7. Generating inputs with <cite>Fuzzed Parameters</cite><a class="headerlink" href="#generating-inputs-with-fuzzed-parameters" title="Permalink to this heading">Â¶</a></h3>
<p>As weâ€™ve seen in the previous section, there can be some stark
performance differences depending on the input tensors. Hence, it
is a good idea to run benchmarks on a number of different inputs.
However, creating all these input tensors can be tedious which is
where <code class="docutils literal notranslate"><span class="pre">torch.utils.benchmark.Fuzzer</span></code> and related classes come in.
Letâ€™s take a look at how we can use the <code class="docutils literal notranslate"><span class="pre">Fuzzer</span></code> to create some test
cases for the benchmark.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.benchmark</span> <span class="kn">import</span> <span class="n">Fuzzer</span><span class="p">,</span> <span class="n">FuzzedParameter</span><span class="p">,</span> <span class="n">FuzzedTensor</span><span class="p">,</span> <span class="n">ParameterAlias</span>

<span class="c1"># Generates random tensors with 128 to 10000000 elements and sizes k0 and k1 chosen from a</span>
<span class="c1"># ``loguniform`` distribution in [1, 10000], 40% of which will be discontiguous on average.</span>
<span class="n">example_fuzzer</span> <span class="o">=</span> <span class="n">Fuzzer</span><span class="p">(</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">FuzzedParameter</span><span class="p">(</span><span class="s1">&#39;k0&#39;</span><span class="p">,</span> <span class="n">minval</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">distribution</span><span class="o">=</span><span class="s1">&#39;loguniform&#39;</span><span class="p">),</span>
        <span class="n">FuzzedParameter</span><span class="p">(</span><span class="s1">&#39;k1&#39;</span><span class="p">,</span> <span class="n">minval</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">distribution</span><span class="o">=</span><span class="s1">&#39;loguniform&#39;</span><span class="p">),</span>
    <span class="p">],</span>
    <span class="n">tensors</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">FuzzedTensor</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;k0&#39;</span><span class="p">,</span> <span class="s1">&#39;k1&#39;</span><span class="p">),</span> <span class="n">min_elements</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">max_elements</span><span class="o">=</span><span class="mi">10000000</span><span class="p">,</span> <span class="n">probability_contiguous</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
    <span class="p">],</span>
    <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">tensors</span><span class="p">,</span> <span class="n">tensor_params</span><span class="p">,</span> <span class="n">params</span> <span class="ow">in</span> <span class="n">example_fuzzer</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="c1"># description is the column label</span>
    <span class="n">sub_label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;k0&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;6</span><span class="si">}</span><span class="s2"> x </span><span class="si">{</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;k1&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;4</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">tensor_params</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">][</span><span class="s1">&#39;is_contiguous&#39;</span><span class="p">]</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;(discontiguous)&#39;</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer" title="torch.utils.benchmark.Timer" class="sphx-glr-backref-module-torch-utils-benchmark sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span></a><span class="p">(</span>
        <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_mul_sum(x, x)&#39;</span><span class="p">,</span>
        <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_mul_sum&#39;</span><span class="p">,</span>
        <span class="nb">globals</span><span class="o">=</span><span class="n">tensors</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Batched dot&#39;</span><span class="p">,</span>
        <span class="n">sub_label</span><span class="o">=</span><span class="n">sub_label</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s1">&#39;mul/sum&#39;</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">(</span><span class="n">min_run_time</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer" title="torch.utils.benchmark.Timer" class="sphx-glr-backref-module-torch-utils-benchmark sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span></a><span class="p">(</span>
        <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_bmm(x, x)&#39;</span><span class="p">,</span>
        <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_bmm&#39;</span><span class="p">,</span>
        <span class="nb">globals</span><span class="o">=</span><span class="n">tensors</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Batched dot&#39;</span><span class="p">,</span>
        <span class="n">sub_label</span><span class="o">=</span><span class="n">sub_label</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s1">&#39;bmm&#39;</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">(</span><span class="n">min_run_time</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="n">compare</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">Compare</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
<span class="n">compare</span><span class="o">.</span><span class="n">trim_significant_figures</span><span class="p">()</span>
<span class="n">compare</span><span class="o">.</span><span class="n">print</span><span class="p">()</span>
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id10">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id10" title="Permalink to this code">Â¶</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> [--------------------- Batched dot ---------------------]
                                      |  mul/sum  |   bmm
 1 threads: ----------------------------------------------
       725    x 257                   |      87   |    180
       49     x 383                   |      15   |     30
       34     x 1468                  |      30   |    118
       187    x 5039                  |     400   |   1200
       2140   x 1296 (discontiguous)  |    2000   |  41000
       78     x 1598                  |      74   |    310
       519    x 763                   |     190   |   1500
       141    x 1082                  |      87   |    500
       78     x 5    (discontiguous)  |       9   |     20
       187    x 1                     |      12   |     10

 Times are in microseconds (us).
</pre></div>
</div>
</div>
<p>There is a lot of flexibility for defining your own <code class="docutils literal notranslate"><span class="pre">fuzzers</span></code> which
is great for creating a powerful set of inputs to benchmark. But to
make things even simpler, PyTorch benchmark module comes with some
built-in <code class="docutils literal notranslate"><span class="pre">fuzzers</span></code> for common benchmarking needs. Letâ€™s take a look at
how we can use one of these built-in <code class="docutils literal notranslate"><span class="pre">fuzzers</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.benchmark.op_fuzzers</span> <span class="kn">import</span> <span class="n">binary</span>

<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">tensors</span><span class="p">,</span> <span class="n">tensor_params</span><span class="p">,</span> <span class="n">params</span> <span class="ow">in</span> <span class="n">binary</span><span class="o">.</span><span class="n">BinaryOpFuzzer</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">sub_label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;k0&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;6</span><span class="si">}</span><span class="s2"> x </span><span class="si">{</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;k1&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">&lt;4</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="s1">&#39;&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">tensor_params</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">][</span><span class="s1">&#39;is_contiguous&#39;</span><span class="p">]</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;(discontiguous)&#39;</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer" title="torch.utils.benchmark.Timer" class="sphx-glr-backref-module-torch-utils-benchmark sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span></a><span class="p">(</span>
        <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_mul_sum(x, x)&#39;</span><span class="p">,</span>
        <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_mul_sum&#39;</span><span class="p">,</span>
        <span class="nb">globals</span><span class="o">=</span><span class="n">tensors</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Batched dot&#39;</span><span class="p">,</span>
        <span class="n">sub_label</span><span class="o">=</span><span class="n">sub_label</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s1">&#39;mul/sum&#39;</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">(</span><span class="n">min_run_time</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer" title="torch.utils.benchmark.Timer" class="sphx-glr-backref-module-torch-utils-benchmark sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span></a><span class="p">(</span>
        <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_bmm(x, x)&#39;</span><span class="p">,</span>
        <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;from __main__ import batched_dot_bmm&#39;</span><span class="p">,</span>
        <span class="nb">globals</span><span class="o">=</span><span class="n">tensors</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Batched dot&#39;</span><span class="p">,</span>
        <span class="n">sub_label</span><span class="o">=</span><span class="n">sub_label</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s1">&#39;bmm&#39;</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">(</span><span class="n">min_run_time</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="n">compare</span> <span class="o">=</span> <span class="n">benchmark</span><span class="o">.</span><span class="n">Compare</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
<span class="n">compare</span><span class="o">.</span><span class="n">trim_significant_figures</span><span class="p">()</span>
<span class="n">compare</span><span class="o">.</span><span class="n">colorize</span><span class="p">(</span><span class="n">rowwise</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">compare</span><span class="o">.</span><span class="n">print</span><span class="p">()</span>
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id11">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id11" title="Permalink to this code">Â¶</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> [----------------------- Batched dot ------------------------]
                                          |  mul/sum  |   bmm
 1 threads: ---------------------------------------------------
       64     x 473  (discontiguous)      |    10000  |   40000
       16384  x 12642115 (discontiguous)  |       31  |      78
       8192   x 892                       |     4800  |   20400
       512    x 64   (discontiguous)      |   110000  |  400000
       493    x 27   (discontiguous)      |     1100  |    2440
       118    x 32   (discontiguous)      |      870  |    2030
       16     x 495  (discontiguous)      |    23600  |   24000
       488    x 62374                     |    90000  |  100000
       240372 x 69                        |    40000  |   16000
       40156  x 32   (discontiguous)      |     2670  |    5000

 Times are in microseconds (us).
</pre></div>
</div>
</div>
</div>
<div class="section" id="collecting-instruction-counts-with-callgrind">
<h3>8. Collecting instruction counts with <code class="docutils literal notranslate"><span class="pre">Callgrind</span></code><a class="headerlink" href="#collecting-instruction-counts-with-callgrind" title="Permalink to this heading">Â¶</a></h3>
<p>One of the challenges of optimizing code is the variation and opacity of
wall time. There are many sources of non-determinism, from adaptive clock
speeds to resource contention with other processes. Furthermore, end-to-end
time gives no insight into where time is being spent, which is really what
weâ€™re interested in when optimizing code.</p>
<p>A complementary approach is to also collect instruction counts. These counts
are a proxy metric and do not capture all aspects of performance
(e.g. memory or I/O bound tasks), however they do have several useful
properties. Instruction counts are reproducible, insensitive to environmental
variation, and offer fine grained insight into where a program is spending
cycles.</p>
<p>To see the utility of instruction counts, let us look at how we might
reduce the overhead of <cite>batched_dot_mul_sum</cite>. The obvious solution is to
move it to C++, so we avoid going between Python and C++ multiple times.</p>
<p>Fortunately, the source is nearly identical. One question that we have to ask
in C++ is whether we should take arguments by value or reference.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">batched_dot_src</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2">/* ---- Python ---- */</span>
<span class="s2">// def batched_dot_mul_sum(a, b):</span>
<span class="s2">//     return a.mul(b).sum(-1)</span>

<span class="s2">torch::Tensor batched_dot_mul_sum_v0(</span>
<span class="s2">    const torch::Tensor a,</span>
<span class="s2">    const torch::Tensor b) {</span>
<span class="s2">  return a.mul(b).sum(-1);</span>
<span class="s2">}</span>

<span class="s2">torch::Tensor batched_dot_mul_sum_v1(</span>
<span class="s2">    const torch::Tensor&amp; a,</span>
<span class="s2">    const torch::Tensor&amp; b) {</span>
<span class="s2">  return a.mul(b).sum(-1);</span>
<span class="s2">}</span>
<span class="s2">&quot;&quot;&quot;</span>


<span class="c1"># PyTorch makes it easy to test our C++ implementations by providing a utility</span>
<span class="c1"># to JIT compile C++ source into Python extensions:</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">torch.utils</span> <span class="kn">import</span> <span class="n">cpp_extension</span>
<span class="n">cpp_lib</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load_inline" title="torch.utils.cpp_extension.load_inline" class="sphx-glr-backref-module-torch-utils-cpp_extension sphx-glr-backref-type-py-function"><span class="n">cpp_extension</span><span class="o">.</span><span class="n">load_inline</span></a><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;cpp_lib&#39;</span><span class="p">,</span>
    <span class="n">cpp_sources</span><span class="o">=</span><span class="n">batched_dot_src</span><span class="p">,</span>
    <span class="n">extra_cflags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;-O3&#39;</span><span class="p">],</span>
    <span class="n">extra_include_paths</span><span class="o">=</span><span class="p">[</span>
        <span class="c1"># `load_inline` needs to know where to find ``pybind11`` headers.</span>
        <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s1">&#39;CONDA_PREFIX&#39;</span><span class="p">),</span> <span class="s1">&#39;include&#39;</span><span class="p">)</span>
    <span class="p">],</span>
    <span class="n">functions</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;batched_dot_mul_sum_v0&#39;</span><span class="p">,</span> <span class="s1">&#39;batched_dot_mul_sum_v1&#39;</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># `load_inline` will create a shared object that is loaded into Python. When we collect</span>
<span class="c1"># instruction counts Timer will create a subprocess, so we need to re-import it. The</span>
<span class="c1"># import process is slightly more complicated for C extensions, but that&#39;s all we&#39;re</span>
<span class="c1"># doing here.</span>
<span class="n">module_import_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span><span class="se">\</span>
<span class="s2"># https://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path</span>
<span class="s2">import importlib.util</span>
<span class="s2">spec = importlib.util.spec_from_file_location(&quot;cpp_lib&quot;, </span><span class="si">{</span><span class="nb">repr</span><span class="p">(</span><span class="n">cpp_lib</span><span class="o">.</span><span class="vm">__file__</span><span class="p">)</span><span class="si">}</span><span class="s2">)</span>
<span class="s2">cpp_lib = importlib.util.module_from_spec(spec)</span>
<span class="s2">spec.loader.exec_module(cpp_lib)&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">textwrap</span>
<span class="k">def</span> <span class="nf">pretty_print</span><span class="p">(</span><span class="n">result</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Import machinery for ``cpp_lib.so`` can get repetitive to look at.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">result</span><span class="p">)</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">textwrap</span><span class="o">.</span><span class="n">indent</span><span class="p">(</span><span class="n">module_import_str</span><span class="p">,</span> <span class="s2">&quot;  &quot;</span><span class="p">),</span> <span class="s2">&quot;  import cpp_lib&quot;</span><span class="p">))</span>


<span class="n">t_baseline</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer" title="torch.utils.benchmark.Timer" class="sphx-glr-backref-module-torch-utils-benchmark sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span></a><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;batched_dot_mul_sum(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="s1">&#39;&#39;&#39;</span><span class="se">\</span>
<span class="s1">from __main__ import batched_dot_mul_sum</span>
<span class="s1">x = torch.randn(2, 2)&#39;&#39;&#39;</span><span class="p">)</span>

<span class="n">t0</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer" title="torch.utils.benchmark.Timer" class="sphx-glr-backref-module-torch-utils-benchmark sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span></a><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;cpp_lib.batched_dot_mul_sum_v0(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;&#39;&#39;</span><span class="se">\</span>
<span class="si">{</span><span class="n">module_import_str</span><span class="si">}</span>
<span class="s1">x = torch.randn(2, 2)&#39;&#39;&#39;</span><span class="p">)</span>

<span class="n">t1</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/benchmark_utils.html#torch.utils.benchmark.Timer" title="torch.utils.benchmark.Timer" class="sphx-glr-backref-module-torch-utils-benchmark sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">benchmark</span><span class="o">.</span><span class="n">Timer</span></a><span class="p">(</span>
    <span class="n">stmt</span><span class="o">=</span><span class="s1">&#39;cpp_lib.batched_dot_mul_sum_v1(x, x)&#39;</span><span class="p">,</span>
    <span class="n">setup</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;&#39;&#39;</span><span class="se">\</span>
<span class="si">{</span><span class="n">module_import_str</span><span class="si">}</span>
<span class="s1">x = torch.randn(2, 2)&#39;&#39;&#39;</span><span class="p">)</span>

<span class="c1"># Moving to C++ did indeed reduce overhead, but it&#39;s hard to tell which</span>
<span class="c1"># calling convention is more efficient. v1 (call with references) seems to</span>
<span class="c1"># be a bit faster, but it&#39;s within measurement error.</span>
<span class="n">pretty_print</span><span class="p">(</span><span class="n">t_baseline</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">())</span>
<span class="n">pretty_print</span><span class="p">(</span><span class="n">t0</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">())</span>
<span class="n">pretty_print</span><span class="p">(</span><span class="n">t1</span><span class="o">.</span><span class="n">blocked_autorange</span><span class="p">())</span>
</pre></div>
</div>
<div class="literal-block-wrapper docutils container" id="id12">
<div class="code-block-caption"><span class="caption-text">Output</span><a class="headerlink" href="#id12" title="Permalink to this code">Â¶</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb16935d2e8&gt;
 batched_dot_mul_sum(x, x)
 setup:
   from __main__ import batched_dot_mul_sum
   x = torch.randn(2, 2)

   6.92 us
   1 measurement, 100000 runs , 1 thread
 &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb16935d2e8&gt;
 cpp_lib.batched_dot_mul_sum_v0(x, x)
 setup:
   import cpp_lib
   x = torch.randn(2, 2)

   5.29 us
   1 measurement, 100000 runs , 1 thread
 &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb16935d2e8&gt;
 cpp_lib.batched_dot_mul_sum_v1(x, x)
 setup:
   import cpp_lib
   x = torch.randn(2, 2)

   5.22 us
   1 measurement, 100000 runs , 1 thread
</pre></div>
</div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s use ``Callgrind`` to determine which is better.</span>
<span class="n">stats_v0</span> <span class="o">=</span> <span class="n">t0</span><span class="o">.</span><span class="n">collect_callgrind</span><span class="p">()</span>
<span class="n">stats_v1</span> <span class="o">=</span> <span class="n">t1</span><span class="o">.</span><span class="n">collect_callgrind</span><span class="p">()</span>

<span class="n">pretty_print</span><span class="p">(</span><span class="n">stats_v0</span><span class="p">)</span>
<span class="n">pretty_print</span><span class="p">(</span><span class="n">stats_v1</span><span class="p">)</span>

<span class="c1"># `.as_standardized` removes file names and some path prefixes, and makes</span>
<span class="c1"># it easier to read the function symbols.</span>
<span class="n">stats_v0</span> <span class="o">=</span> <span class="n">stats_v0</span><span class="o">.</span><span class="n">as_standardized</span><span class="p">()</span>
<span class="n">stats_v1</span> <span class="o">=</span> <span class="n">stats_v1</span><span class="o">.</span><span class="n">as_standardized</span><span class="p">()</span>

<span class="c1"># `.delta` diffs the instruction counts, and `.denoise` removes several</span>
<span class="c1"># functions in the Python interpreter that are known to have significant</span>
<span class="c1"># jitter.</span>
<span class="n">delta</span> <span class="o">=</span> <span class="n">stats_v1</span><span class="o">.</span><span class="n">delta</span><span class="p">(</span><span class="n">stats_v0</span><span class="p">)</span><span class="o">.</span><span class="n">denoise</span><span class="p">()</span>

<span class="c1"># `.transform` is a convenience API for transforming function names. It is</span>
<span class="c1"># useful for increasing cancelation when ``diff-ing`` instructions, as well as</span>
<span class="c1"># just generally improving readability.</span>
<span class="n">replacements</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">(</span><span class="s2">&quot;???:void pybind11&quot;</span><span class="p">,</span> <span class="s2">&quot;pybind11&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;batched_dot_mul_sum_v0&quot;</span><span class="p">,</span> <span class="s2">&quot;batched_dot_mul_sum_v1&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;at::Tensor, at::Tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;...&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;at::Tensor const&amp;, at::Tensor const&amp;&quot;</span><span class="p">,</span> <span class="s2">&quot;...&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;auto torch::detail::wrap_pybind_function_impl_&quot;</span><span class="p">,</span> <span class="s2">&quot;wrap_pybind_function_impl_&quot;</span><span class="p">),</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">before</span><span class="p">,</span> <span class="n">after</span> <span class="ow">in</span> <span class="n">replacements</span><span class="p">:</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="n">delta</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="k">lambda</span> <span class="n">l</span><span class="p">:</span> <span class="n">l</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">before</span><span class="p">,</span> <span class="n">after</span><span class="p">))</span>

<span class="c1"># We can use print options to control how much of the function to display.</span>
<a href="https://pytorch.org/docs/stable/generated/torch.set_printoptions.html#torch.set_printoptions" title="torch.set_printoptions" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">set_printoptions</span></a><span class="p">(</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">160</span><span class="p">)</span>

<span class="c1"># Once parsed, the instruction counts make clear that passing `a` and `b`</span>
<span class="c1"># by reference is more efficient as it skips some ``c10::TensorImpl`` bookkeeping</span>
<span class="c1"># for the intermediate Tensors, and is also works better with ``pybind11``. This</span>
<span class="c1"># is consistent with our noisy wall time observations.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">delta</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>&lt;torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats object at 0x7fb0f06e7630&gt;
cpp_lib.batched_dot_mul_sum_v0(x, x)
setup:
  import cpp_lib
  x = torch.randn(2, 2)
                           All          Noisy symbols removed
    Instructions:      2392671                    2392671
    Baseline:             4367                       4367
100 runs per measurement, 1 thread
Warning: PyTorch was not built with debug symbols.
         Source information may be limited. Rebuild with
         REL_WITH_DEB_INFO=1 for more detailed results.
&lt;torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats object at 0x7fb10400d208&gt;
cpp_lib.batched_dot_mul_sum_v1(x, x)
setup:
  import cpp_lib
  x = torch.randn(2, 2)
                           All          Noisy symbols removed
    Instructions:      2378978                    2378978
    Baseline:             4367                       4367
    100 runs per measurement, 1 thread
    Warning: PyTorch was not built with debug symbols.
             Source information may be limited. Rebuild with
             REL_WITH_DEB_INFO=1 for more detailed results.
    &lt;torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts object at 0x7fb1000ab358&gt;
          86  ???:0x000000000020d9e0
      56  ???:0x000000000020db10
   -1100  pybind11::cpp_function::initialize&lt;wrap_pybind_function_impl_&lt;at::Tensor ... r (&amp;)(...), std::integer_sequence&lt;unsigned long, 0ul, 1ul&gt;)::{lambda(...)
   -1600  ???:wrap_pybind_function_impl_&lt;at::Tensor (&amp;)(...), 0ul, 1ul&gt;(at::Tensor (&amp;)(...), std::integer_sequence&lt;unsigned long, 0ul, 1ul&gt;)::{lambda(...)
   -5200  ???:c10::intrusive_ptr&lt;c10::TensorImpl, c10::UndefinedTensorImpl&gt;::reset_()
   -5935  ???:0x000000000022c0e0
Total: -13693
</pre></div>
</div>
</div>
</div>
<div class="section" id="learn-more">
<h2>Learn More<a class="headerlink" href="#learn-more" title="Permalink to this heading">Â¶</a></h2>
<p>Take a look at these other recipes to continue your learning:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/tutorials/recipes/recipes/profiler.html">PyTorch Profiler</a></p></li>
</ul>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-recipes-recipes-benchmark-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/72c2f17ac50228049705f9a4d76c7815/benchmark.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">benchmark.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/54db51700fabe094cbf7f11f5195d2bd/benchmark.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">benchmark.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>


             </article>
             
            </div>
            <footer>
  

  

    <hr class="rating-hr hr-top">
      <div class="rating-container">
        <div class="rating-prompt">Rate this Tutorial</div>
        <div class="stars-outer">
          <i class="far fa-star" title="1 Star" data-behavior="tutorial-rating" data-count="1"></i>
          <i class="far fa-star" title="2 Stars" data-behavior="tutorial-rating" data-count="2"></i>
          <i class="far fa-star" title="3 Stars" data-behavior="tutorial-rating" data-count="3"></i>
          <i class="far fa-star" title="4 Stars" data-behavior="tutorial-rating" data-count="4"></i>
          <i class="far fa-star" title="5 Stars" data-behavior="tutorial-rating" data-count="5"></i>
        </div>
      </div>
    <hr class="rating-hr hr-bottom"/>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2024, PyTorch.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>
if((window.location.href.indexOf("/prototype/")!= -1) && (window.location.href.indexOf("/prototype/prototype_index")< 1))
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-flask" aria-hidden="true">&nbsp</i> This tutorial describes a prototype feature. Prototype features are typically not available as part of binary distributions like PyPI or Conda, except sometimes behind run-time flags, and are at an early stage for feedback and testing.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  } 
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">PyTorch Benchmark</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#setup">Setup</a></li>
<li><a class="reference internal" href="#steps">Steps</a><ul>
<li><a class="reference internal" href="#defining-functions-to-benchmark">1. Defining functions to benchmark</a></li>
<li><a class="reference internal" href="#benchmarking-with-timeit-timer">2. Benchmarking with <code class="docutils literal notranslate"><span class="pre">timeit.Timer</span></code></a></li>
<li><a class="reference internal" href="#benchmarking-with-torch-utils-benchmark-timer">3. Benchmarking with <code class="docutils literal notranslate"><span class="pre">torch.utils.benchmark.Timer</span></code></a></li>
<li><a class="reference internal" href="#benchmarking-with-blocked-autorange">4. Benchmarking with <cite>Blocked Autorange</cite></a></li>
<li><a class="reference internal" href="#comparing-benchmark-results">5. Comparing benchmark results</a></li>
<li><a class="reference internal" href="#saving-loading-benchmark-results">6. Saving/Loading benchmark results</a></li>
<li><a class="reference internal" href="#generating-inputs-with-fuzzed-parameters">7. Generating inputs with <cite>Fuzzed Parameters</cite></a></li>
<li><a class="reference internal" href="#collecting-instruction-counts-with-callgrind">8. Collecting instruction counts with <code class="docutils literal notranslate"><span class="pre">Callgrind</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#learn-more">Learn More</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
         <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
         <script src="../../_static/jquery.js"></script>
         <script src="../../_static/underscore.js"></script>
         <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../_static/doctools.js"></script>
         <script src="../../_static/clipboard.min.js"></script>
         <script src="../../_static/copybutton.js"></script>
         <script src="../../_static/katex.min.js"></script>
         <script src="../../_static/auto-render.min.js"></script>
         <script src="../../_static/katex_autorenderer.js"></script>
         <script src="../../_static/design-tabs.js"></script>
     

  

  <script type="text/javascript" src="../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>

// Helper function to make it easier to call dataLayer.push() 
function gtag(){window.dataLayer.push(arguments);}

//add microsoft link

if(window.location.href.indexOf("/beginner/basics/")!= -1)
{
  var url="https://docs.microsoft.com/learn/paths/pytorch-fundamentals/?wt.mc_id=aiml-7486-cxa";
  switch(window.location.pathname.split("/").pop().replace('.html',''))
  {
    case"quickstart_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/9-quickstart?WT.mc_id=aiml-7486-cxa";
      break;
    case"tensorqs_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/2-tensors?WT.mc_id=aiml-7486-cxa";
      break;
    case"data_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/3-data?WT.mc_id=aiml-7486-cxa";
      break;
    case"transforms_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/4-transforms?WT.mc_id=aiml-7486-cxa";
      break;
    case"buildmodel_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/5-model?WT.mc_id=aiml-7486-cxa";
      break;
    case"autogradqs_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/6-autograd?WT.mc_id=aiml-7486-cxa";
      break;
    case"optimization_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/7-optimization?WT.mc_id=aiml-7486-cxa";
      break;
    case"saveloadrun_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/8-inference?WT.mc_id=aiml-7486-cxa";
    }
    
    $(".pytorch-call-to-action-links").children().first().before("<a href="+url+' data-behavior="call-to-action-event" data-response="Run in Microsoft Learn" target="_blank"><div id="microsoft-learn-link" style="padding-bottom: 0.625rem;border-bottom: 1px solid #f3f4f7;padding-right: 2.5rem;display: -webkit-box;  display: -ms-flexbox; display: flex; -webkit-box-align: center;-ms-flex-align: center;align-items: center;"><img class="call-to-action-img" src="../../_static/images/microsoft-logo.svg"/><div class="call-to-action-desktop-view">Run in Microsoft Learn</div><div class="call-to-action-mobile-view">Learn</div></div></a>')
  }

  !function(f,b,e,v,n,t,s)
  {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
  n.callMethod.apply(n,arguments):n.queue.push(arguments)};
  if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
  n.queue=[];t=b.createElement(e);t.async=!0;
  t.src=v;s=b.getElementsByTagName(e)[0];
  s.parentNode.insertBefore(t,s)}(window,document,'script',
  'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');

  $("[data-behavior='call-to-action-event']").on('click', function(){
    fbq('trackCustom', "Download", {
      tutorialTitle: $('h1:first').text(),
      downloadLink: this.href,
      tutorialLink: window.location.href,
      downloadTitle: $(this).attr("data-response")
    });
    gtag('event', 'click', {
      'event_category': $(this).attr("data-response"),
      'event_label': $("h1").first().text(),
      'tutorial_link': window.location.href
    });
   });

   $("[data-behavior='tutorial-rating']").on('click', function(){
    fbq('trackCustom', "Tutorial Rating", {
      tutorialLink: window.location.href,
      tutorialTitle: $('h1:first').text(),
      rating: $(this).attr("data-count")
    });

    gtag('event', 'click', {
      'event_category': 'Tutorial Rating',
      'event_label': $("h1").first().text(),
      'value': $(this).attr("data-count")
    });
   });

   if (location.pathname == "/") {
     $(".rating-container").hide();
     $(".hr-bottom").hide();
   }


</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

<script type="text/javascript">
  var collapsedSections = ['PyTorch Recipes', 'Learning PyTorch', 'Image and Video', 'Audio', 'Text', 'Backends', 'Reinforcement Learning', 'Deploying PyTorch Models in Production', 'Code Transforms with FX', 'Frontend APIs', 'Extending PyTorch', 'Model Optimization', 'Parallel and Distributed Training', 'Mobile'];
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>