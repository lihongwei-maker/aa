{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nZeroing out gradients in PyTorch\n================================\nIt is beneficial to zero out gradients when building a neural network.\nThis is because by default, gradients are accumulated in buffers (i.e,\nnot overwritten) whenever ``.backward()`` is called.\n\nIntroduction\n------------\nWhen training your neural network, models are able to increase their\naccuracy through gradient decent. In short, gradient descent is the\nprocess of minimizing our loss (or error) by tweaking the weights and\nbiases in our model.\n\n``torch.Tensor`` is the central class of PyTorch. When you create a\ntensor, if you set its attribute ``.requires_grad`` as ``True``, the\npackage tracks all operations on it. This happens on subsequent backward\npasses. The gradient for this tensor will be accumulated into ``.grad``\nattribute. The accumulation (or sum) of all the gradients is calculated\nwhen .backward() is called on the loss tensor.\n\nThere are cases where it may be necessary to zero-out the gradients of a\ntensor. For example: when you start your training loop, you should zero\nout the gradients so that you can perform this tracking correctly.\nIn this recipe, we will learn how to zero out gradients using the\nPyTorch library. We will demonstrate how to do this by training a neural\nnetwork on the ``CIFAR10`` dataset built into PyTorch.\n\nSetup\n-----\nSince we will be training data in this recipe, if you are in a runable\nnotebook, it is best to switch the runtime to GPU or TPU.\nBefore we begin, we need to install ``torch`` and ``torchvision`` if\nthey aren\u2019t already available.\n\n::\n\n   pip install torchvision\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Steps\n-----\n\nSteps 1 through 4 set up our data and neural network for training. The\nprocess of zeroing out the gradients happens in step 5. If you already\nhave your data and neural network built, skip to 5.\n\n1. Import all necessary libraries for loading our data\n2. Load and normalize the dataset\n3. Build the neural network\n4. Define the loss function\n5. Zero the gradients while training the network\n\n1. Import necessary libraries for loading our data\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFor this recipe, we will just be using ``torch`` and ``torchvision`` to\naccess the dataset.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torch.optim as optim\n\nimport torchvision\nimport torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Load and normalize the dataset\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPyTorch features various built-in datasets (see the Loading Data recipe\nfor more information).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Build the neural network\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWe will use a convolutional neural network. To learn more see the\nDefining a Neural Network recipe.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Define a Loss function and optimizer\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nLet\u2019s use a Classification Cross-Entropy loss and SGD with momentum.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "net = Net()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Zero the gradients while training the network\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis is when things start to get interesting. We simply have to loop\nover our data iterator, and feed the inputs to the network and optimize.\n\nNotice that for each entity of data, we zero out the gradients. This is\nto ensure that we aren\u2019t tracking any unnecessary information when we\ntrain our neural network.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for epoch in range(2):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 2000 == 1999:    # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 2000))\n            running_loss = 0.0\n\nprint('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also use ``model.zero_grad()``. This is the same as using\n``optimizer.zero_grad()`` as long as all your model parameters are in\nthat optimizer. Use your best judgement to decide which one to use.\n\nCongratulations! You have successfully zeroed out gradients PyTorch.\n\nLearn More\n----------\n\nTake a look at these other recipes to continue your learning:\n\n- `Loading data in PyTorch <https://pytorch.org/tutorials/recipes/recipes/loading_data_recipe.html>`__\n- `Saving and loading models across devices in PyTorch <https://pytorch.org/tutorials/recipes/recipes/save_load_across_devices.html>`__\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}