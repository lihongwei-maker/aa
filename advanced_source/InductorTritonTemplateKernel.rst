How to let Torchinductor codegen from user-defined templates
============================================================

This tutorial assumes the reader has an overview of how Torchinductor
generates Triton code and a basic understanding of the Triton languange.
This tutorial is intended for developers who want to further improve the
performance of some operations, like Convolution and Matrix Multiply,
via manually-written triton kernel combined with automatically generated
epilogue.

Motivation
----------

For most of the operations, Torchinductor is able to automatically
codegen a performant Triton kernel. However, for ops with now lowerings,
TorchInductor generate a fallback ``ir.ExternKernel``, which just calls
the original function. The main examples here are ``aten.convolution``
for Convolution, ``aten.mm.out`` for MatrixMultiply, etc. TorchInductor
supports alternate implementations of these ops, such as
``triton.ops.matmul``, but we could get further benifiet from epilogue
fusion that could alleviate the memory-bound bottleneck of pointwise ops
that follow the more expensive kernel. To access this additional
speedup, one can add a Triton template for Torchinductor that allows
these epilogue fusion kernels to be automatically generated.

Overview
--------

Codegen from custom templates is derived from codegen normal Triton
kernels. So in this tutorial, we will only focus on the difference. The
main python file for codegen triton templates is
`triton_template.py <https://github.com/pytorch/torchdynamo/blob/main/torchinductor/codegen/triton_template.py>`__.
Other files that need to modify when adding a new implementation as
template file for Torchinductor codegen are
`scheduler.py <https://github.com/pytorch/torchdynamo/blob/main/torchinductor/scheduler.py>`__,
`ir.py <https://github.com/pytorch/torchdynamo/blob/main/torchinductor/ir.py>`__.

Class TemplateSchedulerNode is a subclass of BaseSchedulerNode. During
the ``init`` of Scheduler, it will create TemplateSchedulerNode if the
node ``should_use_template()``, and append to the ``self.nodes`` list.
And when the Scheduler does ``codegen()``, the TemplateSchedulerNode
will call
```template_codegen()`` <https://github.com/pytorch/torchdynamo/blob/main/torchinductor/codegen/triton_template.py#L299>`__
to generate the combined kernel.

Class TritonTemplateKernel is a subclass of TritonKernel. In
``template_codegen()``, TritonTemplateKernel will be initialized by the
TemplateSchedulerNode with epilog-fusable nodes. And do the following
things:

-  Load template files (jinja2 templates), and fill the blanks later in
   ``codegen()``
-  ``kernel.map_args`` - map const args/ shape/ strides to kernel args

   -  Set map_args in ExternKernel that could codegen from template

-  ``kernel.rename_vars()`` - Set self.cse.store_cache to remove
   redundant stores/loads
-  ``codegen_body()``

   -  add extra args for fused pointwise operations
   -  assign_block_numel() - Match BLOCK names. (e.g. XBLOCK:
      tl.constexpr = BLOCK_M)

-  codegen nodes in the epilog fusion group that are fusable with the
   current TemplateTritonKernel.
-  ``precompute()`` - some triton kernels needs host precompute tensor,
   generate code for this

Template File
-------------

Torchinductor uses `jinja2 <https://palletsprojects.com/p/jinja/>`__
which is a common template engine for Python as the template file. A
typical triton template file used in Torchinductor looks like this

.. code:: py

   def {{kernel_name}}{
     {{template_arg_defs}},
     # some fixed args in the template,
     ...,
     {{extra_argdefs}},  # fusable nodes' args
     BLOCK_M: tl.constexpr,
     BLOCK_N: tl.constexpr,
     ...
   }:
     # The implementation for the operation of the template, like matmul, conv, ...
     ...
     acc = acc.to({{out_def}}.dtype.element_ty)
   {% if keep_store %}
     # store to the original output tensor if it will be used later outside the kernel
   {% endif %}
   {# fill in the code generated by epilog-fusable nodes #}
   {% if pointwise_code %}
   {{ pointwise_code | indent(4, true) }}
   {% endif %}

`triton_mm.j2 <https://github.com/pytorch/torchdynamo/blob/main/torchinductor/codegen/triton_mm.j2>`__
is an example triton template file. For those who are not familiar with
Jinja2 grammar, Torchinductor only uses the basics of Jinja2 syntax as
follows: \* {% … %} for Statements, like for loop, if-else block, etc.
\* {{ … }} for Expressions to print to the template output \* {# … #}
for Comments not included in the template output

The variables (like ``kernel_name``, ``pointwise_code``) are defined by
the dictionary passed to the template. TritonTemplateKernel will load
the jinja2 template files according the node type. And after every
variable is generated by codegen the fusable nodes, those varaibles are
put into a dictionary for the template file to render.

TemplateSchedulerNode
---------------------

Update dependency type
~~~~~~~~~~~~~~~~~~~~~~

The default ExternKernel node’s dependency type is ``StarDep``.
``StarDep("A")`` means the node depends on A[*] (all elements of A). The
entire A buffer must be fully computed before the node with that dep is
scheduled. It prevents A to be fused with other nodes.

But if we want to enable epilogue fusion with the ExternKernel that
could codegen from triton template, we need to update its dependencies
from ``StarDep`` to ``MemoryDep``. Usually if we want to add a triton
template for a new ExternKernel operation, we do not need to modify this
class.

TritonTemplateKernel
--------------------

Init
~~~~

Beside the init from its superclass TritonKernel, during ``__init__``,
it will load the templates by ``env.get_template()`` according to node’s
type, and also set pid_cache if needed. For example, in
`triton_mm.j2 <https://github.com/pytorch/torchdynamo/blob/main/torchinductor/codegen/triton_mm.j2>`__,
because it re-order program ID for better L2 performance, the pid on the
two dimensions are no longer ``tl.program_id(0)`` and
``tl.program_id(1)``. They should be replaced as ``pid_m`` and ``pid_n``
after re-ordering. Otherwise, the following fusion nodes will compute on
different blocks of data. If in your new triton template, there is no
re-ordering of program ID, the pid_cache is optional (could be empty).

map_args
~~~~~~~~

How do we know the ``template_arg_defs`` to be fed into the Triton
template files? This are set in ``map_args()`` in
`ir.py <https://github.com/pytorch/torchdynamo/blob/main/torchinductor/ir.py>`__.
There are four kinds of dictionary - inout_dict, args_dict, const_dict,
other_dict, where the key is the parameter of the kernel, and value is
the argument when the kernel is called. Inout_dict set the input and
output arguments; args_dict set the non-triton-constant args; const_dict
set the arguments whose type is ``tl.constexpr``; other_dict set
arguments that are used elsewhere (like in precompute code) other than
triton kernel itself.

rename_vars
~~~~~~~~~~~

The epilog-fusable nodes for TritonTemplateKernel belong to
SchedulerNode class. They will codegen unware of the TritonTemplate
Kernel. To avoid redundant read/write to the memory, the epilog-fusable
nodes should directly compute on the intermediate triton tensor, like
``acc``. We will need to set ``self.cse.store_cache`` properly to let
fusable nodes know that when they want to load from the output of the
kernel, they should use ``acc``.

Codegen precompute code
~~~~~~~~~~~~~~~~~~~~~~~

Some of the implementation needs precompute tensor on host and provide
it to the CUDA kernel as a lookup table to reduce computational overhead
at runtime. For example,
`triton_conv_delta_x.j2 <https://github.com/pytorch/torchdynamo/blob/main/torchinductor/codegen/triton_conv_delta_x.j2>`__
needs to precompute the pointer offset updates for each iteration of
loop on host. So in ``precompute()``, it will codegen the call to
compute those tensors.

Layout and Loop order
---------------------

Most of the triton implementations of operations do not have a specific
requirements for layout or the loop order for epilog fusion. However,
for operation like Convolution, in
`triton_conv_delta_x.j2 <https://github.com/pytorch/torchdynamo/blob/main/torchinductor/codegen/triton_conv_delta_x.j2>`__,
we use implicit GEMM alorighm where we do dot production on two
dimensions (N*H*W, C) over four dimension tensor (N, C, H, W). So the
intermediate tensor ``acc`` is a 2d tensor (N*H*W, C), indicating that
any fusable nodes should be channel-last to be a valid candidates to be
fused. In this case, we need to modify ``decide_layout()`` and
``simplify_and_reorder()`` so that if the node reads from Convolution
that will codegen from template, it will prefer the channel-last layout
and loop order. Otherwise, they will not be fused with the kernel.

Example (Codegen from triton_mm.j2 template for ir.MatrixMultiply)
------------------------------------------------------------------

demo/mm_triton_template.py; Setting torchinductor.config.triton.mm =
“aten” or “triton”

.. code:: py

   import torch
   import torchdynamo
   import torchinductor.config
   torchinductor.config.debug = True
   torchinductor.config.triton.dense_indexing = True
   torchinductor.config.triton.mm = "triton" # or "aten"
   torch.manual_seed(0)
   # The flag below controls whether to allow TF32 on matmul.
   torch.backends.cuda.matmul.allow_tf32 = True

   *@torchdynamo.optimize("inductor")
   def mm_relu(a, b):
       y = torch.mm(a, b)
       return torch.relu(y)
   shape = ([128, 9216], [9216, 4096])
   dtype = torch.float16
   M, K = shape[0]
   _, N = shape[1]
   torch.manual_seed(0)
   # allocate inputs
   a = torch.randn(shape[0], device="cuda", dtype=dtype)
   b = torch.randn(shape[1], device="cuda", dtype=dtype)
   c = mm_relu(a, b)

When torchinductor.config.triton.mm = “aten”, Torchinductor will treat
mm as an extern kernel call - ``aten.mm.out`` and the call() function
will have one call for ``mm`` and one call to kernel0 for ``relu``.

.. code:: py

   @pointwise_heuristics(size_hints=[524288])
   @triton.jit
   def kernel0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
       xoffset = tl.program_id(0) * XBLOCK
       xindex = xoffset + tl.reshape(tl.arange(0, XBLOCK), [XBLOCK])
       xmask = xindex < xnumel
       x0 = xindex
       tmp0 = tl.load(in_ptr0 + x0 + tl.zeros([XBLOCK], tl.int32), xmask).to(tl.float32)
       tmp1 = tl.maximum(0, tmp0)
       tl.store(out_ptr0 + x0 + tl.zeros([XBLOCK], tl.int32), tmp1, xmask)


   def call(primals_1, primals_2):
       primals_1_size = primals_1.size()
       s0 = primals_1_size[0]
       s1 = primals_1_size[1]
       primals_2_size = primals_2.size()
       s2 = primals_2_size[1]
       buf0 = empty_strided((s0, s2), (s2, 1), device='cuda', dtype=torch.float16)
       aten.mm.out(primals_1, primals_2, out=buf0)
       buf1 = empty_strided((s0, s2), (s2, 1), device='cuda', dtype=torch.float16)
       kernel0_xnumel = s0*s2
       kernel0[grid(kernel0_xnumel)](buf0, buf1, kernel0_xnumel)
       return (buf1, )

When torchinductor.config.triton.mm = “triton”, Torchinductor will load
the template
`triton_mm.j2 <https://github.com/pytorch/torchdynamo/blob/main/torchinductor/codegen/triton_mm.j2>`__
and generate code below. It will codegen only one kernel (kernel0) for
mm+relu.

.. code:: py

   @mm_heuristics()
   @mm_autotune()
   @triton.jit
   def kernel0(
       A,B,M,N,K,stride_am,stride_ak,stride_bk,stride_bn,stride_cm,stride_cn,
       # fusable kernels args
       out_ptr3,
       ks0,
       allow_tf32: tl.constexpr,
       BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,
       BLOCK_K: tl.constexpr, GROUP_M: tl.constexpr,
       SPLIT_K: tl.constexpr, EVEN_K: tl.constexpr,
       ACC_TYPE: tl.constexpr,
   ):
       # matrix multiplication
       pid = tl.program_id(0)
       pid_z = tl.program_id(1)
       grid_m = (M + BLOCK_M - 1) // BLOCK_M
       grid_n = (N + BLOCK_N - 1) // BLOCK_N
       # re-order program ID for better L2 performance
       width = GROUP_M * grid_n
       group_id = pid // width
       group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
       pid_m = group_id * GROUP_M + (pid % group_size)
       pid_n = (pid % width) // (group_size)
       # do matrix multiplication
       rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
       rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
       ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
       rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
       rk = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)
       # pointers
       A_ptrs = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
       B_ptrs = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)
       acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
       for k in range(K, 0, -BLOCK_K * SPLIT_K):
           if EVEN_K:
               a = tl.load(A_ptrs)
               b = tl.load(B_ptrs)
           else:
               a = tl.load(A_ptrs, mask=rk[None, :] < k, other=0.0)
               b = tl.load(B_ptrs, mask=rk[:, None] < k, other=0.0)
           acc += tl.dot(a, b, allow_tf32=allow_tf32)
           A_ptrs += BLOCK_K * SPLIT_K * stride_ak
           B_ptrs += BLOCK_K * SPLIT_K * stride_bk
       acc = acc.to(out_ptr3.dtype.element_ty)


       XBLOCK: tl.constexpr = BLOCK_M
       YBLOCK: tl.constexpr = BLOCK_N
       xnumel = M
       ynumel = N
       xoffset = pid_m * XBLOCK
       xindex = xoffset + tl.reshape(tl.arange(0, XBLOCK), [XBLOCK, 1])
       xmask = xindex < xnumel
       yoffset = pid_n * YBLOCK
       yindex = yoffset + tl.reshape(tl.arange(0, YBLOCK), [1, YBLOCK])
       ymask = yindex < ynumel
       x0 = xindex
       y1 = yindex
       tmp0 = tl.maximum(0, *acc*)
       tl.store(out_ptr3 + y1 + (ks0*x0) + tl.zeros([XBLOCK, YBLOCK], tl.int32), tmp0, xmask & ymask)


   def call(primals_1, primals_2):
       primals_1_size = primals_1.size()
       s0 = primals_1_size[0]
       s1 = primals_1_size[1]
       primals_2_size = primals_2.size()
       s2 = primals_2_size[1]
       buf1 = empty_strided((s0, s2), (s2, 1), device='cuda', dtype=torch.float16)
       
       def grid_kernel0(META):
           return (
               triton.cdiv(s0, META["BLOCK_M"]) * triton.cdiv(s2, META["BLOCK_N"]),
               META["SPLIT_K"],
           )

       kernel0[grid_kernel0](primals_1, primals_2, s0, s2, s1, s1, 1, s2, 1, s2, 1, buf1, s2, GROUP_M=8, ACC_TYPE=tl.float32, allow_tf32=True)
       return (buf1, )
