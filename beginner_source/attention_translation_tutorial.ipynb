{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "import torchtext\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "     batch_size=BATCH_SIZE,\n",
    "     device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim: int, \n",
    "                 n_heads: int, \n",
    "                 dropout: float, \n",
    "                 device: torch.device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.w_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.w_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.w_v = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim // n_heads])).to(device)\n",
    "        \n",
    "    def forward(self, \n",
    "                query: Tensor, \n",
    "                key: Tensor, \n",
    "                value: Tensor, \n",
    "                mask: Tensor = None):\n",
    "        \n",
    "        bsz = query.shape[0]\n",
    "        \n",
    "        #query = key = value [batch size, sent len, hid dim]\n",
    "        \n",
    "        #In the Encoder, these are all \"src\"\n",
    "        #In the Decoder, these are all \"trg\"\n",
    "        \n",
    "        #In the Decoder, \"encoder attention\" has \"trg\" as the query, and \"src\" as the key and value        \n",
    "                \n",
    "        Q = self.w_q(query)\n",
    "        K = self.w_k(key)\n",
    "        V = self.w_v(value)\n",
    "        \n",
    "        #Q, K, V = [batch size, sent len, hid dim]\n",
    "        \n",
    "        Q = Q.view(bsz, -1, self.n_heads, self.hid_dim // self.n_heads).permute(0, 2, 1, 3)\n",
    "        K = K.view(bsz, -1, self.n_heads, self.hid_dim // self.n_heads).permute(0, 2, 1, 3)\n",
    "        V = V.view(bsz, -1, self.n_heads, self.hid_dim // self.n_heads).permute(0, 2, 1, 3)\n",
    "        \n",
    "        #Q, K, V = [batch size, n heads, sent len, hid dim // n heads]\n",
    "        \n",
    "        #K.permute() has shape  [batch size, n heads, hid dim // n heads, sent len]\n",
    "        \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        \n",
    "        #energy = [batch size, n heads, sent len, sent len]\n",
    "        #this gives how much energy each element should pay attention to each other element\n",
    "        #this is also where we apply the mask \n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention = self.dropout(F.softmax(energy, dim=-1))\n",
    "\n",
    "        #attention = [batch size, n heads, sent len, sent len]\n",
    "        \n",
    "        x = torch.matmul(attention, V)\n",
    "        \n",
    "        #x = [batch size, n heads, sent len, hid dim // n heads]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        #x = [batch size, sent len, n heads, hid dim // n heads]\n",
    "        \n",
    "        x = x.view(bsz, -1, self.n_heads * (self.hid_dim // self.n_heads))\n",
    "        \n",
    "        #x = [batch size, src sent len, hid dim]\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        #x = [batch size, sent len, hid dim]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforward(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim: int, \n",
    "                 pf_dim: int, \n",
    "                 dropout: float):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.pf_dim = pf_dim\n",
    "        \n",
    "        self.fc_1 = nn.Conv1d(hid_dim, pf_dim, 1)\n",
    "        self.fc_2 = nn.Conv1d(pf_dim, hid_dim, 1)\n",
    "        \n",
    "        self.do = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, \n",
    "                x: Tensor):\n",
    "        \n",
    "        #x = [batch size, sent len, hid dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        #x = [batch size, hid dim, sent len]\n",
    "        \n",
    "        x = self.dropout(F.relu(self.fc_1(x)))\n",
    "        \n",
    "        #x = [batch size, ff dim, sent len]\n",
    "        \n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        #x = [batch size, hid dim, sent len]\n",
    "        \n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        #x = [batch size, sent len, hid dim]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim: int, \n",
    "                 n_heads: int, \n",
    "                 pf_dim: int, \n",
    "                 self_attention: SelfAttention, \n",
    "                 positionwise_feedforward: PositionwiseFeedforward, \n",
    "                 dropout: float, \n",
    "                 device: torch.device):\n",
    "        super().__init__()      \n",
    "        self.layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = self_attention(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = positionwise_feedforward(hid_dim, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, \n",
    "                src: Tensor, \n",
    "                src_mask: Tensor):\n",
    "        \n",
    "        #src = [batch size, src sent len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src sent len]\n",
    " \n",
    "        #src = [batch size, src sent len, hid dim]\n",
    "        src = self.layer_norm(\n",
    "            src + self.dropout(self.self_attention(\n",
    "                src, src, src, src_mask)))\n",
    "        \n",
    "        #src = [batch size, src sent len, hid dim]        \n",
    "        src = self.layer_norm(\n",
    "            src + self.dropout(\n",
    "                self.positionwise_feedforward(src)))\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim: int, \n",
    "                 hid_dim: int, \n",
    "                 n_layers: int, \n",
    "                 n_heads: int, \n",
    "                 pf_dim: int, \n",
    "                 encoder_layer: EncoderLayer, \n",
    "                 self_attention: SelfAttention, \n",
    "                 positionwise_feedforward: PositionwiseFeedforward, \n",
    "                 dropout: float, \n",
    "                 device: torch.device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.pf_dim = pf_dim\n",
    "        self.encoder_layer = encoder_layer\n",
    "        self.self_attention = self_attention\n",
    "        self.positionwise_feedforward = positionwise_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(1000, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([encoder_layer(hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device) \n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, \n",
    "                src: Tensor, \n",
    "                src_mask: Tensor):      \n",
    "        #src = [batch size, src sent len]\n",
    "        #src_mask = [batch size, 1, 1, src sent len]\n",
    "        \n",
    "        pos = torch.arange(0, src.shape[1]).unsqueeze(0).repeat(src.shape[0], 1).to(self.device)\n",
    "        #pos = [batch size, src sent len]\n",
    "        \n",
    "        src_embedded = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        \n",
    "        #src = [batch size, src sent len, hid dim]\n",
    "        \n",
    "        # each layer is an \"EncoderLayer\"\n",
    "        for layer in self.layers:\n",
    "            src_embedded = layer(src_embedded, src_mask)\n",
    "            \n",
    "        return src_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hid_dim: int, \n",
    "                 n_heads: int, \n",
    "                 pf_dim: int, \n",
    "                 self_attention: SelfAttention, \n",
    "                 positionwise_feedforward: PositionwiseFeedforward, \n",
    "                 dropout: float, \n",
    "                 device: torch.device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_nore = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = self_attention(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention = self_attention(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = positionwise_feedforward(hid_dim, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, \n",
    "                trg: Tensor, \n",
    "                src: Tensor, \n",
    "                trg_mask: Tensor, \n",
    "                src_mask: Tensor):\n",
    "        \n",
    "        #trg = [batch size, trg sent len, hid dim]\n",
    "        #src = [batch size, src sent len, hid dim]\n",
    "        #trg_mask = [batch size, trg sent len]\n",
    "        #src_mask = [batch size, src sent len]\n",
    "                \n",
    "        trg = self.layer_norm(\n",
    "            trg + self.dropout(\n",
    "                self.self_attention(trg, trg, trg, trg_mask)))\n",
    "                \n",
    "        trg = self.layer_norm(\n",
    "            trg + self.do(\n",
    "                self.encoder_attention(trg, src, src, src_mask)))\n",
    "        \n",
    "        trg = self.layer_norm(\n",
    "            trg + self.dropout(\n",
    "                self.positionwise_feedforward(trg)))\n",
    "        \n",
    "        return trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 output_dim: int, \n",
    "                 hid_dim: int, \n",
    "                 n_layers: int, \n",
    "                 n_heads: int, \n",
    "                 pf_dim: int, \n",
    "                 decoder_layer: DecoderLayer, \n",
    "                 self_attention: SelfAttention, \n",
    "                 positionwise_feedforward: PositionwiseFeedforward, \n",
    "                 dropout: float, \n",
    "                 device: torch.device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.pf_dim = pf_dim\n",
    "        self.decoder_layer = decoder_layer\n",
    "        self.self_attention = self_attention\n",
    "        self.positionwise_feedforward = positionwise_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(1000, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([decoder_layer(hid_dim, n_heads, pf_dim, \n",
    "                                                   self_attention, \n",
    "                                                   positionwise_feedforward, \n",
    "                                                   dropout, device)\n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.fc = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, \n",
    "                trg: Tensor, \n",
    "                src: Tensor, \n",
    "                trg_mask: Tensor, \n",
    "                src_mask: Tensor):            \n",
    "        #trg = [batch_size, trg sent len]\n",
    "        #src = [batch_size, src sent len]\n",
    "        #trg_mask = [batch size, trg sent len]\n",
    "        #src_mask = [batch size, src sent len]\n",
    "        \n",
    "        #pos = [batch_size, trg sent len]\n",
    "        pos = torch.arange(0, trg.shape[1]).unsqueeze(0).repeat(trg.shape[0], 1).to(self.device)\n",
    "\n",
    "        #trg = [batch_size, trg sent len]        \n",
    "        trg_embedded = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        \n",
    "        #trg = [batch size, trg sent len, hid dim]\n",
    "        \n",
    "        #trg_mask = [batch size, 1, trg sent len, trg sent len]      \n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg_embedded = layer(trg_embedded, src, trg_mask, src_mask)\n",
    "\n",
    "        #trg = [batch size, trg sent len, hid dim]            \n",
    "\n",
    "        return self.fc(trg_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, \n",
    "                 encoder: Encoder, \n",
    "                 decoder: Decoder, \n",
    "                 pad_idx: int, \n",
    "                 device: torch.device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pad_idx = pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_masks(self, \n",
    "                   src: Tensor, \n",
    "                   trg: Tensor):\n",
    "        \n",
    "        #src = [batch size, src sent len]\n",
    "        #trg = [batch size, trg sent len]\n",
    "        \n",
    "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        trg_pad_mask = (trg != self.pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "\n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), dtype=torch.uint8, device=self.device))\n",
    "        \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        \n",
    "        return src_mask, trg_mask\n",
    "    \n",
    "    def forward(self, \n",
    "                src: Tensor, \n",
    "                trg: Tensor):\n",
    "        \n",
    "        #src = [batch size, src sent len]\n",
    "        #trg = [batch size, trg sent len]\n",
    "                \n",
    "        src_mask, trg_mask = self.make_masks(src, trg)\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        \n",
    "        #enc_src = [batch size, src sent len, hid dim]\n",
    "                \n",
    "        out = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #out = [batch size, trg sent len, output dim]\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(SRC.vocab)\n",
    "hid_dim = 512\n",
    "# n_layers = 2\n",
    "n_layers = 6\n",
    "n_heads = 8\n",
    "pf_dim = 2048\n",
    "dropout = 0.1\n",
    "\n",
    "enc = Encoder(input_dim, hid_dim, n_layers, n_heads, pf_dim, EncoderLayer, SelfAttention, PositionwiseFeedforward, dropout, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = len(TRG.vocab)\n",
    "hid_dim = 512\n",
    "n_layers = 6\n",
    "# n_layers = 2\n",
    "n_heads = 8\n",
    "pf_dim = 2048\n",
    "dropout = 0.1\n",
    "\n",
    "dec = Decoder(output_dim, hid_dim, n_layers, n_heads, pf_dim, DecoderLayer, SelfAttention, PositionwiseFeedforward, dropout, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = SRC.vocab.stoi['<pad>']\n",
    "\n",
    "model = Seq2Seq(enc, dec, pad_idx, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 55,206,149 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, \n",
    "                 model_size: int, \n",
    "                 factor: int, \n",
    "                 warmup: int, \n",
    "                 optimizer: Optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = NoamOpt(hid_dim, 1, 2000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, \n",
    "          iterator: BucketIterator, \n",
    "          optimizer: optim.Adam, \n",
    "          criterion: nn.modules.loss.CrossEntropyLoss, \n",
    "          clip: float):\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg sent len - 1, output dim]\n",
    "        #trg = [batch size, trg sent len]\n",
    "            \n",
    "        output = output.contiguous().view(-1, output.shape[-1])\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "                \n",
    "        #output = [batch size * trg sent len - 1, output dim]\n",
    "        #trg = [batch size * trg sent len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, \n",
    "             iterator: BucketIterator, \n",
    "             criterion: nn.modules.loss.CrossEntropyLoss):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg sent len - 1, output dim]\n",
    "            #trg = [batch size, trg sent len]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg sent len - 1, output dim]\n",
    "            #trg = [batch size * trg sent len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time: int, \n",
    "               end_time: int):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 1\n",
    "CLIP = 1\n",
    "SAVE_DIR = 'models'\n",
    "MODEL_SAVE_PATH = os.path.join(SAVE_DIR, 'transformer-seq2seq.pt')\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "if not os.path.isdir(f'{SAVE_DIR}'):\n",
    "    os.makedirs(f'{SAVE_DIR}')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:03} | Time: {epoch_mins}m {epoch_secs}s| Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Val. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
